{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd2c1dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Program Spectrum Matrix:\n",
      "[1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0]\n",
      "[1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1]\n",
      "[0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1]\n",
      "[1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0]\n",
      "[1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1]\n",
      "[0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0]\n",
      "[1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
      "[1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0]\n",
      "[1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1]\n",
      "[0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1]\n",
      "[0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1]\n",
      "[0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0]\n",
      "[0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0]\n",
      "[1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1]\n",
      "[1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0]\n",
      "[1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0]\n",
      "[0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0]\n",
      "[0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0]\n",
      "[0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "[0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0]\n",
      "[0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0]\n",
      "[1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1]\n",
      "[1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0]\n",
      "[0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1]\n",
      "[0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1]\n",
      "[1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0]\n",
      "[1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1]\n",
      "[0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0]\n",
      "[1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1]\n",
      "[1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "[0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0]\n",
      "[1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
      "[1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0]\n",
      "[0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1]\n",
      "[1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0]\n",
      "[0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0]\n",
      "[0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1]\n",
      "[0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1]\n",
      "[1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1]\n",
      "[1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1]\n",
      "[0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
      "[0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1]\n",
      "[1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1]\n",
      "[0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "[1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n",
      "[0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0]\n",
      "[1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1]\n",
      "\n",
      "First Program Buggy Statements Indices: [469]\n",
      "\n",
      "First Program Outcome Vector: [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_random_spectrum(num_test_cases, num_statements, num_buggy_statements):\n",
    "    \"\"\"\n",
    "    Generate a random spectrum with multiple buggy statements.\n",
    "\n",
    "    Parameters:\n",
    "    - num_test_cases: int, number of test cases\n",
    "    - num_statements: int, number of statements in the program\n",
    "    - num_buggy_statements: int, number of statements to designate as buggy\n",
    "\n",
    "    Returns:\n",
    "    - spectrum_matrix: list of list of int, matrix representing test case execution (0 or 1)\n",
    "    - buggy_statements: list of int, indices of the buggy statements\n",
    "    - outcome_vector: list of int, vector representing test case outcomes (1 for fail, 0 for pass)\n",
    "    \"\"\"\n",
    "    # Choose multiple random buggy statement indices\n",
    "    buggy_statements = random.sample(range(num_statements), num_buggy_statements)\n",
    "    \n",
    "    spectrum_matrix = []\n",
    "    outcome_vector = []\n",
    "\n",
    "    for _ in range(num_test_cases):\n",
    "        test_case = [random.randint(0, 1) for _ in range(num_statements)]\n",
    "        spectrum_matrix.append(test_case)\n",
    "\n",
    "        if any(test_case[i] == 1 for i in buggy_statements):\n",
    "            outcome_vector.append(1)  # Test case failed\n",
    "        else:\n",
    "            outcome_vector.append(0)  # Test case passed\n",
    "\n",
    "    return spectrum_matrix, buggy_statements, outcome_vector\n",
    "\n",
    "def generate_dataset(num_programs, num_test_cases, num_statements, num_buggy_statements):\n",
    "    \"\"\"\n",
    "    Generate a dataset of programs with random spectra.\n",
    "\n",
    "    Parameters:\n",
    "    - num_programs: int, number of programs to generate\n",
    "    - num_test_cases: int, number of test cases per program\n",
    "    - num_statements: int, number of statements in each program\n",
    "    - num_buggy_statements: int, number of statements to designate as buggy\n",
    "\n",
    "    Returns:\n",
    "    - dataset: list of tuples, each tuple containing (spectrum_matrix, buggy_statements, outcome_vector)\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    \n",
    "    for _ in range(num_programs):\n",
    "        spectrum_matrix, buggy_statements, outcome_vector = generate_random_spectrum(num_test_cases, num_statements, num_buggy_statements)\n",
    "        dataset.append((spectrum_matrix, buggy_statements, outcome_vector))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Parameters for dataset generation\n",
    "num_programs = 1000\n",
    "num_test_cases = 50\n",
    "num_statements = 500\n",
    "num_buggy_statements = 1 # Example: designate 3 statements as buggy\n",
    "\n",
    "# Generate the dataset\n",
    "dataset = generate_dataset(num_programs, num_test_cases, num_statements, num_buggy_statements)\n",
    "\n",
    "# Example usage: Accessing the first program's data\n",
    "first_program_spectrum, first_program_buggy_statements, first_program_outcome = dataset[0]\n",
    "print(\"First Program Spectrum Matrix:\")\n",
    "for row in first_program_spectrum:\n",
    "    print(row)\n",
    "\n",
    "print(\"\\nFirst Program Buggy Statements Indices:\", first_program_buggy_statements)\n",
    "print(\"\\nFirst Program Outcome Vector:\", first_program_outcome)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7571e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74249395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def create_bipartite_graph(spectrum, test_case_outcomes, buggy_statements):\n",
    "    # Convert spectrum to a NumPy array if it's not already one\n",
    "    spectrum = np.array(spectrum)\n",
    "    \n",
    "    # Create a new graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    num_test_cases, num_statements = spectrum.shape\n",
    "\n",
    "    # Add test case nodes\n",
    "    test_case_nodes = range(num_test_cases)\n",
    "    G.add_nodes_from(test_case_nodes, bipartite=0)\n",
    "\n",
    "    # Add statement nodes\n",
    "    statement_nodes = range(num_test_cases, num_test_cases + num_statements)\n",
    "    G.add_nodes_from(statement_nodes, bipartite=1)\n",
    "\n",
    "    # Add outcome nodes (pass or fail)\n",
    "    outcome_nodes = [\"Pass\", \"Fail\"]\n",
    "    G.add_nodes_from(outcome_nodes, bipartite=2)\n",
    "\n",
    "    # Connect test case nodes to statement nodes based on spectrum\n",
    "    for i in range(num_test_cases):\n",
    "        for j in range(num_statements):\n",
    "            if spectrum[i][j] == 1:\n",
    "                G.add_edge(i, num_test_cases + j)\n",
    "\n",
    "    # Connect test case nodes to outcome nodes based on test_case_outcomes\n",
    "    for i, outcome in enumerate(test_case_outcomes):\n",
    "        if outcome == 1:  # Failed test case\n",
    "            G.add_edge(i, \"Fail\")\n",
    "        else:  # Passed test case\n",
    "            G.add_edge(i, \"Pass\")\n",
    "\n",
    "    # Add bug label attributes to statement nodes\n",
    "    bug_labels = {num_test_cases + i: (1 if i in buggy_statements else 0) for i in range(num_statements)}\n",
    "    nx.set_node_attributes(G, bug_labels, 'bug_label')\n",
    "\n",
    "    # Add other graph attributes\n",
    "    G.graph['spectrum_matrix'] = spectrum\n",
    "    G.graph['test_case_outcomes'] = test_case_outcomes\n",
    "    G.graph['buggy_statements'] = buggy_statements\n",
    "\n",
    "    return G\n",
    "\n",
    "# Create a directory to store the graphs\n",
    "if not os.path.exists('graphs'):\n",
    "    os.makedirs('graphs')\n",
    "\n",
    "# Example usage: Create and store graphs from the dataset\n",
    "for i in range(1000):\n",
    "    first_program_spectrum, first_program_buggy_statements, first_program_outcome = dataset[i]\n",
    "\n",
    "    G = create_bipartite_graph(first_program_spectrum, first_program_outcome, first_program_buggy_statements)\n",
    "    \n",
    "    # Store the graph\n",
    "    with open(f'graphs/graph_{i}.pickle', 'wb') as f:\n",
    "        pickle.dump(G, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d6faa02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552\n",
      "12684\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552\n",
      "12507\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552\n",
      "12557\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552\n",
      "12474\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAGZCAYAAAAUzjLvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABSdklEQVR4nO3deXhTVd4H8G/apkm6U3akG5QCLS1QWQqVRaWgqODIi6MzDjKyFBGYYXzdRlBA8HXcQEUFZAYc3DdmxEGlOCIou5ZFyloKiJQdutA1zXn/uE2btmnuTZub3DTfz/P0yXJPTk66nd/93bPohBACRERE5LP8PN0AIiIi8iwGA0RERD6OwQAREZGPYzBARETk4xgMEBER+TgGA0RERD6OwQAREZGPYzBARETk4xgMEBER+TgGA0QasW/fPkyaNAldu3aFyWSCyWRCt27dkJmZid27d3u0bbGxsbj99tub/PrJkyejV69eiIiIgMlkQkJCAh555BFcvHjRha0koqYK8HQDiAhYvnw5ZsyYge7du+NPf/oTkpKSoNPpcPDgQbz//vvo378/jh07hq5du3q6qU1y7do1TJ06FfHx8TAajdi9ezcWLVqE9evXIzs7G4GBgZ5uIpFP03FvAiLP+uGHHzB06FDcdttt+OSTT+x2jB9//DHS09PRqVOnRuspKSlBUFCQKm2MjY1Fr1698MUXX7iszjfffBPTp0/HN998g5tuusll9RKR83iZgMjDnn32Wfj7+2P58uWNniGPHz++TiAwceJEhISEYP/+/Rg5ciRCQ0Nx8803AwCysrIwduxYdO7cGUajEfHx8cjMzGyQkp83bx50Oh2ys7Nx1113ISwsDOHh4bjvvvtw4cIFu+346quvkJqaCpPJhB49euAf//hHkz9327ZtAQABAUxQEnka/wqJPKiqqgrffvst+vXrh44dOzr12oqKCowZMwaZmZl4/PHHYTabAQC5ubkYNGgQJk+ejPDwcJw4cQIvv/wybrjhBuzfvx96vb5OPb/5zW9w9913Y9q0aThw4ADmzp2LnJwc7Nixo07ZvXv34uGHH8bjjz+O9u3bY+XKlZg0aRLi4+MxdOhQRW02m80oLy/Hnj17MHfuXNxwww1IT0936nMTkQoEEXnM2bNnBQBxzz33NDhmNptFZWVlzZfFYqk5dv/99wsA4h//+IfD+i0Wi6isrBQnT54UAMS///3vmmNPP/20ACBmz55d5zXvvvuuACDeeeedmudiYmKE0WgUJ0+erHmutLRUREZGiszMTEWfddu2bQJAzdfo0aNFYWGhotcSkbp4mYBIo66//nro9fqar5deeqlBmXHjxjV47vz585g2bRqioqIQEBAAvV6PmJgYAMDBgwcblP/9739f5/Hdd9+NgIAAfPvtt3We79OnD6Kjo2seG41GJCQk4OTJk4o+T3JyMnbt2oXvvvsOr7zyCrKzs5GRkYGSkhJFryci9fAyAZEHtWnTBiaTyW6H+t5776GkpAT5+fkYM2ZMg+NBQUEICwur85zFYsHIkSNx5swZzJ07F8nJyQgODobFYkFaWhpKS0sb1NOhQ4c6jwMCAtC6dWtcunSpzvOtW7du8FqDwWC3TnuCg4PRr18/AMDQoUMxcOBApKWlYfny5Zg9e7aiOohIHQwGiDzI398fN910EzZs2ID8/Pw64wYSExMBACdOnLD7Wp1O1+C5n3/+GXv37sXq1atx//331zx/7NixRttw9uxZXHfddTWPzWYzLl26ZLfzd6V+/frBz88PR44cUfV9iEgeLxMQedgTTzyBqqoqTJs2DZWVlc2qyxogGAyGOs8vX7680de8++67dR5/9NFHMJvNGD58eLPaIue7776DxWJBfHy8qu9DRPKYGSDysPT0dLz++uuYOXMmUlNTMXXqVCQlJcHPzw/5+fn49NNPAaDBJQF7evToga5du+Lxxx+HEAKRkZFYt24dsrKyGn3NZ599hoCAAGRkZNTMJujduzfuvvtul3y+L774Am+99RbGjBmDmJgYVFZWYvfu3ViyZAni4+MxefJkl7wPETUdgwEiDZg2bRoGDRqEV155BYsXL8aZM2eg0+nQuXNnDB48WPHCPHq9HuvWrcOf/vQnZGZmIiAgACNGjMDGjRvrDP6z9dlnn2HevHl48803odPpcMcdd2DJkiUuWxUwPj4egYGBeOaZZ3Du3DkA0iJGkyZNwuOPP47w8HCXvA8RNR1XICTyUfPmzcP8+fNx4cIFtGnTxtPNISIP4pgBIiIiH8dggIiIyMfxMgEREZGPY2aAiIjIxzEYICIi8nGKphZaLBacOXMGoaGhdlc9IyIiIu0RQqCoqAidOnWCn1/j5/+KgoEzZ84gKirKZY0jIiIi9/nll1/QuXPnRo8rCgZCQ0NrKlOyChoRERF5XmFhIaKiomr68cYoCgaslwbCwsIYDBAREXkZuUv8HEBIRETk4xgMEBER+TgGA0RERD7OpbsWVlVVNXs/diJP0Ov18Pf393QziIg8wiXBgBACZ8+exdWrV5tcx7VrOpw6FYiKCh0CAwWioysQHMyVksl9IiIi0KFDB66lQUQ+xyXBgDUQaNeuHYKCghT/Mz14UIe33vLH11/7IS9PByFqX6fTCcTFCYwaZcGUKVXo2ZOBAalDCIGSkhKcP38eANCxY0cPt4iIyL2aHQxUVVXVBAKtW7dW9Jq8PCAzE8jKAgICALO5YRkhdDh+XIe33vLDm28GICMDWL4ciItrbouJGjKZTACA8+fPo127drxkQEQ+pdkDCK1jBIKCghSVX7kSSEwEvv1WemwvELBlPf7tt9LrVq5sakuJHLP+DnPcCxH5GpfNJlByaWDRImDKFKCsTD4IqM9sll43ZYpUD5GrcawAEfkqt00tXLkSmDPHNXXNmQP8/e+uqYuIiMjXuSUYyMsDZs50bZ0zZkj1EhERUfO4JRjIzHT+soAcs1mql4iIiJpH9WAgJ0eaNaBGMJCVBRw86PxrdTqdw6+JEyc2uV2xsbFYsmSJorLZ2dkYP3482rdvD6PRiISEBEyZMgVHjhxp8vsTERE5S/VgYNkyafqgGgICgDffdP51+fn5NV9LlixBWFhYnedeeeUV1ze2ni+++AJpaWkoLy/Hu+++i4MHD2LNmjUIDw/H3LlzVX9/IiIiK9WDgfXrXZ8VsDKbgS+/dP51HTp0qPkKDw+HTqer89zmzZtx/fXXw2g0okuXLpg/fz7MNh9i3rx5iI6OhsFgQKdOnTBr1iwAwPDhw3Hy5EnMnj27JstgT0lJCf74xz9i9OjR+PzzzzFixAjExcVh4MCBePHFF7F8+XIA0hoOkyZNQlxcHEwmE7p3794gUNm0aRMGDBiA4OBgREREID09HSdPnqw5vm7duiZ9FiIi8h0qnbNLioqA48fVfAcgNxcoLgZCQlxT39dff4377rsPr776KoYMGYLc3FxMnToVAPD000/jk08+weLFi/HBBx8gKSkJZ8+exd69ewEAn332GXr37o2pU6diypQpDt/j4sWLePTRR+0ej4iIAABYLBZ07twZH330Edq0aYOtW7di6tSp6NixI+6++26YzWbceeedmDJlCt5//31UVFRg586dNUFIcz4LERH5EKFAQUGBACAKCgoaHCstLRU5OTmitLS0wbHsbCEA9b+ys5V8CvtWrVolwsPDax4PGTJEPPvss3XKrFmzRnTs2FEIIcRLL70kEhISREVFhd36YmJixOLFix2+59/+9jcBQFy+fNnp9k6fPl2MGzdOCCHEpUuXBACxadMmu2Wb+1l8jaPfZSIiIYQoKi8S2fnZYvsv20V2frYoKi/ydJMcctR/21I1M1Bermbt6rzPjz/+iF27dmGRzcpGVVVVKCsrQ0lJCcaPH48lS5agS5cuuOWWWzB69GjccccdCHBiYIQQyvdZWLZsGVauXImTJ0+itLQUFRUV6NOnDwAgMjISEydOxKhRo5CRkYERI0bg7rvvrllb3x2fhYiopcu5kINlu5dh/dH1OH7lOARq/4froEOXVl0wuttoTOs3DYltEz3Y0qZTdcyAwaBm7eq8j8Viwfz587Fnz56ar/379+Po0aMwGo2IiorC4cOH8frrr8NkMmH69OkYOnSoU0vYJiQkAAAOHTrksNxHH32E2bNn44EHHsCGDRuwZ88e/PGPf0RFRUVNmVWrVmHbtm0YPHgwPvzwQyQkJGD79u1u+yxERC1V3pU8jFwzEklvJGHpzqXIvZJbJxAAAAGB3Cu5WLpzKZLeSMLINSORd8X7FsFR9RQwPh7Q6aRkvlp0Oul9XCU1NRWHDx9GvINKTSYTxowZgzFjxuChhx5Cjx49sH//fqSmpiIwMBBVVVUO32PkyJFo06YNnn/+eaxdu7bB8atXryIiIgJbtmzB4MGDMX369Jpjubm5Dcr37dsXffv2xRNPPIFBgwbhvffeQ1paWrM/CxGRr1r500rMWD8DFVXSyVf9IKA+6/GNxzei5+s9sXT0UkxOnax6O11F1WAgJATo0kUa5KeWrl1dN3gQAJ566incfvvtiIqKwvjx4+Hn54d9+/Zh//79WLhwIVavXo2qqioMHDgQQUFBWLNmDUwmE2JiYgBI6wxs3rwZ99xzDwwGA9q0adPgPYKDg7Fy5UqMHz8eY8aMwaxZsxAfH4+LFy/io48+wqlTp/DBBx8gPj4e//znP/H1118jLi4Oa9aswa5duxBXvXVjXl4eVqxYgTFjxqBTp044fPgwjhw5ggkTJrjksxAR+aJFmxdhzrdNWz9fQKC8qhxT1k3BueJzeHLoky5unTpUn1o4erS66wzceqtr6xw1ahS++OILZGVloX///khLS8PLL79c00FGRETgrbfeQnp6OlJSUvDNN99g3bp1Nds3L1iwACdOnEDXrl3Rtm3bRt9n7Nix2Lp1K/R6PX73u9+hR48euPfee1FQUICFCxcCAKZNm4a77roLv/3tbzFw4EBcunSpTpYgKCgIhw4dwrhx45CQkICpU6dixowZyKxemrG5n4WIyNes/GllkwOB+uZ8Owd//8k7NtLRCQWj2QoLCxEeHo6CggKEhYXVOVZWVoa8vDzExcXBaDQ2eG1ODpCU5LoG26u/Z0/16iffIfe7TEQtW96VPHRf2h2VFteNm9L76XF4xmHEtYpzWZ3OcNR/21J92HhiIpCRAXz7rWsXHwoIAG68kYEAERE1zaZNm3DjjTfaPzgJQJSd5wWAVQBOAegP4LZ6x4sAbARwFEA5UNm6EiOPj8TRVUdd1WxVuGWjouXLXX+pICBAqpeIiKgpPvnkk8YPnmnk+R2QAgEA+LnesZ0AXgKwF0AJgCoA54Fjq49Bp9Phueeea1Z71eSWYCAuDnjtNZlCgcVAhz3AdTuk28Bih8WXLpXqJSIiaoo9e/Y0ftBeJvsKgA02j8vqHS9w/H5paWmK2uUJbltdZvJk4Nw5YI7tuIy2OUC/ZUC39UCr44DOZviC0AFXugBHRwO7pwEXahdyWLQImDTJXS0nIqKWqKDAQe/dcCIY8DEAi83j+iPuTlTf6gC0BhAHYFft4XXr1mH48OHONtMt3LrU3JNPAu3bAw/NyUPFyEygaxZQFQD42wnBdAKIzAX6vQkMfA3IzUDghuV4Y1EcAwEiImq2CxcuNH7war3HP6LxSwdW1vXgBICL1V82Tpw6obxxbuaWywR1pK6EbkYidHHfSo/tBQK2qo/r4r6FbkYiRN+VKjeQiIh8QUlJSeMHz9rcLwTwHwUV2t+otkZlgHZXd3VrMLBo8yJMWTcF5VVlEH7OTS0QfmaUV5VhyropWLR5kfwLiIiIHGhsm3kAwEGb+6tQ9/JAY65zfPj0xdMKKvEMtwUDvrqQAxEReaEyAJUADkAaONiYXyHNGgCAc46rzD2g4nK8zeSWYCDvSh5mfjnTpXXO+HKGV24GQURE2iC75l4pgG0ylbwFIKf6/mWZ6gpKlTXMA9wSDGR+kQmzxYUrDgEwW8zI/CLTpXUSEZHvcHiZAJCmFzrKClidqL4td1ysstSHxwzkXMhB1vEsVYKBrONZOHjhoHzhRpw/fx6ZmZmIjo6GwWBAhw4dMGrUKGzbJoWCOp0O//rXv5yuNzY2FkuWLGlyu9ztxIkT0Ol0jufcEhG1MA4HEAKAHoC/goqslwnkFvdXcQff5lI9GFi2exkCdOrMYAzwC8Cbu99s8uvHjRuHvXv34u2338aRI0fw+eefY/jw4bh8WSbXQ0REXk9uu3kIAIEKKmrnitZ4lurBwPqj62EWrs0KWJktZnx57Msmvfbq1av4/vvv8be//Q033ngjYmJiMGDAADzxxBO47bbbEBsbCwD4zW9+A51OV/M4NzcXY8eORfv27RESEoL+/ftj48aNNfUOHz4cJ0+exOzZs6HT6eqkobZu3YqhQ4fCZDIhKioKs2bNwrVr12qOx8bGYuHChZgwYQJCQkIQExODf//737hw4QLGjh2LkJAQJCcnY/fu3XU+i5J6n332WTzwwAMIDQ1FdHQ0VqxYUXPcuiVy3759odPpahbF2LRpEwYMGIDg4GBEREQgPT0dJ0+ebNL3m4hIa2THDJghjRuQc6n61uS4mF+g+2fzK6Vqy4rKi3D8ynE13wK5l3NRXOF46WJ7QkJCEBISgn/9618oL294oWfXLmnZqFWrViE/P7/mcXFxMUaPHo2NGzciOzsbo0aNwh133IFTp6TFqj/77DN07twZCxYsQH5+PvLz8wEA+/fvx6hRo3DXXXdh3759+PDDD/H9999jxowZdd538eLFSE9PR3Z2Nm677Tb84Q9/wIQJE3Dffffhp59+Qnx8PCZMmFDzS6y03pdeegn9+vVDdnY2pk+fjgcffBCHDh0CAOzcuRMAsHHjRuTn5+Ozzz6D2WzGnXfeiWHDhmHfvn3Ytm0bpk6dKn+NjYjIS8j+P9NDtoMHUDvtUOaSgp+/doMBVbcw3nN2D/ou7+vaFtuRnZmNPh36OP26Tz/9FFOmTEFpaSlSU1MxbNgw3HPPPUhJSQEg/aKsXbsWd955p8N6kpKS8OCDD9Z0wLGxsfjzn/+MP//5zzVlJkyYAJPJhOU2uyt9//33GDZsGK5duwaj0YjY2FgMGTIEa9asAQCcPXsWHTt2xNy5c7FgwQIAwPbt2zFo0CDk5+ejQ4cOTapXCIEOHTpg/vz5mDZtGk6cOIG4uDhkZ2ejTx/p+3j58mW0bt0amzZtwrBhw5z+3nojbmFM5Fv8/PwcZwf+AmlTok8gnTo3ttbAQwDaAliI2j0NDGgwoNA/yB/ma+pkyhujdAtjVcOUcrPM0EoPv8+4ceNw5swZfP755xg1ahQ2bdqE1NRUrF69utHXXLt2DY8++igSExMRERGBkJAQHDp0qCYz0Jgff/wRq1evrslIhISEYNSoUbBYLMjLq50iaQ1EAKB9+/YAgOTk5AbPnT9/vsn16nQ6dOjQoaYOeyIjIzFx4sSazMcrr7xSk+UgIvJ2WVlZyi4TRFffd5REaGtT3spOt1RVIjNGwYNU3ZvAEGBQs3qXvI/RaERGRgYyMjLw1FNPYfLkyXj66acxceJEu+UfeeQRfP3113jxxRcRHx8Pk8mE//mf/0FFRYXd8lYWiwWZmZmYNWtWg2PR0dE19/V6fc19awrL3nMWi6XJ9VrrsdbRmFWrVmHWrFn46quv8OGHH2LOnDnIysrS9M5bRERy2rdv7/BkqIYegPUqtHb7cZdQNRiIj4yHDjoIFedT6KBDfGS8y+pLTEysmU6o1+sbjDbdsmULJk6ciN/85jcApDEEJ06cqFMmMDCwwetSU1Nx4MABxMe7rq2uqjcwUBoua29kbd++fdG3b1888cQTGDRoEN577z0GA0TklXbv3o3+/fsrf4EAIDP7sKVQ9TJBSGAIurTqouZboGtkV4QEhjj9ukuXLuGmm27CO++8g3379iEvLw8ff/wxnn/+eYwdOxaAdO3/m2++wdmzZ3HlirTyRHx8PD777DPs2bMHe/fuxe9+97sGZ9ixsbHYvHkzfv31V1y8KG1b9dhjj2Hbtm146KGHsGfPHhw9ehSff/45Zs5s3sqMrqi3Xbt2MJlM+Oqrr3Du3DkUFBQgLy8PTzzxBLZt24aTJ09iw4YNOHLkCHr27Nms9hIReUL37t2dCwQA6dKAknUGrGSS1Dq9dgdgqz60cXS30aquM3Br/K1Nem1ISAgGDhyIxYsXY+jQoejVqxfmzp2LKVOmYOnSpQCkEfhZWVmIiopC377SQMjFixejVatWGDx4MO644w6MGjUKqampdepesGABTpw4ga5du6JtW+liUkpKCr777jscPXoUQ4YMQd++fTF37lx07NixGd8B19QbEBCAV199FcuXL0enTp0wduxYBAUF4dChQxg3bhwSEhIwdepUzJgxA5mZXPWRiLzH4cOHodPpcOTIEedfLKBsNoGVTI+q5dlYqs4mAKQVCJPeSHJdi+vXPz0HPdvybJWaj7MJiFqWtLQ07Nixo+kVzII0VuB1mXLzqm8XoHbGgR7SRkc2dIE6WMqVbH/oOpqYTQAAiW0TkdElAwF+rs0OBPgFIKNLBgMBIiKq49SpU9DpdM0LBACpQ9fLlAmyuW/bz9vZhkBUaHc9YresgLD89uWqBAPLb18uX5CIiHzGrbfeipiYGNdUJiC/n4D6S+m4hVuCgbhWcXjt1tcclgkuB3rnAwNOS7fBMksHLL11KeJaxbmwlURE5K3Onz8PnU6Hr776ynWVmgFckynTQpZfUXVqoa3JqZNxrvgc5nw7p+a5nueBabuB0UeBLlfqRiYWAMdbAeu7Acv6AQdtNoJYdNMiTEqd5K6mExGRhv3hD3/AO++84/qK9ZDdlrhmXwJAGmzoaC8Dt/W4znNr054c+iTah7THi+8+hNf+XYGMXKBSB+jtpGH8AMRfAR7cBczaCWR1BWaODcQjv3+DgQAREeHq1ato3bq17AJqTSZQd0yAPa1s7sttdWCSG4DgOW7fNWHyT0DOGzrcmFe9up7M9Rjr8RvzdMh5Q4dJP2l3AAYREbnHzJkz0apVK/UCAUC6TFAkU+ZXm/syCxRVXrMzqlAj3Ju0WLQImDMHfnA+CgmwCKCsHJgyBTh3DnjySTVaSEREGlZcXIxWrVrBbHbDhj9KTuSd6d/dO6vQKe7LDKxcCcyZI19OiTlzgL//3TV1ERGRV5g7dy5CQ0PdEwgA0mUCJR24dk/4FXNPZiAvD2jmsrsNzJgB3HQTEMcZBURELVlZWRkiIyNRWupodJ4KdKg7QLAxRQAiVW6LytyTGcjMBFwdyZnNUr1ERNRivfjiizCZTO4PBAApM3BFQbnC6lu5pYs1PJtA/WAgJwfIylInGMjKAg4edPqlOp3O4Vdj2xcrERsbiyVLljT59d5s3rx56NOnj6ebQUQtQEVFBcLCwvDII494rhFmKM8MALI9aoBRu9GA+sHAsmVAgErfgIAA4M03nX5Zfn5+zdeSJUsQFhZW57lXXnlFhcYSEZESK1euhMFgQFGR3FB+lekhBQRyrEkLmbED5mIz5s+f38xGqUP9YGD9etdnBazMZuDLL51+WYcOHWq+wsPDodPp6jy3efNmXH/99TAajejSpQvmz59fZ8DKvHnzEB0dDYPBgE6dOmHWrFkAgOHDh+PkyZOYPXt2TZahMVevXsXUqVPRvn17GI1G9OrVC1988QUAaXvle++9F507d0ZQUBCSk5Px/vvv13n9J598guTkZJhMJrRu3RojRozAtWu1S2WtWrUKPXv2hNFoRI8ePfDGG284/J6Ul5dj1qxZaNeuHYxGI2644Qbs2rWr5vjq1asRERFR5zX/+te/aj7j6tWrMX/+fOzdu7fms69evVr2swLAp59+iqSkJBgMBsTGxuKll16q8z6xsbFYuHAhJkyYgJCQEMTExODf//43Lly4gLFjxyIkJATJycnYvXt3nddt3boVQ4cOhclkQlRUFGbNmlXne0RE2mI2m9G2bVtMmTLF002RCEgbFcnZo7xKk8mZbRDdSChQUFAgAIiCgoIGx0pLS0VOTo4oLS1t+MLCQiF0OiEA9b50OiGKipR8DLtWrVolwsPDax5/9dVXIiwsTKxevVrk5uaKDRs2iNjYWDFv3jwhhBAff/yxCAsLE+vXrxcnT54UO3bsECtWrBBCCHHp0iXRuXNnsWDBApGfny/y8/PtvmdVVZVIS0sTSUlJYsOGDSI3N1esW7dOrF+/XgghxOnTp8ULL7wgsrOzRW5urnj11VeFv7+/2L59uxBCiDNnzoiAgADx8ssvi7y8PLFv3z7x+uuvi6Lq78OKFStEx44dxaeffiqOHz8uPv30UxEZGSlWr17d6Pdh1qxZolOnTmL9+vXiwIED4v777xetWrUSly5dsvt9EkKItWvXCuuvUElJiXj44YdFUlJSzWcvKSmR/ay7d+8Wfn5+YsGCBeLw4cNi1apVwmQyiVWrVtW8T0xMjIiMjBTLli0TR44cEQ8++KAIDQ0Vt9xyi/joo4/E4cOHxZ133il69uwpLBaLEEKIffv2iZCQELF48WJx5MgR8cMPP4i+ffuKiRMnNvo9cPi7TESqev/99607AWjnaxYEYhWWnQcBP/lyjz32mFu/r476b1vqBgPZ2eoGAtav7GwlH8Ou+p3ckCFDxLPPPlunzJo1a0THjh2FEEK89NJLIiEhQVRUVNitLyYmRixevNjhe3799dfCz89PHD58WHE7R48eLR5++GEhhBA//vijACBOnDhht2xUVJR477336jz3zDPPiEGDBtktX1xcLPR6vXj33XdrnquoqBCdOnUSzz//vBBCPhgQQoinn35a9O7du04Zuc/6u9/9TmRkZNR57pFHHhGJiYk1j2NiYsR9991X8zg/P18AEHPnzq15btu2bQJATQD2hz/8QUydOrVOvVu2bBF+fn6NdvYMBojcz2w2i86dO3u+47f39TAEUpwIBgLkyz366KNu/f4qDQbUvUxQLreos/be58cff8SCBQsQEhJS8zVlyhTk5+ejpKQE48ePR2lpKbp06YIpU6Zg7dq1Ts953bNnDzp37oyEhAS7x6uqqrBo0SKkpKSgdevWCAkJwYYNG3Dq1CkAQO/evXHzzTcjOTkZ48ePx1tvvYUrV64AAC5cuIBffvkFkyZNqvMZFi5ciNzcXLvvl5ubi8rKSqSnp9c8p9frMWDAABxswgBNZz7rwYMH67wvAKSnp+Po0aOoqqrNz6WkpNTcb9++PQAgOTm5wXPnz58HIP0cV69eXed7MGrUKFgsFuTl5TXrMxGRa6xbtw4BAQE4ffq0p5tinwDQTrZUra7yRaz/x7VG3aGNBoOq1avxPhaLBfPnz8ddd93V4JjRaERUVBQOHz6MrKwsbNy4EdOnT8cLL7yA7777Dnq9snWn5a4ZvfTSS1i8eDGWLFmC5ORkBAcH489//jMqKioAAP7+/sjKysLWrVuxYcMGvPbaa3jyySexY8cOBAVJC2m/9dZbGDhwYJ16/f397b6fEAIAGoxxEELUPOfn51dTzqqyUn6lDbnPavse9dtjy/Z7ay1v7znr0qQWiwWZmZk14zlsRUdHy7abiNQjhEC3bt0aPUHRDB2AeAAbFZZXcF7q8UGRjVA3MxAfDzgYROcSOp30Pi6SmpqKw4cPIz4+vsGXn5/07TKZTBgzZgxeffVVbNq0Cdu2bcP+/fsBAIGBgXXOaO1JSUnB6dOnceTIEbvHt2zZgrFjx+K+++5D79690aVLFxw9erROGZ1Oh/T0dMyfPx/Z2dkIDAzE2rVr0b59e1x33XU4fvx4g/bHNbJAU3x8PAIDA/H999/XPFdZWYndu3ejZ8+eAIC2bduiqKiozgC8PXv21KnH3meX+6yJiYl13heQBv4lJCQ0GrwokZqaigMHDtj9OQYGBja5XiJqnm+//RZ+fn7aDwQAoAzyGxXZUtDdbdq0qYmNUZe6mYGQEKBLF0DNH3rXrtL7uMhTTz2F22+/HVFRURg/fjz8/Pywb98+7N+/HwsXLsTq1atRVVWFgQMHIigoCGvWrIHJZEJMTAwAaeT75s2bcc8998BgMKBNmzYN3mPYsGEYOnQoxo0bh5dffhnx8fE4dOgQdDodbrnlFsTHx+PTTz/F1q1b0apVK7z88ss4e/ZsTce8Y8cOfPPNNxg5ciTatWuHHTt24MKFCzXH582bh1mzZiEsLAy33norysvLsXv3bly5cgV/+ctfGrQnODgYDz74IB555BFERkYiOjoazz//PEpKSjBpkrRDpPXz/vWvf8XMmTOxc+fOmtkCVrGxscjLy6u5NBAaGir7WR9++GH0798fzzzzDH77299i27ZtWLp0qezsBzmPPfYY0tLS8NBDD2HKlCkIDg7GwYMHkZWVhddee61ZdROR84QQ6NOnD/bt2+fppij3OoBgSD2lkqvB8QBkrkKWlZU1u1mqaO4ABNlBVzNnChEQoM7AwYAAqf5msDcw7quvvhKDBw8WJpNJhIWFiQEDBtTMGFi7dq0YOHCgCAsLE8HBwSItLU1s3Lix5rXbtm0TKSkpwmAwCEff3kuXLok//vGPonXr1sJoNIpevXqJL774oubY2LFjRUhIiGjXrp2YM2eOmDBhghg7dqwQQoicnBwxatQo0bZtW2EwGERCQoJ47bXX6tT/7rvvij59+ojAwEDRqlUrMXToUPHZZ5812p7S0lIxc+ZM0aZNG2EwGER6errYuXNnnTJr164V8fHxwmg0ittvv12sWLGizmcsKysT48aNExEREQJAzYwAR59VCCE++eQTkZiYKPR6vYiOjhYvvPBCnfe1NygTgFi7dm3N47y8PAFAZNsMJt25c6fIyMgQISEhIjg4WKSkpIhFixY5/B5wACGR623fvt3zgwGb+mWAQLLCAYTD5Mv5+/u79XuvdAChTgg7F2jrKSwsRHh4OAoKChAWFlbnWFlZGfLy8hAXFwej0djwxTk5QFKS3Fs0XU4OUH1GTNQcsr/LROQUIQQGDx6M7du3e7opTdcewG8ALJMpN6/ebSP0en3N+C93cNR/21J/0aHERCAjw/WrEAYESPUyECAi0py9e/fCz8/PuwMBQNp3oIPrqlMy8NoT3LNR0fLl6gQDy5e7tk4iImoWIQRGjBjRcvYpUbKFsS2ZyW2hoaFNboqa3BMMxMUBrh60tXQpty8mItKQw4cPw8/PD998842nm+I6EVC2c6GVzEBDj+y+qIB7ggEAmDwZWLjQNXUtWgRUj3InIiLPu/POO9GjRw9PN8P1CgH8rKCcdfkAmVF4zi5S5y4uy91bF3tx6MkngfbtgZkzpU2GnPmmBARIX0uXMhAgVSj6HSaiOvLy8hAfH99y/34sqO3o5coBssGAVjU7GAgMDISfnx/OnDmDtm3bIjAw0OFufbjvPuhuuAEBM2bA/5tvIAICoHMQFFiPVw0bBvPSpRCxsYBW52mSVxJCoKKiAhcuXICfnx8XJSJS6L777sO7777r6WaoKwKAkqVsrP82ZIIBpSvVuluzgwE/Pz/ExcUhPz8fZ86cUf7C115D4LFjaPXhhwjZsgX6X36BzmaWo9DpUBkVheIhQ3DlnntQ0bWrtLoA15UnlQQFBSE6OrpmpUkisu/06dOIjY2VXW21RSiEsgWHKgGYIK1C6CAg0OpsApdcJggMDER0dDTMZrNzvxxxcUBGBiwAyouLocvNlTYdMhggqlcWDIJzq0ESNYW/vz8CAgIcZ7WICJmZmVixYoWnm+E+YVDWU1pP+I0AHIwRdDTX35NcNmZAp9NBr9c3PQViNAJ2lu4lIiLPO3/+PKKioty6YI4mFELRngM1mQGZb49WMwPMhxIRkUMPP/ww2rdv73uBACANDIxRUM46rkBmHKVWgwF1NyoiIiKvdfnyZXTu3Fmzc+PdIgKA3K7nIag9tZYZQKjVcRbMDBARUQNPP/00Wrdu7duBACBdJjgtU6YFLK/AzAAREdUoKChAVFQUioqUTK73ARYAJ2TKHAZwu7LqFOwN6BHMDBAREQDgxRdfREREBAMBWxEAzsuUsf12mRwXDQlRsmiB+zEzQETk465du4aoqChcueLMIvw+ohDS9EI55yBtdywzxpJjBoiISHPefPNNhISEMBBojAXKlhi2LowrU1aryzYzM0BE5INKS0sRFxeHc+fOebop2hYBaQ0BOdYgQKav1+r0TGYGiIh8zD//+U8EBQUxEFCiELWrCzri5YuXMjNAROQjysvLER8fj9On5ebKUY0wSNkBOQonCXA2ARERecwnn3wCo9HIQMBZhQC6KihnHTMgM5sgODi4mQ1SBzMDREQtWEVFBRITE5Gbm+vppngnC4B4BeVaV9/6Oy4WEKDNbpeZASKiFuo///kPDAYDA4HmiICyMQNtq2/LbJ4zNCym1S3StRmiEBFRk5nNZvTu3Rs5OTmebor3K6y+9QegZIkAs8398oaHr1692uwmqUGbIQoRETXJf//7X+j1egYCrmIBcBUtvrdkZoCIqAWoqqpC//79kZ2d7emmtCwRAC5A2VoDCnA2ARERqWLr1q0ICAhgIKCGQthN9zfKzjgBW0ajsTmtUQ2DASIiL2WxWJCeno709HRPN6XlCgNwUaaMH2rHE8j0qlodQKjNVhERkUO7d+9GQEAAtm7d6ummtGyFAPbLlLEAsA7RsM0i2JmFwGCAiIiaTQiBESNGoH///pq9/tyiWKDsMsEpm/JWdsYZXLt2rfltUgEHEBIReYn9+/ejd+/eDALcKQJ11w5oTCuV26EyZgaIiDROCIExY8YgJSWFgYC7FULZTIJLyqrT6s+PmQEiIg07fPgwkpKSUFWlZMUbcjkLZGcIAKgdQGgCUNp4MYNBSWXux8wAEZEGCSHw29/+Fj169GAg4EkRAMIVlGtffSuzlXFYWFjz2qMSZgaIiDTm+PHj6N69O8xms3xhUlchgB4AfpEpZ92tsMRxMS5HTEREDgkh8MADD6Br164MBLTCAqC3gnLdlFVXWemipQxdjJkBIiIN+OWXXxAfH4+KigpPN4VsRaB2zIAf6k4dtBWsrDqdTuY6gocwM0BE5GHTp09HdHQ0AwEtKgRg/bE0Fgg4gbMJiIiojvz8fHTp0gVlZUomspNHhMG502bOJiAiIqX+93//F506dWIgoHWFkJ0hUAdnExARkZwLFy4gLi5Os8vSUj0WAEFOlHeQFQCAgoKC5rRGNcwMEBG5yVNPPYV27doxEPAmEZAfK2B7Wi0zJECr40KYGSAiUtmVK1cQGxuLwsJCTzeFnFUI2Q4eA9zREHUxM0BEpKLnnnsOkZGRDAS8lQW1Sw035rI7GqIuZgaIiFRQWFiIuLg4XL7cAnoKXxYB+cyA7TpCnE1AREQA8OqrryI8PJyBQEtQCKBIpsx5m/syvSpnExARtXDFxcXo2rUrzp8/L1+YvIMFgL9MGdvMgcxMUa1eLmJmgIjIBVasWIHQ0FAGAi1NBOSnFpbb3Je5pMC9CYiIWqCSkhJ069YNZ86c8XRTSA1KLhPY9u8ywYDF4oI1jVXAzAARUROtWbMGwcHBDARasjBIAYEc66BBmWBAr9c3s0HqYGaAiMhJ5eXl6NGjB06cOOHpppDaCgGcVVDuHIBY+WJavUzAzAARkRM++eQTGI1GBgK+wgLgqoJy1jIyJ/6hoaHNao5amBkgIlKgoqICycnJOHLkiKebQu4UAeCEwrLbUHf8gB0cM0BE5KXWrVsHg8HAQMAXXQFgVlDOH4CCoSPl5eXyhTyAmQEiokZUVlYiNTUVP//8s6ebQp6iJBCwUrDGFDMDREReJCsrC4GBgQwEfJ3SXlLhqXVQkDP7IbsPMwNERDbMZjPS0tLw448/eroppAVKMwOhysoWFxc3pzWqYWaAiKja5s2bERgYyECAaintJVsD6CBfLDk5uTmtUQ0zA0Tk86qqqjB06FBs3brV000hrVF6id8ERbMOxo8f34zGqIeZASLyadu2bYNer2cgQA11BxCuoJw1I3BNvuioUaOa0SD1MBggIp9ksVhw4403YvDgwRBCbsN68knRAKIUlBtXfasgi3D8+PFmNEg9DAaIyOf8+OOP0Ov12LRpk6ebQlpWAqCHgnJtq28VBANlZTJ7HHsIgwEi8hlCCNx2223o16+fZud7k8b0cm11EydOdG2FLsIBhETkE/bv34++ffuiqqrK000hb+HskgAm1O5eaIefUbvn39ptGRGRCwghcNdddyElJYWBADmnBNJyxEpVOD4sqrQ7NoWZASJqsQ4dOoTk5GSYzc6sKUtk45CCMkWQFh2S6eu1PFCVmQEianGEELj33nvRs2dPBgLUdEGQ3YUQQO1ptdwwFA3/KjIzQEQtSm5uLhITE1FRIZOzJZJjBqDkylIlpPECXoyZASJqEYQQmDRpEuLj4xkIkGsEQNkps17thqiPmQEi8nqnTp1CQkKCZveKJy9VUu+xDvbHBVgzA3KzCQK1e/6t3ZYRESnw4IMPIiYmhoEAqa+x8X/W5+V6VA33uMwMEJFX+vXXX9GtWzeUljo4FSNqjiBIyxH/IFMutPrW9lfRAKBefGqp0u5CVxqOU4iI7PvLX/6Czp07MxAgdZVA2p/AkWDU9qS2fb29RJWSmQkewswAEXmNc+fOIT4+HsXFxZ5uCvmKApnjndzSCtUxM0BEXuGvf/0rOnTowECA3CcIQK5MmRYyVIWZASLStIsXL6Jr164oLCz0dFPI1yhZjvi0zX3OJiAicr2FCxeibdu2DATIc+R+9SwArMkqf8dF/fy12+UyM0BEmnP16lV06dIFV644s0sMkYsFQdllgIsAQgCU2TynR4MBg8KfexMQESnywgsvoFWrVgwEyPOU7iWgs1PezsyBqmLt7prJzAARaUJBQQG6deuGCxcueLopRJIAKFtqWLsn/IoxM0BEHvfaa68hIiKCgQBpSwmACAXlyuSLaB0zA0TkMcXFxejWrRvOnj3r6aYQ2XcdgN0yZaw7FobB8YBDDfe4zAwQkUesWLECoaGhDARIu4IgBQNyrMGAzJAAvVG72xtqOE4hopaopKQE3bt3x+nTp+ULE3lSCYDWCsoFV99ec1ysslS76xEzM0BEbvP2228jODiYgQB5D5m1AwDUBgNytDuZgJkBIlJfWVkZEhMTkZeX5+mmECkXBOAqpJ5S6TRDL8XMABGp6oMPPoDJZGIgQN6nBMAFtPhAAGBmgIhUUl5ejuTkZBw9etTTTSFqukAnysrsTaAL0DV+0MOYGSAil1u7di2MRiMDAfJuQQDkfoV1qB0LINPX64M4m4CIfEBlZSX69OmDnJwcTzeFqPnMAI7LlBEADgFIgnRZwYGK4gqXNEsNzAwQkUusX78eBoOBgQC1HAFwmPavoaQMIO1wqFHMDBBRs5jNZvTr1w979+71dFOIXKsEyjr6fLUboj5mBoioyTZu3IjAwEAGAtRyKbnMr+H1A5RiZoCInFZVVYVBgwZh165dnm4KkXqCABgAFMmUa1d9KzObQMs9LjMDROSUzZs3Q6/XMxCglq8Eys76r1TfyvSoepN2ZxMwGCAiRSwWC2644QYMGzYMQrSADdyJlOigoEyb6luZrYzNpdpdvYjBABHJ2rp1K/R6PX744QdPN4XIfYIADKy+72iPgl7VtzIxsjBrN4hmMEBEjbJYLLjxxhuRnp4Oi0XD86KI1GBG7SZEjn79rWW029fL0vBwBiLypF27dmHQoEGoqmoBQ6WJmiIAQHn1fSUdvVwZJTsgeggzA0RUh8ViwS233IIBAwYwECDfVoLaYMAVNPznxMwAEdXYs2cP+vXrxyCAyCrIibIGOAwe/A3aTQ0wM0BEEEJg7Nix6Nu3LwMBIqsgOHfKLDNZoKpSu39bzAwQ+biff/4Zffv2hdms3WlPRB5RAvn9BGx7UbkxAxoeg8vMAJEPu/vuu5GcnMxAgKgxBpnjw23uczYBEXmTQ4cOoXfv3qio0O6WqkQeFwTHp8xGAB1tHnM2ARF5i9///vfo2bMnAwEiOSUAzjo4XgbgXzaPdTL1aXfIADMDRL7i2LFj6NWrF8rLXTlXiqiFkzubtz2lNsLhRkX+Ju2mBpgZIPIBDzzwALp168ZAgMgZQZCfWhhmc18m2WYxa3cEITMDRC3YyZMn0bNnT5SWOtpXlYjsMkN+++Jfbe7L9PXcm4CI3G769OmIjY1lIEDUVEpOl20DALm+XruxADMDRC3N6dOn0aNHD1y7ds3TTSHybiUAChWUKwVgUlBOboChBzEzQNSCzJ49G1FRUQwEiFylQEGZiwrrYmaAiNSUn5+P7t27o6hI7gInESkWBOAXBeWsf3YmOJxN4GfU7vm3dltGRIo89thj6NSpEwMBIlcrgfwAQqB2FoHc0h3anUzAzACRtzp//jwSEhJQUKAkj0lETaJkoSBrGZnLABaLdqMBZgaIvNDcuXPRvn17BgJEagoCoGT4jXUtIbm+XsNbgDAzQORFLl26hG7duuHKlSuebgpRy1cC+dQ/IL+ZkRdgZoDISzzzzDNo06YNAwEid1IyAyBU9VaojpkBIo27cuUKEhIScPGi0vlLROQSQZB6SbnsQOvqW84mICI1PP/884iMjGQgQOQJfgACFZSzLjgkM9hQ56/dVYeYGSDSoIKCAnTv3h3nzp3zdFOIfFc8gN1OlJcJBiyVnE1ARAq98soriIiIYCBA5GltAUQ4UV4mGBAV2l2CkJkBIo0oKipCz5498euvv8oXJiL36Asg19ONUB8zA0Qa8OabbyIsLIyBAJHW9PJ0A9yDmQEiD7p27RoSExNx6tQpTzeFiJpLbjZBoHbPv7XbMqIWbuXKlQgJCWEgQKRlDjr3BvwdH/bz126Xy8wAkZuVlpYiKSkJeXl5nm4KEclR8mdaDmkVwjKb5/QAKusWE/7aHUCo3TCFqAX65z//iaCgIAYCRN7irIIy5dW3tnsPVDYsVlWsZNcjz2BmgMgNysrKkJKSgqNHj3q6KUTkDCW9pF71VqiOmQEilX3wwQcwmUwMBIi8Ubl8EXtZAG/DzACRSsrLy9G3b18cPHjQ000hoqZS0tFbhwKEASh0UE7DPS4zA0Qq+PTTT2EymRgIEHm7VgrKWPcvkBkSoDdq93qChuMUIu9TUVGB66+/Hj///LOnm0JErhChoIx1/6FrjotVlmr3egIzA0Qu8vnnn8NoNDIQIGpJOiooY1BYl3YnEzAzQNRclZWVGDhwILKzsz3dFCJyNbldhzsqKOMFmBkgaoYvv/wSBoOBgQBRS5UvczxQ5riXYGaAqAnMZjPS09Oxc+dOTzeFiNR02onjMnsTaLnHZWaAyElZWVkwGAwMBIh8wXmZ41UAiqvvy1wu0Js4m4DI61VVVWHo0KHYunWrp5tCRO5SoaBMCYCQ6lsHKq9xNgGRV9u8eTMCAwMZCBD5GiV7C8kEATUszWmIuhgMEDlQVVWF4cOHY9iwYbBYNPyXTETqUJI/L5MvonW8TEDUiO+//x7Dhw9HVZWGJwcTkbpCFJQxqd4K1TEzQFSPxWJBRkYGhgwZwkCAyNe1VVDGeilBLijQ8Ok3gwEiGzt27IDBYMDGjRs93RQi0oJ4BWWsswhkelQtzyZgMEAEKRswevRopKWlwWw2e7o5RKQVSjIDbapvZcYOmEu1+79Fw0kLIvfYvXs3Bg0axCCAiBryV1AmuPpWZoyxMCuZmuAZzAyQz7JYLBgzZgz69+/PQICI7LsK5T2ldvt6WcwMkE/au3cv+vfvj8pK7S4CQkQaUAYpO9DCZxYzM0A+RQiBcePGoU+fPgwEiEheEQCliUO502sNn35ruGlErnXgwAGkpqaiokLJ+qJERADKoTz9r4fDwCEgULtdLjMD1OIJIXDvvfeiV69eDASIyDk5MscNqN2/QObfi7lMu2OTtBumELnAoUOH0KdPH5SXl3u6KUTkjY7LHC8H8F8At0A+g6DhcQfMDFCLJITAhAkT0LNnTwYCRNR0SnpJ6/oCnE1ApB1HjhxB7969UVbWAnYPISLPMkJ+V8J21bdywYCSNQs8hJkBajGEEHjggQfQvXt3BgJE5BpKEouXqm91DksBGt7qhJkBahGOHz+O5ORklJQo3ViciEgBJWfz1k7eCKDUQVUm7aYGmBkgryaEQGZmJrp27cpAgIhc7zoFZTpW38rMJrCYtTuCkJkB8lp5eXlITk7GtWvXPN0UImqpBgM4COnUubG+vFf1LfcmIHIfIQRmzpyJLl26MBAgInVZNyFy1I8rKaPkuAcxM0Be5ZdffkFiYiKKi4s93RQi8gWFLqxLboChBzEzQF7jL3/5C6KjoxkIEJH76KtvXXFWz8wAUdP9+uuvSExMRGGhK0N0IiIFnNnPzASHswn8jNo9/9Zuy4gAPPbYY+jcuTMDASLyDIMTZeW2PtHuZAJmBkibzp49i549e+Lq1auebgoR+TKTE2VlLgNYLNqNBpgZIM2ZO3cuOnbsyECAiDxPbqPBbjb35fp67W5ayMwAace5c+eQmJiIy5cve7opREQSuVPmgW5pheqYGSBNmDdvHjp06MBAgIi05YrM8XVuaYXqmBkgjzp//jySkpJw8eJFTzeFiKihIJnjbWzuczYBkfOeffZZtG/fnoEAEWlXkczxszb3ZXYl1Plrd9UhZgbI7S5fvoyePXvi/Pnznm4KEZFjchsN2k4nlAkGuDcBUbXnn38erVu3ZiBARN5Bbp0B20WJZIIBS7l2pxYyM0BuceXKFSQmJuLs2bPyhYmItELJeUslapct9lLMDJDqlixZgsjISAYCROR9ShSUuap2I9THzACp5urVq0hKSsKZM2c83RQioqY5raDMBQBtIT+bIFC759/abRl5taVLl6JVq1YMBIjIuynJDFjLhDouZqmw4Lnnnmtui1TBzAC5VGFhIZKSknD6tJJwmohI45QsIXwCQD8A1+SLpqWlNa89KmFmgFxm2bJlCA8PZyBARC2HkoGBh6tvy+WLrlunzSULmRmgZisqKkJycjJOnjzp6aYQEbmWUUEZ6/RCBWsKafVkiZkBapa///3vCAsLYyBARC2TzDiAOhSsKRQWFtbkpqiJmQFqkuLiYqSkpCAvL8/TTSEiUs91AHYrLNsK0swCB4qK5NY39gxmBshpq1evRmhoKAMBImr5rnOibBv5Ijk5OU1uipqYGSDFrl27ht69eyM3N9fTTSEico/WTpStlC9y5YrcnsiewcwAKfLOO+8gJCSEgQAR+RZ/yG9jbKVgsKFWV2JlZoAcKikpQd++fXHkyBFPN4WIyL0OAOgGIAXAdgXl/wfAz46LWCza3KyImQFq1AcffIDg4GAGAkTkmz4GcBFAH9dVqdVggJkBaqCsrAx9+/bFoUOHPN0UIiLPKgEQ70R5mf0JDAa5PZE9g5kBquPjjz9GUFAQAwEiIkAaL+Cgc29AZuEhrjNAmlZeXo7rr78eBw4c8HRTiIi0owjKtiguB2CA7MZGV68qqcz9mBkgrF27FiaTiYEAEVF9/gCUTABQsC8BAFRWKph/6AHMDPiw8vJy9O/fH/v37/d0U4iItCkIynpKJRsaAdDpFGxg4AHMDPiodevWwWQyMRAgInKkCMq2MVZ4wi+Egg0MPICZAR9TUVGBgQMHYs+ePZ5uChGR9vnDucwAZxOQ1q1fvx5Go5GBABGRUkFwLjMg06tyNgF5TGVlJdLS0vDTTz95uilERN6lBICS/tuaGShzXKywsLCZDVIHMwMt3FdffQWDwcBAgIioKYKgbNEh674EMgsMcjYBuVVlZSXS09Oxa9cuTzeFiMh7FQHoJFOmI2oXG5IZH6jV5YiZGWiBsrKyYDAYGAgQETWXP4DTMmVC3dEQdTEz0IKYzWakp6dj586dnm4KEVHLEAQgR6bMRZv7AXA44JCzCUhV//3vfxEYGMhAgIjIlYpQt7O357LNfZnFh4KCgprZIHUwGPByZrMZgwcPxs0336zZxSyIiLyWP2RnCAAAzlXfVjguVlRU1MwGqYPBgBfbtGkTDAYDtm3b5ummEBG1TEGQnSEAALhSfStzTmY2K1m0wP04ZsALVVVVYdiwYfjhhx883RQiopatBLLbEgOQVh4EZIMBrWJmwMts2bIFgYGBDASIiNwhCLUdvSOi3m0j9HqFOxq5GYMBL1FVVYWhQ4di6NChmp2nSkTU4ihZZwBQNq4A2l10iMGAF/jhhx8QGBiILVu2eLopRES+xR9AgoJy1uyBzMzB0FBtLkrAYEDDLBYLhg8fjhtuuIHZACIiTwgC0FZBuTbVtzLjA0tLHWxp6EEMBjRq+/bt0Ov1+O677zzdFCIi31UEKTsgJ7j61ktnEzAY0BghBG6++WYMGjSI2QAiIk+zBgJKFw700tkEnFqoITt27MDgwYMZBBARaUUQgFOQFhPyB1AlU56zCaiprNmAtLQ0BgJERFpSBKAQUicvFwgAsmsSaHU2ATMDHrZr1y4GAUREWuUPwOhEeSMAB2MEw8LCmtkgdTAz4CFCCGRkZGDAgAEMBIiItCoIgNyK7/6o3ZNAZm8CrWYGGAx4wI8//oiAgABs3LjR000hIiJHSgCckSlTBWBT9X2ZczsGAwQhBEaOHIl+/foxG0BE5A2CoGyGQEj1rUzZqiolAw/cj2MG3CQ7O5tBABGRtymC7EJCAIBLajdEXcwMqMyaDUhNTWUgQETkbfwBKJkNqPCEXwhtLkTAzICK9u7dyyCAiMibBQEIVFCuXfWtCQ5nE4SEhDR+0IOYGVCBEAK33HIL+vTpw0CAiMibFUHZafPV6luZ2QRaHTPAYMDF9u/fD39/f3z99deebgoRETWXP4B0BeX6V9/KXAXQ6gkigwEXGjVqFFJSUjR7TYiIiJwUBOC66vuOxg5YdzaU6esrKmRSBx7CMQMucODAAaSkpGg24iMioiYqQm0Hr80lAlyCmYFmGjVqFHr16sVAgIioJfKHS3ci1GrmmJmBJjpw4ACSk5M1+4MlIiIXCILiaYMAZGcTBAcHN7NB6mBmoAluvvlm9OrVi4EAEVFLVwJlUwut/B0fDgjQ5jm4NlulUcwGEBH5mCA4d9pcZnPfAKC87mE/P22eg2uzVRp04403MhtARORrbAcQNuY6m/u2SxeX1y8IXL16tdlNUgMzAzJycnKQlJTk6WYQEZEn+EPKDjjSxw3tUBkzAw4MGTKEgQARkS8LAlAsU+ag8uq0ml1uUZmB4mLg2DGgvBwwGID4eKApy0AfPHgQiYmJrm8gERF5lyIAOpkyF2zu2xknYMtoNDa/TSrw+mAgJwdYtgxYvx44fhywDbp0OqBLF2D0aGDaNEBJ/56WloYdO3ao12AiIvIe/pAPBmzJ5Ns5gNDF8vKAkSOBpCTgtdeA3Ny6gQAgPc7NlY4nJUnl8/Ls15eTkwOdTsdAgIiIagVBfsyALdusgJ3lixkMuNDKlUCPHkBWlnOvy8qSXrdyZd3nU1JSODaAiIgaKoF0qcAR2+O2Mw/sLF987dq15rdJBV53mWDRImDOnKa/vqICmDIFOHcOGDv2ZyQnJ7uucURE1LIEAShQUK4Sjjcy0jivygysXNm8QMDWnDlAcvJi11RGREQtUxGA0wrKXVVWnVZnE3hNMJCXJw0CbGglpNEd9acNCACvAugBaXhnRwAPArhiU2Y5gFhXN5WIiFqKDwFkKyj3OoAXIbscscFgaH6bVOA1wcDddwNVDTaL+BXArOr79a/DPAzgz5DCNX8AhQDeAjAAtRdy/CH9pImIiOywQBo3oEQ5ZDc1CgsLa2aD1OEVwUBODrB7t70jE1F3IWirXwEshpQdOAdpC6kSSD+lYwAiq8vpIAUHPVzbYCIi8j12BgzWV1FRoX47msArgoFHH7X37DsAvoX9jaa3y9RY/yf2tya0ioiIyDklJUrTDO7lFcHA11/Xf+Y8pOv/jeVjbCMv65ZTjlaNuLXJbSMiIi/nxlkAHEDYREVFgNlc/9lJaDhGwFZ3m/ulkC762P4A6gcRAQCCm9pEIiLyZgrS+65ibtihaYLmg4Ft2+o/8yKAL2D/8oCV7XJRSqIw69gBIiIi9bRv397TTbBL88HAxo22j4oBPOWg9JOQMgaXmvBOI5rwGiIi8nrOXiaQu/LsQLdu3Zr2QpVpPhg4fNj20UlIaf/GPAvgHgAnZGq1l6bpbuc5IiJq8Zy9TFD/yrMcm8Bh5MiRTr6Ze2g+GCit0/dfaaxYvTK7FJSrv2ORNreVJCIilTmbGVCaFdDZ3LaS7qakpDj5Zu6h+WCg7sDLfAWvOAgpg2CldPsFbY7wJCIilTmbGVDaXVjLWVBzLrut4UA4TdB8MFB3t0eZdR4BSDMFbLeNamzk5rf1HjMYICLySc5sUayH8nPMYEjdVjiAYcCIf4zAc88952zr3ELzuxbWzQyEK3hFHICbAXwuU25gvceaj4uIiEgNdwJIqL4/T6ZsLICRkPYikDMBgM3kAb8A7fYzmg8GAuq00NHgQauT8kUAAF3rPdbm3E8iIlKZzH4CdRwF0E5h2ci6D/11SrLbnqHdMKWasc64vmgFr2iLhjsY2q253mMlgQYREbU4zgQDQN1Fbh2pNzDRpDc5+Ubuo/lgoO5sggRIF2EcJTTuAXBRQc31ZxNo94dEREQqcvaEvYm7EJeatXvSqflgwGI7FhBGyF/Z0AM4raDm+vVo/ltBRERqcHYBoSZuPFhlcTYF4T6a7wEb7ulwDY6v75ugbApi/Yo1/60gIiI1OPvvX0kwENiUhniO5nvAhsGA3BTAi1C2ONE3TtZLREQtkkW+SB1KrkTbCRiEhvsZzQcDfg1aqOSbeYOCMhlNqJeIiHze9U17ma6pGxq4geanFjbMDIQAKHTwit4AShTU3LneY83HRUREpAZn/v0HQNnENi+j+R6wYTAgNwDjZjRtNgEzA0REPsn2MkGwTNnBUD4V8XLdh7xM0AwNLxM4WoXQBGmdgabMJtDuD4mIiNxEbs+6oVC+tl29/kvLlwk0Hww0zAw4muD5u+pbziYgIiKFbP/9X3VQzh/SeWQT1xnQMs33gAENRjWcclD679W3ZQpqrj+bgMsRk+9pB+BuAH+svlW6yipRi1LVyH175SwAIhTWe7zuQy0vR6z5AYTGBikbfzj+aflBWcq//mwC7a4MReRKtwN4BkAipCW6bBOXAtJurjkA5gL4wu2tI/IAZ9YC+gjAKIVl622Bw+WIm6G0QR8tt++A0mv/9WcTaPeHROQKNwC4AGAdpDk3gWi48Jqu+vne1eUuQNlEXSKv5swJ+zkAlxSWrTfEjcsRN4OlwWIQSqYNKlFc77HmvxVETbYawGYArasfyw1jsh5vXf261aq0ikgjnBnXV4wGswTsstOlcDniZmg4gDBI5hUZAG5TUHP90E7z3wqiJtkA4P7q+86OZbaWv7+6HqIWyZl//xYoW2rY2VUNPUzzPaDz6wxkAfiPgprrL1nMqYXU8qxG7eiYpk5qsr4uA8A/mtsgIi1ypuMWULbivZ0/OK4z0AwN1xm4Ctc0O7veY+3+kIiaaiKk/0m2X/+r8LUnqsuvrn7MvxAiSIFDgYJydv5gtLzOgOZnEzTMDABAGBxPBlWi/mwCzcdFRE5rA2nws+3w2E4KX9sRwDbUDojW7r8xomZy9t9/FwB7VGiHB3lpMKAkLHMkCg1nE/C8h1qO3gD2QlqY+wyA3zehDgOAtHrPWQOC4QA2NbFtRJpje5kgDI63vwFqR+I6iZcJmqHhZQLA8TnKdTI16gDcaud57f6QiJx1l839kwDmAxgIIBLS5NzWANpDGo57HYA7AOwH8F9IHX1rSMGADkA/NJzDc0K1lhN5WD8FZeov0NGY8roPtXyZQPPBQOOXCRrzq1yNAI7aeV7z3woixWwvBURCWghtMoD3ASwA0BPShbZ7ALwOKUU4AFKYHAhpoODb1a83oHZr9h3Vt1x7gFoU23//QyHfHZRD2fmjq2bCu4HmLxM0XI4YAIocvQLST7LCQRl7Q0e5HDG1DO0A/Gzz+MHq239W35YDmAXg/wC8AmAFpMm4MZB29XgB0mWGE9Xlp6B29VVrGL0A0lTD865uPJEn1J+kFgjHq9pHoPZanCP16tDycsSaPx1u08bes2ZIYZn1q3W9Y/YCgbE25TfZOa5kFQki7TsPqZMHgDkAdgF4A0B/SLuzGiBlOZ+CtNqGHtL/vnxImc+pkLIC9nYB6VJ9OwtAN3WaT+R+9Tt+6zmlrfrrdivJ+NfbM6+NyW6HpgmaDwZiYpSUEpD+pVnF1zt+L4CFMnUo3ZOSSNvGAFhcfX8spDzYTEgX1/4B4H8g/eEPri6zFlLA0Lv6qx2AhwAMqz5uu9iQ9TW/AtiqTvOJ3O9qvceVkJYo9ofU6fuj7mUBM2ojY0fq7U0QE6GoQ/MIzV8mGDYMWCjXj6MrpH9nVseqb62bGv0eQC+HNTz++A3o1eudJraSyHM2b96MFStWYMGCBejSpQuit21D7uuv1xz/AFKo/AWkrdqnAZgAaeDgVgB9AMRCmnkQD2lPgioAn0MaiPg+pADjHpv3/AxA8bx5+DYyEsuXL8eRI0ewdu1adOzYUdXPSqSGHRd2YPr26bVP6CFd77deUa6CFBRYAwIT5IMBPRrsTTAsbpjdolqg+WAgrf7cJrus80BiADwKaTLVItReCLpFtoYnnxyBELk9kIg0qLKyEitWrMCtt96Kfv36ATffXCcY0EH6Q/e3eewPYI1NHf+BdLZvzan5A+hrc/wn1A0GAKBXZiZ6deiA6Oho3HnnndDpdEhNTXXdByNyk4SKhLrBgDUzoIMUEPihtjvRQQoGilAbIPSBtPHtYZtK7QQLaZ0VdWgeoflgICQE0OuBykolpU9CSnDWtxbAaDS2r4FeDwYC1HJ06CDNya3e5es2AC8D+B2k8QBJkC4XWEfaLAfwdwCtIG1d/DaAaNS9cDai+nZV9e06Pz/0PnIEZzdvxv/93/8hPDwc/fv3V/FDEaknJDAEej89Ki3VHY0ZdQcV2t6/ufq2DLWZgj12Kq03rVDvp0dIoHY7Gs2PGQCAUbJ7Rx+CFKaFNnJ8PByNe5avn8jLdKqdXHgTpM5/P6T1BE4BuB61w2w3Qkr7x0L63/Y0pCmGs6uP/wnAyOr7CdW3C3Q6ZGRkYPbs2UhISMCWLVvQtm1b1T4OkdpGdbXpCGLR8FQ5BMCdqJ1X2xa1O4DZU2/qYZ36NUgnhP2Z/LYKCwsRHh6OgoIChIU5muOvjpwcIClJ3fp79lSvfiK3+/xzYOxY9epftw64/Xb16idys5wLOUh6Q72OJmd6Dnq2dX9Ho7T/9orMQGIi0E/JqlBN0K8fAwFqgcaMASIj1ak7MpKBALU4iW0T0a+jOh1Nv479PBIIOMMrggEA+OijxpYmbjo/P6leohZp7VrvqpfIwz4a/xH8dK7taPx0fvhovPY7Gq8JBuLigOXLXVvnihVSvUQt0tChwIQJrq3z/vuleolaoLhWcVh+u2s7mhW3r0BcK+13NF4TDADA5MlK1hxQZtEiYNIk19RFpFlvvw2MGCFfTomMDGD1atfURaRRk1MnY+GNruloFt20CJNSvaOj8apgAACefBJ46y1pOmBT6PXAypXAX//q2nYRaVZWVvMzBPffD2zYIF+OqAV4cuiTeOuOt6D3a1pHo/fTY+UdK/HXId7T0XhdMABIGYLDh6UTFWdkZEivY0aAfM7bbwPffef8oMLISOl1zAiQj5mcOhmHZxxGRhfnOpqMLhk4POOw12QErLxiaqEjOTnAsmXAl18Cx441PB4fD9x6K/Dgg5w1QARAmnY4dy5w8KD91bz0eumPZdEizhoggjTtcNnuZfjy2Jc4drlhRxMfGY9b42/Fg/0e1NysAaX9t9cHA7aKi6WAoLwcMBikQIArCxI5cPasdOZfXCz9sQwbJq1gSER2FVcU49jlYyg3l8MQYEB8ZLymVxb0yWCAiIiIarWoRYeIiIhIPQwGiIiIfByDASIiIh/HYICIiMjHMRggIiLycQwGiIiIfByDASIiIh/HYICIiMjHMRggIiLycQwGiIiIfByDASIiIh/HYICIiMjHMRggIiLycQwGiIiIfByDASIiIh/HYICIiMjHBSgpJIQAABQWFqraGCIiInIda79t7ccboygYKCoqAgBERUU1s1lERETkbkVFRQgPD2/0uE7IhQsALBYLzpw5g9DQUOh0Opc2kIiIiNQhhEBRURE6deoEP7/GRwYoCgaIiIio5eIAQiIiIh/HYICIiMjHMRggIiLycQwGiIiIfByDASIiIh/HYICIiMjHMRggIiLycf8P4+TjnP1Hd7EAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552\n",
      "12586\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Function to load a graph from a pickle file\n",
    "def load_graph(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        G = pickle.load(f)\n",
    "    return G\n",
    "\n",
    "\n",
    "\n",
    "# Directory where the graphs are stored\n",
    "graphs_dir = 'graphs'\n",
    "\n",
    "\n",
    "# Get list of graph files\n",
    "graph_files = [f for f in os.listdir(graphs_dir) if f.endswith('.pickle')]\n",
    "\n",
    "\n",
    "\n",
    "# Visualize each graph individually\n",
    "for i in range (0, 5):\n",
    "    # Load the graph\n",
    "    G = load_graph(os.path.join(graphs_dir, graph_files[i]))\n",
    "    print(len(G.nodes()))\n",
    "    print(len(G.edges()))\n",
    "    \n",
    "    # Separate nodes by type\n",
    "    test_case_nodes = [n for n, d in G.nodes(data=True) if d['bipartite'] == 0]\n",
    "    statement_nodes = [n for n, d in G.nodes(data=True) if d['bipartite'] == 1]\n",
    "    bug_status_nodes = [n for n, d in G.nodes(data=True) if d['bipartite'] == 2]\n",
    "\n",
    "    # Create positions for each set of nodes\n",
    "    pos = {}\n",
    "    pos.update((node, (1, index)) for index, node in enumerate(test_case_nodes))\n",
    "    pos.update((node, (2, 0)) for node in bug_status_nodes)\n",
    "    pos.update((node, (3, index)) for index, node in enumerate(statement_nodes))\n",
    "\n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=test_case_nodes, node_color='b', label='Test Cases')\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=statement_nodes, node_color='g', label='Statements')\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=bug_status_nodes, node_color='r', label='Test case outcome')\n",
    "\n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(G, pos)\n",
    "\n",
    "    # Add labels\n",
    "    nx.draw_networkx_labels(G, pos)\n",
    "\n",
    "    # Add legend\n",
    "    plt.legend()\n",
    "\n",
    "    # Show plot\n",
    "    plt.title(f'Graph {i}')\n",
    "    plt.show()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4e1403d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 140\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m    139\u001b[0m graphs_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgraphs\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Directory where the graphs are stored\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_and_preprocess_dataset(graphs_dir)\n\u001b[1;32m    141\u001b[0m train_gnn_model(dataset)\n",
      "Cell \u001b[0;32mIn[17], line 59\u001b[0m, in \u001b[0;36mload_and_preprocess_dataset\u001b[0;34m(graphs_dir)\u001b[0m\n\u001b[1;32m     56\u001b[0m     bug_labels \u001b[38;5;241m=\u001b[39m   dataset[i][\u001b[38;5;241m1\u001b[39m]  \n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# Convert to DGLGraph format\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m     dgl_graph \u001b[38;5;241m=\u001b[39m convert_to_dgl_format(spectrum_matrix, bug_labels)\n\u001b[1;32m     61\u001b[0m     dataset\u001b[38;5;241m.\u001b[39mappend(dgl_graph)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "Cell \u001b[0;32mIn[17], line 14\u001b[0m, in \u001b[0;36mconvert_to_dgl_format\u001b[0;34m(spectrum_matrix, bug_labels)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_dgl_format\u001b[39m(spectrum_matrix, bug_labels):\n\u001b[0;32m---> 14\u001b[0m     num_test_cases, num_statements \u001b[38;5;241m=\u001b[39m spectrum_matrix\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Create a new DGLGraph\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     g \u001b[38;5;241m=\u001b[39m dgl\u001b[38;5;241m.\u001b[39mgraph(([], []), num_nodes\u001b[38;5;241m=\u001b[39mnum_test_cases \u001b[38;5;241m+\u001b[39m num_statements)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import dgl\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to convert a bipartite graph to DGLGraph format\n",
    "def convert_to_dgl_format(spectrum_matrix, bug_labels):\n",
    "    num_test_cases, num_statements = spectrum_matrix.shape\n",
    "    \n",
    "    # Create a new DGLGraph\n",
    "    g = dgl.graph(([], []), num_nodes=num_test_cases + num_statements)\n",
    "    \n",
    "    # Add edges between test case nodes and statement nodes based on spectrum\n",
    "    src = []\n",
    "    dst = []\n",
    "    for i in range(num_test_cases):\n",
    "        for j in range(num_statements):\n",
    "            if spectrum_matrix[i][j] == 1:\n",
    "                src.append(i)\n",
    "                dst.append(num_test_cases + j)\n",
    "    \n",
    "    g.add_edges(src, dst)\n",
    "    \n",
    "    # Add self-loops to all nodes\n",
    "    g = dgl.add_self_loop(g)\n",
    "    \n",
    "    # Add node features (spectrum matrix)\n",
    "    features = torch.zeros((num_test_cases + num_statements, num_statements), dtype=torch.float)\n",
    "    features[:num_test_cases, :] = torch.tensor(spectrum_matrix, dtype=torch.float)\n",
    "    g.ndata['x'] = features\n",
    "    \n",
    "    # Add labels (bug labels)\n",
    "    labels = torch.zeros((num_test_cases + num_statements, 1), dtype=torch.float)\n",
    "    labels[:num_test_cases, 0] = torch.tensor(bug_labels, dtype=torch.float)\n",
    "    g.ndata['y'] = labels\n",
    "    \n",
    "    return g\n",
    "\n",
    "# Load and preprocess the dataset of graphs\n",
    "def load_and_preprocess_dataset(graphs_dir):\n",
    "    graph_files = [f for f in os.listdir(graphs_dir) if f.endswith('.pickle')]\n",
    "    \n",
    "    \n",
    "    for graph_file in graph_files:\n",
    "        with open(os.path.join(graphs_dir, graph_file), 'rb') as f:\n",
    "            G = pickle.load(f)\n",
    "        \n",
    "        # Extract spectrum and bug labels from G (assuming you have stored these in G)\n",
    "        spectrum_matrix =  dataset[i][0] \n",
    "        bug_labels =   dataset[i][1]  \n",
    "        \n",
    "        # Convert to DGLGraph format\n",
    "        dgl_graph = convert_to_dgl_format(spectrum_matrix, bug_labels)\n",
    "        \n",
    "        dataset.append(dgl_graph)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Define your GNN model\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=64):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = dgl.nn.GraphConv(num_features, hidden_dim, allow_zero_in_degree=True)\n",
    "        self.conv2 = dgl.nn.GraphConv(hidden_dim, 1, allow_zero_in_degree=True)\n",
    "    \n",
    "    def forward(self, g, features):\n",
    "        h = features\n",
    "        h = torch.relu(self.conv1(g, h))\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "\n",
    "# Custom collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    batched_graph = dgl.batch(batch)\n",
    "    return batched_graph\n",
    "\n",
    "# Custom loss function for SBFL\n",
    "class CustomSBFLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomSBFLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        # logits: Tensor of predicted suspiciousness scores\n",
    "        # labels: Tensor of ground truth (0 for non-buggy, 1 for buggy)\n",
    "        \n",
    "        # Step 1: Sort logits and corresponding labels by descending suspiciousness score\n",
    "        sorted_indices = torch.argsort(logits, descending=True)\n",
    "        sorted_labels = labels[sorted_indices]\n",
    "        \n",
    "        # Step 2: Calculate penalty for non-buggy statements appearing before the first buggy statement\n",
    "        found_buggy = False\n",
    "        wasted_effort = 0.0\n",
    "        for i, label in enumerate(sorted_labels):\n",
    "            if label == 1:  # Found a buggy statement\n",
    "                found_buggy = True\n",
    "                break\n",
    "            else:  # Non-buggy statement\n",
    "                wasted_effort += 1.0  # Penalize non-buggy statement\n",
    "                \n",
    "        # If no buggy statements are found, penalize the total number of statements\n",
    "        if not found_buggy:\n",
    "            wasted_effort = len(sorted_labels)\n",
    "\n",
    "        # Convert to a tensor and set requires_grad to True to enable backpropagation\n",
    "        loss = torch.tensor(wasted_effort, requires_grad=True)\n",
    "        return loss\n",
    "\n",
    "# Function to train the GNN model\n",
    "def train_gnn_model(dataset, num_epochs=50, batch_size=32):\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    model = GNNModel(num_features=dataset[0].ndata['x'].shape[1])  # Initialize the model\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = CustomSBFLoss()  # Use the custom loss function\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch, batch.ndata['x']).squeeze()\n",
    "            labels = batch.ndata['y'].squeeze()\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print the output of the neural network\n",
    "            #print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {loss.item()}\")\n",
    "            #print(\"Logits:\", logits)\n",
    "        \n",
    "        # Print loss or validation metrics after each epoch if needed\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "# Example usage:\n",
    "graphs_dir = 'graphs'  # Directory where the graphs are stored\n",
    "dataset = load_and_preprocess_dataset(graphs_dir)\n",
    "train_gnn_model(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7b8278a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 459.0\n",
      "Epoch [2/50], Loss: 424.0\n",
      "Epoch [3/50], Loss: 438.0\n",
      "Epoch [4/50], Loss: 310.0\n",
      "Epoch [5/50], Loss: 248.0\n",
      "Epoch [6/50], Loss: 265.0\n",
      "Epoch [7/50], Loss: 164.0\n",
      "Epoch [8/50], Loss: 425.0\n",
      "Epoch [9/50], Loss: 200.0\n",
      "Epoch [10/50], Loss: 526.0\n",
      "Epoch [11/50], Loss: 307.0\n",
      "Epoch [12/50], Loss: 346.0\n",
      "Epoch [13/50], Loss: 512.0\n",
      "Epoch [14/50], Loss: 276.0\n",
      "Epoch [15/50], Loss: 145.0\n",
      "Epoch [16/50], Loss: 539.0\n",
      "Epoch [17/50], Loss: 61.0\n",
      "Epoch [18/50], Loss: 273.0\n",
      "Epoch [19/50], Loss: 8.0\n",
      "Epoch [20/50], Loss: 425.0\n",
      "Epoch [21/50], Loss: 517.0\n",
      "Epoch [22/50], Loss: 483.0\n",
      "Epoch [23/50], Loss: 123.0\n",
      "Epoch [24/50], Loss: 512.0\n",
      "Epoch [25/50], Loss: 427.0\n",
      "Epoch [26/50], Loss: 258.0\n",
      "Epoch [27/50], Loss: 31.0\n",
      "Epoch [28/50], Loss: 406.0\n",
      "Epoch [29/50], Loss: 145.0\n",
      "Epoch [30/50], Loss: 217.0\n",
      "Epoch [31/50], Loss: 165.0\n",
      "Epoch [32/50], Loss: 530.0\n",
      "Epoch [33/50], Loss: 88.0\n",
      "Epoch [34/50], Loss: 446.0\n",
      "Epoch [35/50], Loss: 175.0\n",
      "Epoch [36/50], Loss: 190.0\n",
      "Epoch [37/50], Loss: 368.0\n",
      "Epoch [38/50], Loss: 66.0\n",
      "Epoch [39/50], Loss: 525.0\n",
      "Epoch [40/50], Loss: 2.0\n",
      "Epoch [41/50], Loss: 317.0\n",
      "Epoch [42/50], Loss: 435.0\n",
      "Epoch [43/50], Loss: 434.0\n",
      "Epoch [44/50], Loss: 519.0\n",
      "Epoch [45/50], Loss: 245.0\n",
      "Epoch [46/50], Loss: 89.0\n",
      "Epoch [47/50], Loss: 217.0\n",
      "Epoch [48/50], Loss: 206.0\n",
      "Epoch [49/50], Loss: 89.0\n",
      "Epoch [50/50], Loss: 228.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import dgl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dgl.data.utils import load_graphs\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import dgl.nn.pytorch as dglnn\n",
    "\n",
    "def convert_to_dgl_format(G):\n",
    "    # Convert node labels to consecutive integers\n",
    "    mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "    \n",
    "    g = dgl.from_networkx(G)\n",
    "    \n",
    "    # Add node features (spectrum matrix and bug labels)\n",
    "    num_test_cases = len([n for n, d in G.nodes(data=True) if d['bipartite'] == 0])\n",
    "    num_statements = len([n for n, d in G.nodes(data=True) if d['bipartite'] == 1])\n",
    "    \n",
    "    spectrum_matrix = torch.tensor(G.graph['spectrum_matrix'], dtype=torch.float)\n",
    "    test_case_outcomes = torch.tensor(G.graph['test_case_outcomes'], dtype=torch.float)\n",
    "    bug_labels = torch.zeros(len(G.nodes), dtype=torch.float)  # Adjust to total number of nodes\n",
    "    \n",
    "    for node, data in G.nodes(data=True):\n",
    "        if data['bipartite'] == 1:\n",
    "            bug_labels[node] = data['bug_label']\n",
    "    \n",
    "    # Create a feature matrix for the nodes\n",
    "    num_total_nodes = num_test_cases + num_statements + 2  # Including \"Bug\" and \"No Bug\" nodes\n",
    "    node_features = torch.zeros((num_total_nodes, spectrum_matrix.shape[1]))\n",
    "    \n",
    "    node_features[:num_test_cases, :] = spectrum_matrix  # Add spectrum matrix as features for test case nodes\n",
    "    node_features[num_test_cases:num_test_cases + num_statements, :] = torch.zeros((num_statements, spectrum_matrix.shape[1]))  # Placeholder for statement nodes\n",
    "    \n",
    "    g.ndata['x'] = node_features\n",
    "    g.ndata['y'] = bug_labels.view(-1, 1)\n",
    "    \n",
    "    return g\n",
    "\n",
    "def load_and_preprocess_dataset(graphs_dir):\n",
    "    dataset = []\n",
    "    graph_files = [f for f in os.listdir(graphs_dir) if f.endswith('.pickle')]\n",
    "    \n",
    "    for graph_file in graph_files:\n",
    "        with open(os.path.join(graphs_dir, graph_file), 'rb') as f:\n",
    "            G = pickle.load(f)\n",
    "            dgl_graph = convert_to_dgl_format(G)\n",
    "            dataset.append(dgl_graph)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_dim=64):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.conv1 = dglnn.GraphConv(in_feats, hidden_dim, allow_zero_in_degree=True)\n",
    "        self.conv2 = dglnn.GraphConv(hidden_dim, 1, allow_zero_in_degree=True)\n",
    "    \n",
    "    def forward(self, g, features):\n",
    "        h = torch.relu(self.conv1(g, features))\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class CustomSBFLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomSBFLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        sorted_indices = torch.argsort(logits, descending=True)\n",
    "        sorted_labels = labels[sorted_indices]\n",
    "        \n",
    "        found_buggy = False\n",
    "        wasted_effort = 0.0\n",
    "        for i, label in enumerate(sorted_labels):\n",
    "            if label == 1:\n",
    "                found_buggy = True\n",
    "                break\n",
    "            else:\n",
    "                wasted_effort += 1.0\n",
    "                \n",
    "        if not found_buggy:\n",
    "            wasted_effort = len(sorted_labels)\n",
    "\n",
    "        loss = torch.tensor(wasted_effort, requires_grad=True)\n",
    "        return loss\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return dgl.batch(batch)\n",
    "\n",
    "def train_gnn_model(dataset, num_epochs=50, batch_size=32):\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    model = GCNModel(in_feats=dataset[0].ndata['x'].shape[1])  # Initialize the model\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = CustomSBFLoss()  # Use the custom loss function\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch, batch.ndata['x']).squeeze()\n",
    "            labels = batch.ndata['y'].squeeze()\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "        \n",
    "# Load the dataset\n",
    "graphs_dir = 'graphs'  # Directory where the graphs are stored\n",
    "dataset = load_and_preprocess_dataset(graphs_dir)\n",
    "\n",
    "# Train the GCN model\n",
    "train_gnn_model(dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "608e50c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.004234091844409704\n",
      "Epoch [2/50], Loss: 0.0030658578034490347\n",
      "Epoch [3/50], Loss: 0.002494873246178031\n",
      "Epoch [4/50], Loss: 0.0031177212949842215\n",
      "Epoch [5/50], Loss: 0.002231161342933774\n",
      "Epoch [6/50], Loss: 0.0026338316965848207\n",
      "Epoch [7/50], Loss: 0.0027656613383442163\n",
      "Epoch [8/50], Loss: 0.0019803063478320837\n",
      "Epoch [9/50], Loss: 0.0027595250867307186\n",
      "Epoch [10/50], Loss: 0.001980419270694256\n",
      "Epoch [11/50], Loss: 0.002457222668454051\n",
      "Epoch [12/50], Loss: 0.0025347606278955936\n",
      "Epoch [13/50], Loss: 0.0019693339709192514\n",
      "Epoch [14/50], Loss: 0.0022020316682755947\n",
      "Epoch [15/50], Loss: 0.0024380027316510677\n",
      "Epoch [16/50], Loss: 0.0020130604971200228\n",
      "Epoch [17/50], Loss: 0.0020613817032426596\n",
      "Epoch [18/50], Loss: 0.0020099510438740253\n",
      "Epoch [19/50], Loss: 0.0021922020241618156\n",
      "Epoch [20/50], Loss: 0.002207574900239706\n",
      "Epoch [21/50], Loss: 0.0027760311495512724\n",
      "Epoch [22/50], Loss: 0.002437384333461523\n",
      "Epoch [23/50], Loss: 0.002142434474080801\n",
      "Epoch [24/50], Loss: 0.0026474224869161844\n",
      "Epoch [25/50], Loss: 0.002411171793937683\n",
      "Epoch [26/50], Loss: 0.0023394112940877676\n",
      "Epoch [27/50], Loss: 0.001902237767353654\n",
      "Epoch [28/50], Loss: 0.0020542440470308065\n",
      "Epoch [29/50], Loss: 0.002061521867290139\n",
      "Epoch [30/50], Loss: 0.0021292557939887047\n",
      "Epoch [31/50], Loss: 0.0019305858295410872\n",
      "Epoch [32/50], Loss: 0.0022232583723962307\n",
      "Epoch [33/50], Loss: 0.0019052400020882487\n",
      "Epoch [34/50], Loss: 0.002050951588898897\n",
      "Epoch [35/50], Loss: 0.0019395165145397186\n",
      "Epoch [36/50], Loss: 0.0019424529746174812\n",
      "Epoch [37/50], Loss: 0.0019115007016807795\n",
      "Epoch [38/50], Loss: 0.00199994002468884\n",
      "Epoch [39/50], Loss: 0.002321856329217553\n",
      "Epoch [40/50], Loss: 0.0024321535602211952\n",
      "Epoch [41/50], Loss: 0.0019149997970089316\n",
      "Epoch [42/50], Loss: 0.0019122334197163582\n",
      "Epoch [43/50], Loss: 0.00210467423312366\n",
      "Epoch [44/50], Loss: 0.0019492320716381073\n",
      "Epoch [45/50], Loss: 0.0021442612633109093\n",
      "Epoch [46/50], Loss: 0.0020744088105857372\n",
      "Epoch [47/50], Loss: 0.0021003393922001123\n",
      "Epoch [48/50], Loss: 0.0022217645309865475\n",
      "Epoch [49/50], Loss: 0.002017337828874588\n",
      "Epoch [50/50], Loss: 0.001990813761949539\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "from dgl.data import DGLDataset\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# Custom loss function\n",
    "class CustomSBFLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomSBFLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        return self.mse_loss(predictions, targets)\n",
    "\n",
    "# GCN model\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats=16):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.conv1 = dgl.nn.GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = dgl.nn.GraphConv(h_feats, 1)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "\n",
    "# Convert NetworkX graph to DGL format\n",
    "def convert_to_dgl_format(G):\n",
    "    mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "    G = nx.relabel_nodes(G, mapping)\n",
    "\n",
    "    g = dgl.from_networkx(G)\n",
    "    \n",
    "    num_test_cases = len([n for n, d in G.nodes(data=True) if d['bipartite'] == 0])\n",
    "    num_statements = len([n for n, d in G.nodes(data=True) if d['bipartite'] == 1])\n",
    "    \n",
    "    spectrum_matrix = torch.tensor(G.graph['spectrum_matrix'], dtype=torch.float)\n",
    "    test_case_outcomes = torch.tensor(G.graph['test_case_outcomes'], dtype=torch.float)\n",
    "    bug_labels = torch.zeros(len(G.nodes), dtype=torch.float)\n",
    "    \n",
    "    for node, data in G.nodes(data=True):\n",
    "        if data['bipartite'] == 1:\n",
    "            bug_labels[node] = data['bug_label']\n",
    "    \n",
    "    num_total_nodes = num_test_cases + num_statements + 2\n",
    "    node_features = torch.zeros((num_total_nodes, spectrum_matrix.shape[1]))\n",
    "    \n",
    "    node_features[:num_test_cases, :] = spectrum_matrix\n",
    "    node_features[num_test_cases:num_test_cases + num_statements, :] = torch.zeros((num_statements, spectrum_matrix.shape[1]))\n",
    "    \n",
    "    g.ndata['x'] = node_features\n",
    "    g.ndata['y'] = bug_labels.view(-1, 1)\n",
    "    \n",
    "    return g\n",
    "\n",
    "# Load and preprocess dataset\n",
    "def load_and_preprocess_dataset(graphs_dir):\n",
    "    dataset = []\n",
    "    graph_files = [f for f in os.listdir(graphs_dir) if f.endswith('.pickle')]\n",
    "    \n",
    "    for graph_file in graph_files:\n",
    "        with open(os.path.join(graphs_dir, graph_file), 'rb') as f:\n",
    "            G = pickle.load(f)\n",
    "            dgl_graph = convert_to_dgl_format(G)\n",
    "            dataset.append(dgl_graph)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# DataLoader collate function\n",
    "def collate_fn(batch):\n",
    "    return dgl.batch(batch)\n",
    "\n",
    "# Train GCN model\n",
    "def train_gnn_model(dataset, num_epochs=50, batch_size=32):\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    model = GCNModel(in_feats=dataset[0].ndata['x'].shape[1])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = CustomSBFLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch, batch.ndata['x']).squeeze()\n",
    "            labels = batch.ndata['y'].squeeze()\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "# Load the dataset\n",
    "graphs_dir = 'graphs'\n",
    "dataset = load_and_preprocess_dataset(graphs_dir)\n",
    "\n",
    "# Train the GCN model\n",
    "train_gnn_model(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0270ab7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'spectrum_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 117\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m    116\u001b[0m graphs_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgraphs\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Directory where the graphs are stored\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_and_preprocess_dataset(graphs_dir)\n\u001b[1;32m    118\u001b[0m train_gnn_model(dataset)\n",
      "Cell \u001b[0;32mIn[8], line 54\u001b[0m, in \u001b[0;36mload_and_preprocess_dataset\u001b[0;34m(graphs_dir)\u001b[0m\n\u001b[1;32m     51\u001b[0m     G \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Extract spectrum and bug labels from G (assuming you have stored these in G)\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m spectrum_matrix \u001b[38;5;241m=\u001b[39m G[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspectrum_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Replace with actual key if different\u001b[39;00m\n\u001b[1;32m     55\u001b[0m bug_labels \u001b[38;5;241m=\u001b[39m G[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbug_labels\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Replace with actual key if different\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Convert to DGLGraph format\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/networkx/classes/graph.py:513\u001b[0m, in \u001b[0;36mGraph.__getitem__\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, n):\n\u001b[1;32m    490\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a dict of neighbors of node n.  Use: 'G[n]'.\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \n\u001b[1;32m    492\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;124;03m    AtlasView({1: {}})\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madj[n]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/networkx/classes/coreviews.py:81\u001b[0m, in \u001b[0;36mAdjacencyView.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AtlasView(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_atlas[name])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'spectrum_matrix'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import dgl\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to convert a bipartite graph to DGLGraph format\n",
    "def convert_to_dgl_format(spectrum_matrix, bug_labels):\n",
    "    num_test_cases, num_statements = spectrum_matrix.shape\n",
    "    \n",
    "    # Create a new DGLGraph\n",
    "    g = dgl.graph(([], []), num_nodes=num_test_cases + num_statements)\n",
    "    \n",
    "    # Add edges between test case nodes and statement nodes based on spectrum\n",
    "    src = []\n",
    "    dst = []\n",
    "    for i in range(num_test_cases):\n",
    "        for j in range(num_statements):\n",
    "            if spectrum_matrix[i][j] == 1:\n",
    "                src.append(i)\n",
    "                dst.append(num_test_cases + j)\n",
    "    \n",
    "    g.add_edges(src, dst)\n",
    "    \n",
    "    # Add self-loops to all nodes\n",
    "    g = dgl.add_self_loop(g)\n",
    "    \n",
    "    # Add node features (spectrum matrix)\n",
    "    features = torch.zeros((num_test_cases + num_statements, num_statements), dtype=torch.float)\n",
    "    features[:num_test_cases, :] = torch.tensor(spectrum_matrix, dtype=torch.float)\n",
    "    g.ndata['x'] = features\n",
    "    \n",
    "    # Add labels (bug labels)\n",
    "    labels = torch.zeros((num_test_cases + num_statements, 1), dtype=torch.float)\n",
    "    labels[:num_test_cases, 0] = torch.tensor(bug_labels, dtype=torch.float)\n",
    "    g.ndata['y'] = labels\n",
    "    \n",
    "    return g\n",
    "\n",
    "# Load and preprocess the dataset of graphs\n",
    "def load_and_preprocess_dataset(graphs_dir):\n",
    "    graph_files = [f for f in os.listdir(graphs_dir) if f.endswith('.pickle')]\n",
    "    dataset = []\n",
    "    \n",
    "    for graph_file in graph_files:\n",
    "        with open(os.path.join(graphs_dir, graph_file), 'rb') as f:\n",
    "            G = pickle.load(f)\n",
    "        \n",
    "        # Extract spectrum and bug labels from G (assuming you have stored these in G)\n",
    "        spectrum_matrix = G['spectrum_matrix']  # Replace with actual key if different\n",
    "        bug_labels = G['bug_labels']  # Replace with actual key if different\n",
    "        \n",
    "        # Convert to DGLGraph format\n",
    "        dgl_graph = convert_to_dgl_format(spectrum_matrix, bug_labels)\n",
    "        \n",
    "        dataset.append(dgl_graph)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Define your GNN model\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=64):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = dgl.nn.GraphConv(num_features, hidden_dim, allow_zero_in_degree=True)\n",
    "        self.conv2 = dgl.nn.GraphConv(hidden_dim, 1, allow_zero_in_degree=True)\n",
    "    \n",
    "    def forward(self, g, features):\n",
    "        h = features\n",
    "        h = torch.relu(self.conv1(g, h))\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "\n",
    "# Custom collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    batched_graph = dgl.batch(batch)\n",
    "    return batched_graph\n",
    "\n",
    "# Custom loss function for SBFL\n",
    "class CustomSBFLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomSBFLoss, self).__init__()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        logits = logits.squeeze()\n",
    "        labels = labels.squeeze()\n",
    "        return self.bce_loss(logits, labels)\n",
    "\n",
    "# Function to train the GNN model\n",
    "def train_gnn_model(dataset, num_epochs=50, batch_size=32):\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    model = GNNModel(num_features=dataset[0].ndata['x'].shape[1])  # Initialize the model\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = CustomSBFLoss()  # Use the custom loss function\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch, batch.ndata['x']).squeeze()\n",
    "            labels = batch.ndata['y'].squeeze()\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Print loss or validation metrics after each epoch if needed\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader)}')\n",
    "\n",
    "# Example usage:\n",
    "graphs_dir = 'graphs'  # Directory where the graphs are stored\n",
    "dataset = load_and_preprocess_dataset(graphs_dir)\n",
    "train_gnn_model(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b2f4387",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 99\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m train_sbfl_model(sbfl_dataset)\n",
      "Cell \u001b[0;32mIn[29], line 91\u001b[0m, in \u001b[0;36mtrain_sbfl_model\u001b[0;34m(dataset, num_epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m     89\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     90\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(spectrum_matrix, outcomes)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m---> 91\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, buggy_statements)\n\u001b[1;32m     92\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     93\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[29], line 52\u001b[0m, in \u001b[0;36mCustomSBFLoss.forward\u001b[0;34m(self, logits, buggy_statements)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[1;32m     51\u001b[0m     logit \u001b[38;5;241m=\u001b[39m logits[i]\n\u001b[0;32m---> 52\u001b[0m     buggy_statement \u001b[38;5;241m=\u001b[39m buggy_statements[i]\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# Sort logits by descending suspiciousness score\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     sorted_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margsort(logit, descending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Dataset class\n",
    "class SBFLDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        spectrum_matrix, buggy_statements, outcomes = self.dataset[idx]\n",
    "        return torch.tensor(spectrum_matrix, dtype=torch.float), buggy_statements, torch.tensor(outcomes, dtype=torch.float)\n",
    "\n",
    "\n",
    "sbfl_dataset = SBFLDataset(dataset)\n",
    "\n",
    "# Neural Network Model\n",
    "class SBFLModel(nn.Module):\n",
    "    def __init__(self, num_test_cases, num_statements):\n",
    "        super(SBFLModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_test_cases * num_statements + num_test_cases, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, num_statements)\n",
    "\n",
    "    def forward(self, spectrum_matrix, outcomes):\n",
    "        # Flatten the spectrum matrix and concatenate with outcomes\n",
    "        batch_size = spectrum_matrix.size(0)\n",
    "        flattened_spectrum = spectrum_matrix.view(batch_size, -1)\n",
    "        x = torch.cat((flattened_spectrum, outcomes), dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Custom Loss Function\n",
    "class CustomSBFLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomSBFLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits, buggy_statements):\n",
    "        batch_size = logits.size(0)\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            logit = logits[i]\n",
    "            buggy_statement = buggy_statements[i]\n",
    "            \n",
    "            # Sort logits by descending suspiciousness score\n",
    "            sorted_indices = torch.argsort(logit, descending=True)\n",
    "            \n",
    "            # Create a set of buggy statement indices for quick lookup\n",
    "            buggy_set = set(buggy_statement)\n",
    "            \n",
    "            # Calculate penalty for non-buggy statements appearing before the first buggy statement\n",
    "            wasted_effort = 0.0\n",
    "            for idx in sorted_indices:\n",
    "                if idx.item() in buggy_set:  # Found a buggy statement\n",
    "                    break\n",
    "                else:  # Non-buggy statement\n",
    "                    wasted_effort += 1.0  # Penalize non-buggy statement\n",
    "            \n",
    "            # If no buggy statements are found, penalize the total number of statements\n",
    "            if not any(idx.item() in buggy_set for idx in sorted_indices):\n",
    "                wasted_effort = len(sorted_indices)\n",
    "\n",
    "            total_loss += wasted_effort\n",
    "\n",
    "        # Convert to a tensor and set requires_grad to True to enable backpropagation\n",
    "        loss = torch.tensor(total_loss, requires_grad=True) / batch_size\n",
    "        return loss\n",
    "\n",
    "# Train the Model\n",
    "def train_sbfl_model(dataset, num_epochs=50, batch_size=32, learning_rate=0.001):\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    model = SBFLModel(num_test_cases=num_test_cases, num_statements=num_statements)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = CustomSBFLoss()  # Use the custom loss function\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for i, (spectrum_matrix, buggy_statements, outcomes) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(spectrum_matrix, outcomes).squeeze()\n",
    "            loss = criterion(logits, buggy_statements)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader)}')\n",
    "\n",
    "# Train the model\n",
    "train_sbfl_model(sbfl_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d8c6daf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training files: 3000\n",
      "Validation files: 1000\n",
      "Testing files: 1000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Directories\n",
    "graphs_dir = 'graphs'\n",
    "train_dir = 'graphs_train'\n",
    "val_dir = 'graphs_val'\n",
    "test_dir = 'graphs_test'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Get list of all graph files\n",
    "graph_files = [f for f in os.listdir(graphs_dir) if f.endswith('.pickle')]\n",
    "\n",
    "# Shuffle the list to ensure random distribution\n",
    "random.shuffle(graph_files)\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "train_files = graph_files[:3000]\n",
    "val_files = graph_files[3000:4000]\n",
    "test_files = graph_files[4000:]\n",
    "\n",
    "# Function to copy files to a directory\n",
    "def copy_files(files, source_dir, dest_dir):\n",
    "    for file in files:\n",
    "        shutil.copy(os.path.join(source_dir, file), os.path.join(dest_dir, file))\n",
    "\n",
    "# Copy files to respective directories\n",
    "copy_files(train_files, graphs_dir, train_dir)\n",
    "copy_files(val_files, graphs_dir, val_dir)\n",
    "copy_files(test_files, graphs_dir, test_dir)\n",
    "\n",
    "print(f\"Training files: {len(train_files)}\")\n",
    "print(f\"Validation files: {len(val_files)}\")\n",
    "print(f\"Testing files: {len(test_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f867d5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import dgl\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Function to convert a bipartite graph to DGLGraph format\n",
    "def convert_to_dgl_format(spectrum_matrix, bug_labels):\n",
    "    num_test_cases, num_statements = spectrum_matrix.shape\n",
    "    \n",
    "    # Create a new DGLGraph\n",
    "    g = dgl.graph(([], []), num_nodes=num_test_cases + num_statements)\n",
    "    \n",
    "    # Add edges between test case nodes and statement nodes based on spectrum\n",
    "    src = []\n",
    "    dst = []\n",
    "    for i in range(num_test_cases):\n",
    "        for j in range(num_statements):\n",
    "            if spectrum_matrix[i][j] == 1:\n",
    "                src.append(i)\n",
    "                dst.append(num_test_cases + j)\n",
    "    \n",
    "    g.add_edges(src, dst)\n",
    "    \n",
    "    # Add self-loops to all nodes\n",
    "    g = dgl.add_self_loop(g)\n",
    "    \n",
    "    # Add node features (spectrum matrix)\n",
    "    features = torch.zeros((num_test_cases + num_statements, num_statements), dtype=torch.float)\n",
    "    features[:num_test_cases, :] = torch.tensor(spectrum_matrix, dtype=torch.float)\n",
    "    g.ndata['x'] = features\n",
    "    \n",
    "    # Add labels (bug labels)\n",
    "    labels = torch.zeros((num_test_cases + num_statements, 1), dtype=torch.float)\n",
    "    labels[:num_test_cases, 0] = torch.tensor(bug_labels, dtype=torch.float)\n",
    "    g.ndata['y'] = labels\n",
    "    \n",
    "    return g\n",
    "\n",
    "# Load and preprocess the dataset of graphs\n",
    "def load_and_preprocess_dataset(graphs_dir):\n",
    "    graph_files = [f for f in os.listdir(graphs_dir) if f.endswith('.pickle')]\n",
    "    dataset = []\n",
    "    \n",
    "    for graph_file in graph_files:\n",
    "        with open(os.path.join(graphs_dir, graph_file), 'rb') as f:\n",
    "            G = pickle.load(f)\n",
    "        \n",
    "        # Extract spectrum and bug labels from G (assuming you have stored these in G)\n",
    "        spectrum_matrix = np.random.randint(0, 2, size=(50, 500))  # Replace with actual data\n",
    "        bug_labels = np.random.randint(0, 2, size=50)  # Replace with actual data\n",
    "        \n",
    "        # Convert to DGLGraph format\n",
    "        dgl_graph = convert_to_dgl_format(spectrum_matrix, bug_labels)\n",
    "        \n",
    "        dataset.append(dgl_graph)\n",
    "    \n",
    "    return dataset\n",
    "# Load datasets\n",
    "train_dataset = load_and_preprocess_dataset(train_dir)\n",
    "val_dataset = load_and_preprocess_dataset(val_dir)\n",
    "test_dataset = load_and_preprocess_dataset(test_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "17943bd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GNNModel.__init__() got an unexpected keyword argument 'input_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_val_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m train_gnn_model(train_dataset, val_dataset)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Testing function (optional)\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_gnn_model\u001b[39m(test_dataset):\n",
      "Cell \u001b[0;32mIn[70], line 25\u001b[0m, in \u001b[0;36mtrain_gnn_model\u001b[0;34m(train_dataset, val_dataset, num_epochs, batch_size)\u001b[0m\n\u001b[1;32m     22\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: dgl\u001b[38;5;241m.\u001b[39mbatch(x))\n\u001b[1;32m     23\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: dgl\u001b[38;5;241m.\u001b[39mbatch(x))\n\u001b[0;32m---> 25\u001b[0m model \u001b[38;5;241m=\u001b[39m GNNModel(input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     27\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n",
      "\u001b[0;31mTypeError\u001b[0m: GNNModel.__init__() got an unexpected keyword argument 'input_dim'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define your GNN model\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=64):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = dgl.nn.GraphConv(num_features, hidden_dim, allow_zero_in_degree=True)\n",
    "        self.conv2 = dgl.nn.GraphConv(hidden_dim, 1, allow_zero_in_degree=True)\n",
    "    \n",
    "    def forward(self, g, features):\n",
    "        h = features\n",
    "        h = torch.relu(self.conv1(g, h))\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "# Define GNNModel, train_gnn_model, and any other necessary functions from the previous examples\n",
    "\n",
    "# Custom collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    batched_graph = dgl.batch(batch)\n",
    "    return batched_graph\n",
    "\n",
    "# Training function\n",
    "def train_gnn_model(train_dataset, val_dataset, num_epochs=50, batch_size=32):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: dgl.batch(x))\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: dgl.batch(x))\n",
    "    \n",
    "    model = GNNModel(input_dim=500, hidden_dim=256, output_dim=1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch, batch.ndata['x'])\n",
    "            labels = batch.ndata['y']\n",
    "            loss = criterion(logits.squeeze(), labels.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Validate the model\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                logits = model(batch, batch.ndata['x'])\n",
    "                labels = batch.ndata['y']\n",
    "                loss = criterion(logits.squeeze(), labels.squeeze())\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train_gnn_model(train_dataset, val_dataset)\n",
    "\n",
    "# Testing function (optional)\n",
    "def test_gnn_model(test_dataset):\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: dgl.batch(x))\n",
    "    model = GNNModel(input_dim=500, hidden_dim=256, output_dim=1)\n",
    "    model.load_state_dict(torch.load('gnn_model.pth'))  # Load your trained model\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            logits = model(batch, batch.ndata['x'])\n",
    "            labels = batch.ndata['y']\n",
    "            predicted = (torch.sigmoid(logits) > 0.5).float()\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = total_correct / len(test_loader.dataset)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Test the model\n",
    "test_gnn_model(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c85b10be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Batch [1/32], Loss: 0.7093328833580017\n",
      "Logits: tensor([ 0.0021, -0.0007,  0.0015,  ...,  0.0341,  0.0310,  0.0361],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [2/32], Loss: 0.6792421340942383\n",
      "Logits: tensor([-0.0024, -0.0028, -0.0051,  ..., -0.0342, -0.0267, -0.0298],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [3/32], Loss: 0.6518663167953491\n",
      "Logits: tensor([-0.0071, -0.0092, -0.0077,  ..., -0.0966, -0.1011, -0.1024],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [4/32], Loss: 0.6274996399879456\n",
      "Logits: tensor([-0.0118, -0.0098, -0.0120,  ..., -0.1436, -0.1423, -0.1345],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [5/32], Loss: 0.6020605564117432\n",
      "Logits: tensor([-0.0148, -0.0162, -0.0152,  ..., -0.2129, -0.2112, -0.1985],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [6/32], Loss: 0.5764575004577637\n",
      "Logits: tensor([-0.0202, -0.0162, -0.0164,  ..., -0.2686, -0.2740, -0.2549],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [7/32], Loss: 0.5493020415306091\n",
      "Logits: tensor([-0.0201, -0.0244, -0.0211,  ..., -0.3482, -0.3349, -0.3484],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [8/32], Loss: 0.5227444767951965\n",
      "Logits: tensor([-0.0305, -0.0273, -0.0280,  ..., -0.4056, -0.4410, -0.4163],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [9/32], Loss: 0.49756863713264465\n",
      "Logits: tensor([-0.0344, -0.0357, -0.0333,  ..., -0.4693, -0.4956, -0.4884],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [10/32], Loss: 0.473561555147171\n",
      "Logits: tensor([-0.0383, -0.0377, -0.0381,  ..., -0.5717, -0.5566, -0.5791],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [11/32], Loss: 0.4495665431022644\n",
      "Logits: tensor([-0.0380, -0.0424, -0.0437,  ..., -0.6455, -0.6395, -0.6207],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [12/32], Loss: 0.42623764276504517\n",
      "Logits: tensor([-0.0470, -0.0484, -0.0461,  ..., -0.7267, -0.6969, -0.7008],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [13/32], Loss: 0.40366944670677185\n",
      "Logits: tensor([-0.0512, -0.0525, -0.0517,  ..., -0.7990, -0.7705, -0.7918],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [14/32], Loss: 0.38297754526138306\n",
      "Logits: tensor([-0.0589, -0.0558, -0.0545,  ..., -0.8425, -0.8629, -0.8197],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [15/32], Loss: 0.36161306500434875\n",
      "Logits: tensor([-0.0595, -0.0619, -0.0613,  ..., -0.9916, -0.9391, -0.9822],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [16/32], Loss: 0.34231871366500854\n",
      "Logits: tensor([-0.0699, -0.0669, -0.0685,  ..., -1.0199, -1.0036, -0.9716],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [17/32], Loss: 0.32392969727516174\n",
      "Logits: tensor([-0.0756, -0.0721, -0.0712,  ..., -1.0955, -1.1025, -1.0680],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [18/32], Loss: 0.3060646057128906\n",
      "Logits: tensor([-0.0805, -0.0788, -0.0761,  ..., -1.1828, -1.1888, -1.1638],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [19/32], Loss: 0.28902536630630493\n",
      "Logits: tensor([-0.0811, -0.0807, -0.0848,  ..., -1.2528, -1.2215, -1.2811],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [20/32], Loss: 0.27394163608551025\n",
      "Logits: tensor([-0.0877, -0.0903, -0.0831,  ..., -1.3382, -1.3058, -1.3198],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [21/32], Loss: 0.2589716613292694\n",
      "Logits: tensor([-0.0883, -0.0922, -0.0932,  ..., -1.4562, -1.4833, -1.3723],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [22/32], Loss: 0.24525867402553558\n",
      "Logits: tensor([-0.0993, -0.0963, -0.0921,  ..., -1.5063, -1.4961, -1.5389],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [23/32], Loss: 0.23220603168010712\n",
      "Logits: tensor([-0.1029, -0.1039, -0.0980,  ..., -1.6083, -1.6285, -1.6036],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [24/32], Loss: 0.22021172940731049\n",
      "Logits: tensor([-0.1044, -0.1024, -0.1071,  ..., -1.6920, -1.6829, -1.6281],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [25/32], Loss: 0.20973940193653107\n",
      "Logits: tensor([-0.1050, -0.1110, -0.1068,  ..., -1.7395, -1.7563, -1.7488],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [26/32], Loss: 0.19852155447006226\n",
      "Logits: tensor([-0.1187, -0.1156, -0.1181,  ..., -1.8416, -1.8242, -1.8476],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [27/32], Loss: 0.18894046545028687\n",
      "Logits: tensor([-0.1212, -0.1229, -0.1165,  ..., -1.7710, -1.8530, -1.9141],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [28/32], Loss: 0.18039311468601227\n",
      "Logits: tensor([-0.1269, -0.1260, -0.1210,  ..., -2.0005, -2.0211, -1.9842],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [29/32], Loss: 0.17201761901378632\n",
      "Logits: tensor([-0.1298, -0.1307, -0.1328,  ..., -2.0495, -2.1190, -2.0787],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [30/32], Loss: 0.16410546004772186\n",
      "Logits: tensor([-0.1341, -0.1336, -0.1365,  ..., -2.1543, -2.1813, -2.1201],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [31/32], Loss: 0.15789321064949036\n",
      "Logits: tensor([-0.1376, -0.1391, -0.1450,  ..., -2.1256, -2.1620, -2.1599],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Batch [32/32], Loss: 0.15086381137371063\n",
      "Logits: tensor([-0.1434, -0.1437, -0.1441,  ..., -2.2179, -2.3379, -2.2831],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [1/50], Loss: 0.15086381137371063\n",
      "Epoch [2/50], Batch [1/32], Loss: 0.14534655213356018\n",
      "Logits: tensor([-0.1493, -0.1455, -0.1445,  ..., -2.1444, -2.2692, -2.3892],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [2/32], Loss: 0.13981740176677704\n",
      "Logits: tensor([-0.1492, -0.1519, -0.1503,  ..., -2.3484, -2.3307, -2.3943],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [3/32], Loss: 0.13501842319965363\n",
      "Logits: tensor([-0.1504, -0.1565, -0.1577,  ..., -2.4533, -2.5313, -2.5898],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [4/32], Loss: 0.12975256145000458\n",
      "Logits: tensor([-0.1593, -0.1613, -0.1602,  ..., -2.6463, -2.6089, -2.6176],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [5/32], Loss: 0.12570039927959442\n",
      "Logits: tensor([-0.1621, -0.1660, -0.1640,  ..., -2.6581, -2.6077, -2.6655],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [6/32], Loss: 0.12202152609825134\n",
      "Logits: tensor([-0.1654, -0.1685, -0.1656,  ..., -2.6781, -2.7353, -2.7853],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [7/32], Loss: 0.11828183382749557\n",
      "Logits: tensor([-0.1733, -0.1698, -0.1743,  ..., -2.8107, -2.8202, -2.8478],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [8/32], Loss: 0.11504069715738297\n",
      "Logits: tensor([-0.1750, -0.1748, -0.1736,  ..., -2.7937, -2.8538, -2.7939],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [9/32], Loss: 0.11220552027225494\n",
      "Logits: tensor([-0.1821, -0.1750, -0.1855,  ..., -2.8057, -2.9518, -2.9783],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [10/32], Loss: 0.10944100469350815\n",
      "Logits: tensor([-0.1863, -0.1842, -0.1847,  ..., -2.9598, -2.9292, -3.0592],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [11/32], Loss: 0.10691241174936295\n",
      "Logits: tensor([-0.1891, -0.1857, -0.1875,  ..., -3.0256, -3.0649, -3.1159],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [12/32], Loss: 0.10416191071271896\n",
      "Logits: tensor([-0.1892, -0.1883, -0.1918,  ..., -3.0595, -3.0343, -3.0204],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [13/32], Loss: 0.10227588564157486\n",
      "Logits: tensor([-0.1937, -0.1913, -0.1894,  ..., -3.2126, -3.0497, -3.0773],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [14/32], Loss: 0.100255087018013\n",
      "Logits: tensor([-0.1953, -0.1979, -0.1973,  ..., -3.1595, -3.3044, -3.1633],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [15/32], Loss: 0.0983913242816925\n",
      "Logits: tensor([-0.1992, -0.1959, -0.1995,  ..., -3.3188, -3.2592, -3.2626],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [16/32], Loss: 0.09683483839035034\n",
      "Logits: tensor([-0.1981, -0.2000, -0.2029,  ..., -3.3143, -3.3131, -3.2346],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [17/32], Loss: 0.0947127491235733\n",
      "Logits: tensor([-0.2066, -0.1986, -0.2029,  ..., -3.3388, -3.4356, -3.3375],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [18/32], Loss: 0.09357151389122009\n",
      "Logits: tensor([-0.2030, -0.2115, -0.2071,  ..., -3.4216, -3.5697, -3.3951],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [19/32], Loss: 0.09240955114364624\n",
      "Logits: tensor([-0.2099, -0.2084, -0.2124,  ..., -3.5416, -3.5482, -3.4665],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [20/32], Loss: 0.09088610112667084\n",
      "Logits: tensor([-0.2121, -0.2148, -0.2118,  ..., -3.5439, -3.5030, -3.5272],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [21/32], Loss: 0.08963347226381302\n",
      "Logits: tensor([-0.2137, -0.2183, -0.2171,  ..., -3.5690, -3.6098, -3.6091],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [22/32], Loss: 0.08853830397129059\n",
      "Logits: tensor([-0.2204, -0.2183, -0.2181,  ..., -3.5891, -3.6320, -3.5863],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [23/32], Loss: 0.08757228404283524\n",
      "Logits: tensor([-0.2225, -0.2178, -0.2195,  ..., -3.3721, -3.5323, -3.6501],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [24/32], Loss: 0.0864543691277504\n",
      "Logits: tensor([-0.2274, -0.2224, -0.2248,  ..., -3.6527, -3.7065, -3.6873],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [25/32], Loss: 0.08602318167686462\n",
      "Logits: tensor([-0.2240, -0.2357, -0.2270,  ..., -3.8027, -3.7021, -3.8092],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [26/32], Loss: 0.08450769633054733\n",
      "Logits: tensor([-0.2277, -0.2280, -0.2226,  ..., -3.6373, -3.8099, -3.8070],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [27/32], Loss: 0.08382418751716614\n",
      "Logits: tensor([-0.2314, -0.2309, -0.2326,  ..., -3.7202, -3.9088, -3.8152],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [28/32], Loss: 0.08319301158189774\n",
      "Logits: tensor([-0.2321, -0.2289, -0.2300,  ..., -3.7364, -3.6696, -3.8967],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [29/32], Loss: 0.08243075758218765\n",
      "Logits: tensor([-0.2338, -0.2316, -0.2371,  ..., -3.8208, -3.7660, -3.7870],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [30/32], Loss: 0.08161725103855133\n",
      "Logits: tensor([-0.2364, -0.2286, -0.2373,  ..., -3.7996, -3.9761, -3.7950],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [31/32], Loss: 0.08144085109233856\n",
      "Logits: tensor([-0.2409, -0.2355, -0.2381,  ..., -3.8837, -3.9184, -3.9892],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Batch [32/32], Loss: 0.08125912398099899\n",
      "Logits: tensor([-0.2429, -0.2369, -0.2391,  ..., -4.0731, -3.9958, -4.0486],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [2/50], Loss: 0.08125912398099899\n",
      "Epoch [3/50], Batch [1/32], Loss: 0.08048700541257858\n",
      "Logits: tensor([-0.2355, -0.2435, -0.2354,  ..., -4.1661, -4.0095, -3.9779],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [2/32], Loss: 0.07952713221311569\n",
      "Logits: tensor([-0.2399, -0.2439, -0.2454,  ..., -3.7562, -4.0400, -3.8520],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [3/32], Loss: 0.07867730408906937\n",
      "Logits: tensor([-0.2466, -0.2444, -0.2479,  ..., -4.1776, -4.1451, -3.9373],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [4/32], Loss: 0.07916475087404251\n",
      "Logits: tensor([-0.2463, -0.2445, -0.2543,  ..., -4.2193, -4.2042, -3.9466],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [5/32], Loss: 0.0783807635307312\n",
      "Logits: tensor([-0.2466, -0.2473, -0.2468,  ..., -4.1859, -4.1879, -4.2193],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [6/32], Loss: 0.07751413434743881\n",
      "Logits: tensor([-0.2491, -0.2488, -0.2539,  ..., -4.2812, -4.2058, -4.1132],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [7/32], Loss: 0.07766421139240265\n",
      "Logits: tensor([-0.2511, -0.2459, -0.2528,  ..., -4.0967, -4.2454, -4.2655],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [8/32], Loss: 0.07676517963409424\n",
      "Logits: tensor([-0.2580, -0.2565, -0.2543,  ..., -4.2349, -4.1486, -4.0711],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [9/32], Loss: 0.07670941948890686\n",
      "Logits: tensor([-0.2500, -0.2539, -0.2567,  ..., -4.1794, -4.3075, -4.3933],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [10/32], Loss: 0.07621745765209198\n",
      "Logits: tensor([-0.2561, -0.2534, -0.2558,  ..., -4.2781, -4.3073, -4.3564],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [11/32], Loss: 0.07659143954515457\n",
      "Logits: tensor([-0.2554, -0.2566, -0.2579,  ..., -4.2655, -4.2112, -4.3218],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [12/32], Loss: 0.07586158066987991\n",
      "Logits: tensor([-0.2579, -0.2667, -0.2564,  ..., -4.2474, -4.2727, -4.2071],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [13/32], Loss: 0.0750090479850769\n",
      "Logits: tensor([-0.2598, -0.2648, -0.2571,  ..., -4.4840, -4.4618, -4.5670],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [14/32], Loss: 0.07495062798261642\n",
      "Logits: tensor([-0.2603, -0.2628, -0.2638,  ..., -4.4157, -4.1954, -4.3257],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [15/32], Loss: 0.07498127222061157\n",
      "Logits: tensor([-0.2566, -0.2610, -0.2630,  ..., -4.4318, -4.3059, -4.4776],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [16/32], Loss: 0.07493385672569275\n",
      "Logits: tensor([-0.2622, -0.2679, -0.2600,  ..., -4.3856, -4.5327, -4.2730],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [17/32], Loss: 0.07425408065319061\n",
      "Logits: tensor([-0.2670, -0.2678, -0.2649,  ..., -4.5646, -4.4116, -4.3400],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [18/32], Loss: 0.0742613673210144\n",
      "Logits: tensor([-0.2694, -0.2662, -0.2661,  ..., -4.4422, -4.5318, -4.4354],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [19/32], Loss: 0.07395046949386597\n",
      "Logits: tensor([-0.2708, -0.2670, -0.2709,  ..., -4.6527, -4.4375, -4.4588],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [20/32], Loss: 0.07348369807004929\n",
      "Logits: tensor([-0.2687, -0.2662, -0.2702,  ..., -4.4663, -4.5136, -4.5642],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [21/32], Loss: 0.07319789379835129\n",
      "Logits: tensor([-0.2722, -0.2747, -0.2708,  ..., -4.6436, -4.2634, -4.5558],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [22/32], Loss: 0.07301797717809677\n",
      "Logits: tensor([-0.2752, -0.2716, -0.2759,  ..., -4.4352, -4.6842, -4.6028],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [23/32], Loss: 0.07324159890413284\n",
      "Logits: tensor([-0.2772, -0.2757, -0.2727,  ..., -4.6105, -4.4484, -4.6592],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [24/32], Loss: 0.07230100780725479\n",
      "Logits: tensor([-0.2761, -0.2745, -0.2701,  ..., -4.6751, -4.3581, -4.4812],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [25/32], Loss: 0.07331104576587677\n",
      "Logits: tensor([-0.2736, -0.2745, -0.2800,  ..., -4.6451, -4.7218, -4.5472],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [26/32], Loss: 0.07272180914878845\n",
      "Logits: tensor([-0.2761, -0.2729, -0.2781,  ..., -4.6896, -4.5399, -4.5475],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [27/32], Loss: 0.07187313586473465\n",
      "Logits: tensor([-0.2778, -0.2781, -0.2787,  ..., -4.6892, -4.6759, -4.6769],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [28/32], Loss: 0.0720793604850769\n",
      "Logits: tensor([-0.2771, -0.2751, -0.2753,  ..., -4.7684, -4.6021, -4.6673],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [29/32], Loss: 0.07210822403430939\n",
      "Logits: tensor([-0.2809, -0.2819, -0.2798,  ..., -4.7323, -4.7035, -4.7064],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [30/32], Loss: 0.07123389840126038\n",
      "Logits: tensor([-0.2759, -0.2787, -0.2805,  ..., -4.4936, -4.9444, -4.8391],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [31/32], Loss: 0.07143363356590271\n",
      "Logits: tensor([-0.2809, -0.2770, -0.2813,  ..., -4.8544, -4.8214, -4.6075],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Batch [32/32], Loss: 0.07108467817306519\n",
      "Logits: tensor([-0.2823, -0.2807, -0.2775,  ..., -4.8296, -4.7321, -4.7258],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [3/50], Loss: 0.07108467817306519\n",
      "Epoch [4/50], Batch [1/32], Loss: 0.0712851732969284\n",
      "Logits: tensor([-0.2855, -0.2806, -0.2843,  ..., -4.7507, -4.8472, -4.7442],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [2/32], Loss: 0.07207182794809341\n",
      "Logits: tensor([-0.2839, -0.2840, -0.2813,  ..., -4.7936, -4.8516, -4.7431],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [3/32], Loss: 0.07094474136829376\n",
      "Logits: tensor([-0.2841, -0.2790, -0.2877,  ..., -4.8415, -4.8109, -4.7104],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [4/32], Loss: 0.07077161967754364\n",
      "Logits: tensor([-0.2876, -0.2810, -0.2943,  ..., -4.9298, -4.8409, -4.7124],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [5/32], Loss: 0.07086867094039917\n",
      "Logits: tensor([-0.2917, -0.2901, -0.2872,  ..., -4.9410, -4.7957, -4.9052],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [6/32], Loss: 0.0704835057258606\n",
      "Logits: tensor([-0.2969, -0.2852, -0.2880,  ..., -4.9758, -4.9914, -4.9557],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [7/32], Loss: 0.07071851938962936\n",
      "Logits: tensor([-0.2925, -0.2870, -0.2899,  ..., -4.5531, -4.7715, -4.9309],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [8/32], Loss: 0.07068463414907455\n",
      "Logits: tensor([-0.2881, -0.2920, -0.2917,  ..., -5.0230, -5.1035, -4.9029],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [9/32], Loss: 0.07045753300189972\n",
      "Logits: tensor([-0.2937, -0.2918, -0.2920,  ..., -4.9469, -4.7847, -4.6919],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [10/32], Loss: 0.06967266649007797\n",
      "Logits: tensor([-0.2913, -0.2939, -0.2871,  ..., -4.9607, -4.7356, -4.9638],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [11/32], Loss: 0.07064195722341537\n",
      "Logits: tensor([-0.2884, -0.2899, -0.2937,  ..., -5.0074, -5.0776, -4.9484],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [12/32], Loss: 0.07088255882263184\n",
      "Logits: tensor([-0.2983, -0.2947, -0.2952,  ..., -4.8240, -5.0169, -4.9264],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [13/32], Loss: 0.07041015475988388\n",
      "Logits: tensor([-0.2897, -0.2933, -0.2898,  ..., -4.9740, -5.0575, -5.0514],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [14/32], Loss: 0.07002188265323639\n",
      "Logits: tensor([-0.2975, -0.2978, -0.2964,  ..., -4.8005, -4.9719, -5.1339],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [15/32], Loss: 0.06991469115018845\n",
      "Logits: tensor([-0.2942, -0.2963, -0.2925,  ..., -5.0906, -5.0948, -5.1343],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [16/32], Loss: 0.06999678164720535\n",
      "Logits: tensor([-0.2985, -0.2979, -0.2971,  ..., -4.9790, -4.8582, -5.0923],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [17/32], Loss: 0.06989873200654984\n",
      "Logits: tensor([-0.2978, -0.2961, -0.2960,  ..., -4.9770, -5.1942, -5.1664],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [18/32], Loss: 0.06978737562894821\n",
      "Logits: tensor([-0.2949, -0.3026, -0.3004,  ..., -5.0783, -5.1319, -5.1304],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [19/32], Loss: 0.06995563954114914\n",
      "Logits: tensor([-0.2960, -0.2952, -0.2945,  ..., -5.0988, -5.1099, -5.1788],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [20/32], Loss: 0.0696859359741211\n",
      "Logits: tensor([-0.2976, -0.3019, -0.3014,  ..., -5.1184, -4.9485, -5.2247],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [21/32], Loss: 0.06938529014587402\n",
      "Logits: tensor([-0.3041, -0.2963, -0.3046,  ..., -5.0828, -5.0376, -4.9413],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [22/32], Loss: 0.06970279663801193\n",
      "Logits: tensor([-0.2971, -0.3057, -0.3034,  ..., -5.2169, -5.1856, -5.1989],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [23/32], Loss: 0.06940904259681702\n",
      "Logits: tensor([-0.3027, -0.2966, -0.2942,  ..., -4.9790, -5.1108, -5.1014],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [24/32], Loss: 0.0689328983426094\n",
      "Logits: tensor([-0.2973, -0.3040, -0.2980,  ..., -5.1787, -5.2646, -5.0700],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [25/32], Loss: 0.06859704852104187\n",
      "Logits: tensor([-0.3058, -0.3046, -0.3040,  ..., -5.0450, -5.2779, -5.0388],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [26/32], Loss: 0.06940555572509766\n",
      "Logits: tensor([-0.3060, -0.3037, -0.3019,  ..., -5.1415, -5.3531, -5.1375],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [27/32], Loss: 0.06890752911567688\n",
      "Logits: tensor([-0.3068, -0.3072, -0.2998,  ..., -5.2728, -5.2997, -5.3608],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [28/32], Loss: 0.06821752339601517\n",
      "Logits: tensor([-0.3047, -0.3112, -0.3085,  ..., -5.3325, -5.1541, -5.1305],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [29/32], Loss: 0.06882022321224213\n",
      "Logits: tensor([-0.3034, -0.3044, -0.3061,  ..., -5.1791, -5.1580, -5.2442],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [30/32], Loss: 0.0691683441400528\n",
      "Logits: tensor([-0.3015, -0.3071, -0.3051,  ..., -5.1670, -5.2719, -5.0874],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [31/32], Loss: 0.06839297711849213\n",
      "Logits: tensor([-0.3087, -0.3089, -0.3014,  ..., -5.3524, -5.3425, -5.1484],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Batch [32/32], Loss: 0.0687684714794159\n",
      "Logits: tensor([-0.3055, -0.3103, -0.3083,  ..., -5.1854, -5.2508, -5.2094],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [4/50], Loss: 0.0687684714794159\n",
      "Epoch [5/50], Batch [1/32], Loss: 0.06862303614616394\n",
      "Logits: tensor([-0.3102, -0.3079, -0.3113,  ..., -5.3443, -5.1772, -5.3735],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [2/32], Loss: 0.06855012476444244\n",
      "Logits: tensor([-0.3077, -0.3155, -0.3064,  ..., -5.3872, -5.4405, -5.0507],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [3/32], Loss: 0.06817281246185303\n",
      "Logits: tensor([-0.3123, -0.3100, -0.3093,  ..., -5.6001, -5.4031, -5.6072],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [4/32], Loss: 0.06889849901199341\n",
      "Logits: tensor([-0.3164, -0.3100, -0.3109,  ..., -5.2996, -5.2631, -5.2478],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [5/32], Loss: 0.06812310963869095\n",
      "Logits: tensor([-0.3089, -0.3127, -0.3107,  ..., -5.3046, -5.3257, -5.1723],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [6/32], Loss: 0.06850405782461166\n",
      "Logits: tensor([-0.3112, -0.3158, -0.3136,  ..., -5.2972, -5.3761, -5.5281],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [7/32], Loss: 0.06794741004705429\n",
      "Logits: tensor([-0.3119, -0.3157, -0.3136,  ..., -5.5577, -5.3480, -5.3994],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [8/32], Loss: 0.0676150843501091\n",
      "Logits: tensor([-0.3149, -0.3146, -0.3142,  ..., -5.4069, -5.5485, -5.1217],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [9/32], Loss: 0.06809311360120773\n",
      "Logits: tensor([-0.3147, -0.3166, -0.3175,  ..., -5.3617, -5.4565, -5.3330],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [10/32], Loss: 0.06741227954626083\n",
      "Logits: tensor([-0.3180, -0.3140, -0.3130,  ..., -5.5632, -5.4986, -5.4708],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [11/32], Loss: 0.06827478110790253\n",
      "Logits: tensor([-0.3186, -0.3160, -0.3144,  ..., -5.4708, -5.2807, -5.3132],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [12/32], Loss: 0.0681099072098732\n",
      "Logits: tensor([-0.3165, -0.3156, -0.3172,  ..., -5.4188, -5.3992, -5.4348],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [13/32], Loss: 0.06794821470975876\n",
      "Logits: tensor([-0.3154, -0.3146, -0.3118,  ..., -5.4920, -5.3141, -5.6520],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [14/32], Loss: 0.06881621479988098\n",
      "Logits: tensor([-0.3158, -0.3169, -0.3161,  ..., -5.3172, -5.4897, -5.3886],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [15/32], Loss: 0.06775236129760742\n",
      "Logits: tensor([-0.3182, -0.3158, -0.3177,  ..., -5.5083, -5.4061, -5.5479],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [16/32], Loss: 0.06778053939342499\n",
      "Logits: tensor([-0.3196, -0.3198, -0.3145,  ..., -5.5016, -5.5409, -5.5153],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [17/32], Loss: 0.06785761564970016\n",
      "Logits: tensor([-0.3252, -0.3154, -0.3237,  ..., -5.6100, -5.6202, -5.4162],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [18/32], Loss: 0.06791136413812637\n",
      "Logits: tensor([-0.3204, -0.3106, -0.3230,  ..., -5.5629, -5.7213, -5.5497],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [19/32], Loss: 0.06808578968048096\n",
      "Logits: tensor([-0.3233, -0.3198, -0.3197,  ..., -5.3247, -5.5481, -5.3758],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [20/32], Loss: 0.06847453862428665\n",
      "Logits: tensor([-0.3211, -0.3198, -0.3196,  ..., -5.3807, -5.2885, -5.5322],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [21/32], Loss: 0.06819714605808258\n",
      "Logits: tensor([-0.3201, -0.3168, -0.3260,  ..., -5.7241, -5.6250, -5.5646],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [22/32], Loss: 0.0676971897482872\n",
      "Logits: tensor([-0.3251, -0.3271, -0.3207,  ..., -5.5948, -5.6193, -5.5873],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [23/32], Loss: 0.06748893111944199\n",
      "Logits: tensor([-0.3225, -0.3230, -0.3203,  ..., -5.6914, -5.5490, -5.5843],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [24/32], Loss: 0.0677337497472763\n",
      "Logits: tensor([-0.3186, -0.3189, -0.3277,  ..., -5.4680, -5.5309, -5.5968],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [25/32], Loss: 0.06787272542715073\n",
      "Logits: tensor([-0.3255, -0.3238, -0.3202,  ..., -5.7846, -5.3419, -5.5969],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [26/32], Loss: 0.06802123785018921\n",
      "Logits: tensor([-0.3245, -0.3258, -0.3240,  ..., -5.5157, -5.7360, -5.5621],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [27/32], Loss: 0.06709137558937073\n",
      "Logits: tensor([-0.3279, -0.3259, -0.3281,  ..., -5.6760, -5.4165, -5.6997],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [28/32], Loss: 0.06766477227210999\n",
      "Logits: tensor([-0.3181, -0.3240, -0.3258,  ..., -5.4721, -5.3927, -5.5167],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [29/32], Loss: 0.06706199049949646\n",
      "Logits: tensor([-0.3276, -0.3300, -0.3271,  ..., -5.5227, -5.5682, -5.6085],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [30/32], Loss: 0.0676601231098175\n",
      "Logits: tensor([-0.3276, -0.3236, -0.3262,  ..., -5.8400, -5.5638, -5.5671],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [31/32], Loss: 0.06769836694002151\n",
      "Logits: tensor([-0.3255, -0.3357, -0.3292,  ..., -5.5340, -5.5628, -5.4703],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Batch [32/32], Loss: 0.06819959729909897\n",
      "Logits: tensor([-0.3274, -0.3256, -0.3315,  ..., -5.6371, -5.4458, -5.6647],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [5/50], Loss: 0.06819959729909897\n",
      "Epoch [6/50], Batch [1/32], Loss: 0.06746678799390793\n",
      "Logits: tensor([-0.3253, -0.3226, -0.3256,  ..., -5.6413, -5.4759, -5.7841],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [2/32], Loss: 0.06731493026018143\n",
      "Logits: tensor([-0.3273, -0.3218, -0.3281,  ..., -5.6067, -5.3946, -5.5850],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [3/32], Loss: 0.06750195473432541\n",
      "Logits: tensor([-0.3230, -0.3301, -0.3356,  ..., -5.7097, -5.8390, -5.8147],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [4/32], Loss: 0.06678789108991623\n",
      "Logits: tensor([-0.3299, -0.3314, -0.3312,  ..., -5.6911, -5.6963, -5.7020],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [5/32], Loss: 0.06735529005527496\n",
      "Logits: tensor([-0.3335, -0.3334, -0.3278,  ..., -5.5241, -5.4566, -5.7326],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [6/32], Loss: 0.06725464016199112\n",
      "Logits: tensor([-0.3268, -0.3311, -0.3308,  ..., -5.7904, -5.7398, -5.7461],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [7/32], Loss: 0.06660176068544388\n",
      "Logits: tensor([-0.3316, -0.3271, -0.3349,  ..., -5.4757, -5.7628, -5.6419],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [8/32], Loss: 0.06683716922998428\n",
      "Logits: tensor([-0.3366, -0.3321, -0.3301,  ..., -5.6638, -5.9986, -5.5378],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [9/32], Loss: 0.06663569808006287\n",
      "Logits: tensor([-0.3325, -0.3275, -0.3387,  ..., -5.8699, -5.6795, -5.8562],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [10/32], Loss: 0.06700323522090912\n",
      "Logits: tensor([-0.3332, -0.3335, -0.3341,  ..., -5.8663, -5.7546, -5.7635],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [11/32], Loss: 0.06744607537984848\n",
      "Logits: tensor([-0.3345, -0.3369, -0.3340,  ..., -5.8343, -5.7081, -5.9047],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [12/32], Loss: 0.0673934817314148\n",
      "Logits: tensor([-0.3345, -0.3303, -0.3324,  ..., -5.8569, -5.7369, -5.6299],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [13/32], Loss: 0.06759315729141235\n",
      "Logits: tensor([-0.3298, -0.3349, -0.3264,  ..., -5.6991, -5.7585, -5.8059],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [14/32], Loss: 0.06639701128005981\n",
      "Logits: tensor([-0.3378, -0.3295, -0.3329,  ..., -5.9304, -5.8396, -5.7235],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [15/32], Loss: 0.06671155989170074\n",
      "Logits: tensor([-0.3342, -0.3347, -0.3314,  ..., -5.7400, -5.6097, -5.9560],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [16/32], Loss: 0.06744848191738129\n",
      "Logits: tensor([-0.3378, -0.3309, -0.3349,  ..., -5.6234, -5.7704, -5.7580],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [17/32], Loss: 0.06702567636966705\n",
      "Logits: tensor([-0.3337, -0.3350, -0.3360,  ..., -5.8821, -5.8077, -5.7945],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [18/32], Loss: 0.06724631041288376\n",
      "Logits: tensor([-0.3410, -0.3303, -0.3339,  ..., -6.1534, -5.7963, -5.8615],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [19/32], Loss: 0.06669387221336365\n",
      "Logits: tensor([-0.3395, -0.3367, -0.3361,  ..., -6.0810, -5.7182, -5.8589],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [20/32], Loss: 0.06634076684713364\n",
      "Logits: tensor([-0.3412, -0.3395, -0.3391,  ..., -6.0817, -6.0852, -5.7104],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [21/32], Loss: 0.06611024588346481\n",
      "Logits: tensor([-0.3347, -0.3376, -0.3404,  ..., -5.9380, -5.9044, -5.6652],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [22/32], Loss: 0.06740815937519073\n",
      "Logits: tensor([-0.3415, -0.3439, -0.3378,  ..., -5.7616, -5.8759, -5.7749],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [23/32], Loss: 0.06730640679597855\n",
      "Logits: tensor([-0.3385, -0.3351, -0.3356,  ..., -5.8968, -5.9732, -5.9026],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [24/32], Loss: 0.06632252782583237\n",
      "Logits: tensor([-0.3406, -0.3368, -0.3403,  ..., -5.7056, -6.0185, -5.8187],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [25/32], Loss: 0.06721474975347519\n",
      "Logits: tensor([-0.3379, -0.3324, -0.3417,  ..., -5.9228, -5.9441, -5.6860],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [26/32], Loss: 0.06650319695472717\n",
      "Logits: tensor([-0.3433, -0.3382, -0.3382,  ..., -5.8734, -5.8334, -5.8180],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [27/32], Loss: 0.06701409071683884\n",
      "Logits: tensor([-0.3402, -0.3364, -0.3362,  ..., -5.8674, -5.7971, -5.6492],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [28/32], Loss: 0.06688568741083145\n",
      "Logits: tensor([-0.3367, -0.3435, -0.3411,  ..., -6.0559, -5.8292, -6.0184],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [29/32], Loss: 0.06641078740358353\n",
      "Logits: tensor([-0.3466, -0.3442, -0.3410,  ..., -6.0331, -5.9215, -5.8283],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [30/32], Loss: 0.06677421927452087\n",
      "Logits: tensor([-0.3435, -0.3416, -0.3425,  ..., -5.9452, -6.1857, -5.6231],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [31/32], Loss: 0.06766218692064285\n",
      "Logits: tensor([-0.3431, -0.3370, -0.3412,  ..., -6.0448, -5.9132, -5.7776],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Batch [32/32], Loss: 0.06715260446071625\n",
      "Logits: tensor([-0.3453, -0.3425, -0.3423,  ..., -5.7911, -5.9064, -5.7748],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [6/50], Loss: 0.06715260446071625\n",
      "Epoch [7/50], Batch [1/32], Loss: 0.0659710168838501\n",
      "Logits: tensor([-0.3410, -0.3437, -0.3449,  ..., -5.9267, -6.0974, -5.8820],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [2/32], Loss: 0.06714373826980591\n",
      "Logits: tensor([-0.3451, -0.3427, -0.3379,  ..., -5.9774, -5.9977, -5.9285],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [3/32], Loss: 0.06680095940828323\n",
      "Logits: tensor([-0.3409, -0.3401, -0.3465,  ..., -6.0972, -6.0102, -6.1270],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [4/32], Loss: 0.06684023886919022\n",
      "Logits: tensor([-0.3469, -0.3480, -0.3470,  ..., -5.8099, -6.1853, -6.0015],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [5/32], Loss: 0.0660858005285263\n",
      "Logits: tensor([-0.3442, -0.3422, -0.3441,  ..., -5.7945, -5.7833, -5.9810],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [6/32], Loss: 0.06606955081224442\n",
      "Logits: tensor([-0.3376, -0.3447, -0.3427,  ..., -6.0078, -6.0996, -5.8536],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [7/32], Loss: 0.0665755495429039\n",
      "Logits: tensor([-0.3462, -0.3495, -0.3448,  ..., -6.1361, -5.9832, -6.0211],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [8/32], Loss: 0.06614786386489868\n",
      "Logits: tensor([-0.3456, -0.3462, -0.3466,  ..., -6.0221, -6.1005, -5.7793],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [9/32], Loss: 0.06651810556650162\n",
      "Logits: tensor([-0.3470, -0.3474, -0.3384,  ..., -6.0638, -5.9266, -5.8169],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [10/32], Loss: 0.06627573072910309\n",
      "Logits: tensor([-0.3495, -0.3470, -0.3445,  ..., -6.1156, -5.9483, -5.9259],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [11/32], Loss: 0.06600990146398544\n",
      "Logits: tensor([-0.3425, -0.3448, -0.3435,  ..., -6.2630, -6.1347, -6.0899],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [12/32], Loss: 0.06653330475091934\n",
      "Logits: tensor([-0.3456, -0.3455, -0.3511,  ..., -5.6229, -6.0601, -5.8274],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [13/32], Loss: 0.06610749661922455\n",
      "Logits: tensor([-0.3452, -0.3448, -0.3427,  ..., -6.0089, -6.1205, -5.8852],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [14/32], Loss: 0.06672482937574387\n",
      "Logits: tensor([-0.3447, -0.3481, -0.3514,  ..., -6.1819, -6.0789, -5.8238],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [15/32], Loss: 0.06608601659536362\n",
      "Logits: tensor([-0.3500, -0.3481, -0.3479,  ..., -6.1671, -6.1353, -6.0495],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [16/32], Loss: 0.06688818335533142\n",
      "Logits: tensor([-0.3406, -0.3488, -0.3521,  ..., -5.9175, -6.0352, -5.9009],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [17/32], Loss: 0.0669076144695282\n",
      "Logits: tensor([-0.3489, -0.3493, -0.3437,  ..., -6.1667, -6.1936, -6.0274],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [18/32], Loss: 0.06584425270557404\n",
      "Logits: tensor([-0.3512, -0.3442, -0.3482,  ..., -6.0138, -6.0524, -6.0379],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [19/32], Loss: 0.06630796939134598\n",
      "Logits: tensor([-0.3513, -0.3483, -0.3454,  ..., -6.2480, -6.3464, -6.0969],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [20/32], Loss: 0.0661042332649231\n",
      "Logits: tensor([-0.3465, -0.3488, -0.3490,  ..., -6.1061, -6.0066, -6.0318],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [21/32], Loss: 0.06665966659784317\n",
      "Logits: tensor([-0.3440, -0.3499, -0.3456,  ..., -6.1901, -5.9960, -6.2245],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [22/32], Loss: 0.06621084362268448\n",
      "Logits: tensor([-0.3425, -0.3496, -0.3438,  ..., -6.1583, -6.3940, -6.1326],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [23/32], Loss: 0.06641349196434021\n",
      "Logits: tensor([-0.3494, -0.3481, -0.3528,  ..., -6.0629, -6.2332, -6.1865],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [24/32], Loss: 0.06750095635652542\n",
      "Logits: tensor([-0.3495, -0.3475, -0.3571,  ..., -6.0733, -6.1682, -6.3539],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [25/32], Loss: 0.06684576719999313\n",
      "Logits: tensor([-0.3506, -0.3490, -0.3468,  ..., -6.0865, -6.2235, -6.1137],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [26/32], Loss: 0.06601039320230484\n",
      "Logits: tensor([-0.3470, -0.3517, -0.3516,  ..., -6.1743, -6.1611, -6.2770],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [27/32], Loss: 0.0667019933462143\n",
      "Logits: tensor([-0.3541, -0.3541, -0.3512,  ..., -6.1045, -6.0984, -6.1365],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [28/32], Loss: 0.06644950062036514\n",
      "Logits: tensor([-0.3540, -0.3480, -0.3525,  ..., -6.3165, -6.2504, -6.2043],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [29/32], Loss: 0.06642994284629822\n",
      "Logits: tensor([-0.3524, -0.3506, -0.3448,  ..., -6.1835, -6.2955, -6.2100],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [30/32], Loss: 0.06638872623443604\n",
      "Logits: tensor([-0.3553, -0.3528, -0.3510,  ..., -6.2976, -6.2847, -6.0590],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [31/32], Loss: 0.06590360403060913\n",
      "Logits: tensor([-0.3564, -0.3531, -0.3574,  ..., -6.1275, -6.3515, -6.1965],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Batch [32/32], Loss: 0.0660565048456192\n",
      "Logits: tensor([-0.3480, -0.3542, -0.3513,  ..., -6.1317, -6.1755, -6.1194],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [7/50], Loss: 0.0660565048456192\n",
      "Epoch [8/50], Batch [1/32], Loss: 0.06640441715717316\n",
      "Logits: tensor([-0.3470, -0.3516, -0.3509,  ..., -6.3695, -6.3037, -6.1979],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [2/32], Loss: 0.06583631783723831\n",
      "Logits: tensor([-0.3516, -0.3592, -0.3538,  ..., -6.0768, -6.3331, -6.1979],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [3/32], Loss: 0.06539588421583176\n",
      "Logits: tensor([-0.3559, -0.3564, -0.3516,  ..., -6.2615, -6.4235, -5.9304],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [4/32], Loss: 0.06626813113689423\n",
      "Logits: tensor([-0.3590, -0.3562, -0.3523,  ..., -6.3700, -5.8487, -6.2551],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [5/32], Loss: 0.06587496399879456\n",
      "Logits: tensor([-0.3570, -0.3539, -0.3510,  ..., -6.2476, -6.3043, -6.0184],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [6/32], Loss: 0.06693430989980698\n",
      "Logits: tensor([-0.3614, -0.3515, -0.3558,  ..., -6.2747, -6.4157, -6.3901],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [7/32], Loss: 0.06599916517734528\n",
      "Logits: tensor([-0.3534, -0.3572, -0.3549,  ..., -6.0597, -6.3267, -6.4915],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [8/32], Loss: 0.06604193896055222\n",
      "Logits: tensor([-0.3589, -0.3560, -0.3558,  ..., -6.3606, -6.4244, -5.9631],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [9/32], Loss: 0.06674249470233917\n",
      "Logits: tensor([-0.3531, -0.3606, -0.3516,  ..., -6.2038, -6.1958, -6.3759],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [10/32], Loss: 0.06652727723121643\n",
      "Logits: tensor([-0.3588, -0.3551, -0.3533,  ..., -6.2945, -6.3756, -6.3004],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [11/32], Loss: 0.06578672677278519\n",
      "Logits: tensor([-0.3557, -0.3581, -0.3527,  ..., -6.3808, -6.1964, -6.3402],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [12/32], Loss: 0.06692076474428177\n",
      "Logits: tensor([-0.3563, -0.3546, -0.3524,  ..., -6.4328, -6.2073, -6.4097],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [13/32], Loss: 0.06551367789506912\n",
      "Logits: tensor([-0.3578, -0.3576, -0.3574,  ..., -6.2895, -6.3487, -6.4393],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [14/32], Loss: 0.06619323790073395\n",
      "Logits: tensor([-0.3609, -0.3556, -0.3557,  ..., -6.0387, -6.4639, -6.4099],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [15/32], Loss: 0.06612692028284073\n",
      "Logits: tensor([-0.3568, -0.3571, -0.3615,  ..., -6.2761, -6.2592, -6.3055],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [16/32], Loss: 0.0663725733757019\n",
      "Logits: tensor([-0.3538, -0.3665, -0.3555,  ..., -6.2061, -6.4050, -6.2828],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [17/32], Loss: 0.06526268273591995\n",
      "Logits: tensor([-0.3604, -0.3584, -0.3583,  ..., -6.4560, -6.2454, -6.4413],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [18/32], Loss: 0.06567759066820145\n",
      "Logits: tensor([-0.3593, -0.3539, -0.3521,  ..., -6.2688, -6.3855, -6.1400],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [19/32], Loss: 0.06626728922128677\n",
      "Logits: tensor([-0.3581, -0.3602, -0.3592,  ..., -6.4613, -6.3418, -6.0315],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [20/32], Loss: 0.06653816252946854\n",
      "Logits: tensor([-0.3586, -0.3564, -0.3607,  ..., -6.1969, -6.2266, -6.4807],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [21/32], Loss: 0.0652766078710556\n",
      "Logits: tensor([-0.3643, -0.3620, -0.3614,  ..., -6.3797, -6.2356, -6.1198],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [22/32], Loss: 0.0660758838057518\n",
      "Logits: tensor([-0.3619, -0.3625, -0.3564,  ..., -6.1965, -6.3041, -6.3146],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [23/32], Loss: 0.0658767968416214\n",
      "Logits: tensor([-0.3615, -0.3622, -0.3599,  ..., -6.3636, -6.4583, -6.3384],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [24/32], Loss: 0.06566470861434937\n",
      "Logits: tensor([-0.3676, -0.3609, -0.3627,  ..., -6.2866, -6.3848, -6.5773],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [25/32], Loss: 0.06659142673015594\n",
      "Logits: tensor([-0.3647, -0.3629, -0.3648,  ..., -6.3996, -6.2867, -6.3835],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [26/32], Loss: 0.06620118767023087\n",
      "Logits: tensor([-0.3532, -0.3651, -0.3618,  ..., -6.4564, -6.3625, -6.3447],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [27/32], Loss: 0.06645777821540833\n",
      "Logits: tensor([-0.3618, -0.3598, -0.3662,  ..., -6.5813, -6.4634, -6.4309],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [28/32], Loss: 0.06598778814077377\n",
      "Logits: tensor([-0.3585, -0.3614, -0.3558,  ..., -6.4000, -6.3704, -6.5591],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [29/32], Loss: 0.06562503427267075\n",
      "Logits: tensor([-0.3600, -0.3554, -0.3579,  ..., -6.3615, -6.4137, -6.3180],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [30/32], Loss: 0.06640035659074783\n",
      "Logits: tensor([-0.3634, -0.3651, -0.3629,  ..., -6.1949, -6.3615, -6.4052],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [31/32], Loss: 0.06607389450073242\n",
      "Logits: tensor([-0.3648, -0.3592, -0.3626,  ..., -6.2193, -6.6279, -6.3461],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Batch [32/32], Loss: 0.06509526818990707\n",
      "Logits: tensor([-0.3610, -0.3628, -0.3644,  ..., -6.2476, -6.4084, -6.2398],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [8/50], Loss: 0.06509526818990707\n",
      "Epoch [9/50], Batch [1/32], Loss: 0.06555740535259247\n",
      "Logits: tensor([-0.3617, -0.3599, -0.3660,  ..., -6.3901, -6.5684, -6.6317],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [2/32], Loss: 0.06601689755916595\n",
      "Logits: tensor([-0.3603, -0.3624, -0.3645,  ..., -5.8340, -6.4279, -6.3518],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [3/32], Loss: 0.0651504322886467\n",
      "Logits: tensor([-0.3571, -0.3622, -0.3632,  ..., -6.4048, -6.4597, -6.5325],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [4/32], Loss: 0.06610701978206635\n",
      "Logits: tensor([-0.3612, -0.3621, -0.3606,  ..., -6.5812, -6.4725, -6.6334],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [5/32], Loss: 0.06647443026304245\n",
      "Logits: tensor([-0.3563, -0.3645, -0.3661,  ..., -6.3651, -6.4197, -6.1636],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [6/32], Loss: 0.06600441783666611\n",
      "Logits: tensor([-0.3621, -0.3675, -0.3614,  ..., -6.3938, -6.5147, -6.3316],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [7/32], Loss: 0.06558893620967865\n",
      "Logits: tensor([-0.3672, -0.3688, -0.3628,  ..., -6.4307, -6.6452, -6.2299],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [8/32], Loss: 0.06564848124980927\n",
      "Logits: tensor([-0.3607, -0.3651, -0.3656,  ..., -6.3437, -6.7158, -6.5143],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [9/32], Loss: 0.06637796014547348\n",
      "Logits: tensor([-0.3634, -0.3593, -0.3662,  ..., -6.5100, -6.3974, -6.5642],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [10/32], Loss: 0.06663770228624344\n",
      "Logits: tensor([-0.3625, -0.3636, -0.3638,  ..., -6.4606, -6.6215, -6.6224],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [11/32], Loss: 0.06569894403219223\n",
      "Logits: tensor([-0.3626, -0.3653, -0.3599,  ..., -6.5908, -6.3427, -6.3430],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [12/32], Loss: 0.06570263206958771\n",
      "Logits: tensor([-0.3641, -0.3681, -0.3646,  ..., -6.5423, -6.2217, -6.6300],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [13/32], Loss: 0.06612545996904373\n",
      "Logits: tensor([-0.3633, -0.3612, -0.3612,  ..., -6.6431, -6.5454, -6.6458],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [14/32], Loss: 0.0661734789609909\n",
      "Logits: tensor([-0.3674, -0.3636, -0.3612,  ..., -6.1110, -6.6329, -6.4229],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [15/32], Loss: 0.06504461169242859\n",
      "Logits: tensor([-0.3684, -0.3629, -0.3631,  ..., -6.4981, -6.5509, -6.6251],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [16/32], Loss: 0.06566603481769562\n",
      "Logits: tensor([-0.3660, -0.3581, -0.3615,  ..., -6.5121, -6.3026, -6.5717],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [17/32], Loss: 0.06556862592697144\n",
      "Logits: tensor([-0.3641, -0.3742, -0.3637,  ..., -6.8701, -6.3805, -6.5195],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [18/32], Loss: 0.06627623736858368\n",
      "Logits: tensor([-0.3716, -0.3697, -0.3674,  ..., -6.5309, -6.6507, -6.6493],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [19/32], Loss: 0.06545272469520569\n",
      "Logits: tensor([-0.3671, -0.3627, -0.3649,  ..., -6.6682, -6.5744, -6.4234],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [20/32], Loss: 0.06522297859191895\n",
      "Logits: tensor([-0.3646, -0.3692, -0.3672,  ..., -6.5740, -6.4939, -6.4618],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [21/32], Loss: 0.06508739292621613\n",
      "Logits: tensor([-0.3711, -0.3663, -0.3648,  ..., -6.5608, -6.7078, -6.6815],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [22/32], Loss: 0.06639932841062546\n",
      "Logits: tensor([-0.3689, -0.3669, -0.3679,  ..., -6.6809, -6.6955, -6.4495],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [23/32], Loss: 0.06593883782625198\n",
      "Logits: tensor([-0.3656, -0.3730, -0.3673,  ..., -6.6270, -6.6993, -6.5099],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [24/32], Loss: 0.06585106253623962\n",
      "Logits: tensor([-0.3721, -0.3676, -0.3663,  ..., -6.5924, -6.2743, -6.4700],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [25/32], Loss: 0.06582240760326385\n",
      "Logits: tensor([-0.3677, -0.3621, -0.3698,  ..., -6.5830, -6.7673, -6.7180],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [26/32], Loss: 0.06568469852209091\n",
      "Logits: tensor([-0.3608, -0.3705, -0.3708,  ..., -6.4441, -6.4710, -6.3733],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [27/32], Loss: 0.06645622849464417\n",
      "Logits: tensor([-0.3709, -0.3690, -0.3645,  ..., -6.4162, -6.5933, -6.4958],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [28/32], Loss: 0.06628698110580444\n",
      "Logits: tensor([-0.3675, -0.3720, -0.3694,  ..., -6.3969, -6.5303, -6.5336],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [29/32], Loss: 0.0658174678683281\n",
      "Logits: tensor([-0.3662, -0.3690, -0.3696,  ..., -6.7703, -6.5503, -6.4489],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [30/32], Loss: 0.06584975123405457\n",
      "Logits: tensor([-0.3721, -0.3692, -0.3720,  ..., -6.5277, -6.5345, -6.4937],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [31/32], Loss: 0.06604284048080444\n",
      "Logits: tensor([-0.3702, -0.3669, -0.3725,  ..., -6.7283, -6.1775, -6.6078],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Batch [32/32], Loss: 0.06554734706878662\n",
      "Logits: tensor([-0.3670, -0.3690, -0.3681,  ..., -6.6089, -6.5645, -6.7207],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [9/50], Loss: 0.06554734706878662\n",
      "Epoch [10/50], Batch [1/32], Loss: 0.06669806689023972\n",
      "Logits: tensor([-0.3725, -0.3671, -0.3685,  ..., -6.8543, -6.6398, -6.6103],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [2/32], Loss: 0.06502187997102737\n",
      "Logits: tensor([-0.3715, -0.3721, -0.3659,  ..., -6.6399, -6.7163, -6.4686],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [3/32], Loss: 0.0653068870306015\n",
      "Logits: tensor([-0.3698, -0.3683, -0.3728,  ..., -6.6047, -6.7051, -6.4345],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [4/32], Loss: 0.0653715506196022\n",
      "Logits: tensor([-0.3687, -0.3736, -0.3643,  ..., -6.6166, -6.3829, -6.5497],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [5/32], Loss: 0.06557571887969971\n",
      "Logits: tensor([-0.3752, -0.3711, -0.3678,  ..., -6.6186, -6.5801, -6.7940],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [6/32], Loss: 0.0651426762342453\n",
      "Logits: tensor([-0.3726, -0.3721, -0.3681,  ..., -6.5860, -6.5532, -6.7052],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [7/32], Loss: 0.06581000983715057\n",
      "Logits: tensor([-0.3690, -0.3717, -0.3739,  ..., -6.7670, -6.6703, -6.6557],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [8/32], Loss: 0.06566345691680908\n",
      "Logits: tensor([-0.3681, -0.3707, -0.3658,  ..., -6.4899, -6.7912, -6.6811],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [9/32], Loss: 0.0650835931301117\n",
      "Logits: tensor([-0.3680, -0.3693, -0.3704,  ..., -6.8398, -6.7174, -6.6830],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [10/32], Loss: 0.06668310612440109\n",
      "Logits: tensor([-0.3741, -0.3746, -0.3702,  ..., -6.4910, -6.6880, -6.9007],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [11/32], Loss: 0.06552319973707199\n",
      "Logits: tensor([-0.3756, -0.3711, -0.3697,  ..., -6.5227, -6.5431, -6.7856],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [12/32], Loss: 0.0657721534371376\n",
      "Logits: tensor([-0.3692, -0.3620, -0.3716,  ..., -6.8060, -6.7002, -6.5652],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [13/32], Loss: 0.06569589674472809\n",
      "Logits: tensor([-0.3661, -0.3753, -0.3728,  ..., -6.7056, -6.8057, -6.7824],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [14/32], Loss: 0.06583540141582489\n",
      "Logits: tensor([-0.3734, -0.3680, -0.3676,  ..., -6.7123, -6.9425, -6.6208],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [15/32], Loss: 0.06546537578105927\n",
      "Logits: tensor([-0.3660, -0.3703, -0.3660,  ..., -6.5640, -6.6718, -6.7154],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [16/32], Loss: 0.06585801392793655\n",
      "Logits: tensor([-0.3668, -0.3749, -0.3725,  ..., -6.5167, -6.6970, -6.5977],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [17/32], Loss: 0.06555648893117905\n",
      "Logits: tensor([-0.3684, -0.3656, -0.3689,  ..., -6.7591, -6.7748, -6.6761],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [18/32], Loss: 0.06602341681718826\n",
      "Logits: tensor([-0.3736, -0.3720, -0.3700,  ..., -6.6596, -6.7963, -6.7871],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [19/32], Loss: 0.06527367979288101\n",
      "Logits: tensor([-0.3767, -0.3650, -0.3753,  ..., -6.7435, -6.4872, -6.4413],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [20/32], Loss: 0.06568285077810287\n",
      "Logits: tensor([-0.3673, -0.3742, -0.3765,  ..., -6.8452, -6.7440, -6.8478],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [21/32], Loss: 0.06535910069942474\n",
      "Logits: tensor([-0.3712, -0.3771, -0.3682,  ..., -6.8435, -6.9169, -6.2829],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [22/32], Loss: 0.06558656692504883\n",
      "Logits: tensor([-0.3704, -0.3715, -0.3673,  ..., -6.7557, -6.8503, -6.6056],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [23/32], Loss: 0.0663447305560112\n",
      "Logits: tensor([-0.3738, -0.3748, -0.3712,  ..., -6.4863, -6.4065, -6.7296],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [24/32], Loss: 0.06576959788799286\n",
      "Logits: tensor([-0.3754, -0.3733, -0.3685,  ..., -6.7610, -6.7912, -6.8447],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [25/32], Loss: 0.06633078306913376\n",
      "Logits: tensor([-0.3727, -0.3740, -0.3723,  ..., -6.5605, -6.6946, -6.5237],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [26/32], Loss: 0.06509901583194733\n",
      "Logits: tensor([-0.3725, -0.3707, -0.3685,  ..., -6.6109, -6.7948, -6.9041],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [27/32], Loss: 0.0650162324309349\n",
      "Logits: tensor([-0.3650, -0.3716, -0.3767,  ..., -6.8239, -6.9219, -6.8088],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [28/32], Loss: 0.06617021560668945\n",
      "Logits: tensor([-0.3728, -0.3770, -0.3765,  ..., -6.4557, -6.8277, -6.8146],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [29/32], Loss: 0.06636279821395874\n",
      "Logits: tensor([-0.3753, -0.3739, -0.3771,  ..., -6.6197, -6.4895, -6.9245],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [30/32], Loss: 0.06540033221244812\n",
      "Logits: tensor([-0.3715, -0.3738, -0.3723,  ..., -6.3432, -7.1167, -6.8437],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [31/32], Loss: 0.06624392420053482\n",
      "Logits: tensor([-0.3757, -0.3727, -0.3698,  ..., -6.8726, -6.7174, -6.6250],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Batch [32/32], Loss: 0.0660208910703659\n",
      "Logits: tensor([-0.3720, -0.3721, -0.3726,  ..., -6.8735, -6.5663, -6.7933],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [10/50], Loss: 0.0660208910703659\n",
      "Epoch [11/50], Batch [1/32], Loss: 0.0661737322807312\n",
      "Logits: tensor([-0.3741, -0.3744, -0.3731,  ..., -6.8345, -6.7634, -6.8443],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [2/32], Loss: 0.0666089579463005\n",
      "Logits: tensor([-0.3758, -0.3762, -0.3704,  ..., -6.7388, -6.8587, -6.9349],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [3/32], Loss: 0.06539079546928406\n",
      "Logits: tensor([-0.3726, -0.3761, -0.3716,  ..., -6.7464, -6.8111, -6.9073],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [4/32], Loss: 0.06630786508321762\n",
      "Logits: tensor([-0.3775, -0.3745, -0.3744,  ..., -6.7393, -6.4892, -6.8224],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [5/32], Loss: 0.06529677659273148\n",
      "Logits: tensor([-0.3706, -0.3745, -0.3744,  ..., -6.5277, -6.7994, -6.5890],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [6/32], Loss: 0.06547129154205322\n",
      "Logits: tensor([-0.3758, -0.3731, -0.3718,  ..., -7.1468, -6.8394, -6.7060],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [7/32], Loss: 0.06558903306722641\n",
      "Logits: tensor([-0.3702, -0.3761, -0.3792,  ..., -6.8749, -7.0380, -6.8125],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [8/32], Loss: 0.06509213894605637\n",
      "Logits: tensor([-0.3754, -0.3777, -0.3742,  ..., -6.7906, -6.8757, -6.9157],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [9/32], Loss: 0.06556051224470139\n",
      "Logits: tensor([-0.3747, -0.3701, -0.3762,  ..., -6.9416, -6.8086, -6.8149],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [10/32], Loss: 0.06553741544485092\n",
      "Logits: tensor([-0.3721, -0.3721, -0.3715,  ..., -6.8178, -6.7145, -6.5950],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [11/32], Loss: 0.0665411502122879\n",
      "Logits: tensor([-0.3743, -0.3736, -0.3776,  ..., -6.9524, -6.7117, -6.6150],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [12/32], Loss: 0.06543538719415665\n",
      "Logits: tensor([-0.3713, -0.3758, -0.3722,  ..., -6.7309, -6.8183, -6.8507],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [13/32], Loss: 0.06558255106210709\n",
      "Logits: tensor([-0.3783, -0.3689, -0.3834,  ..., -6.7993, -6.7086, -6.8398],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [14/32], Loss: 0.0649939626455307\n",
      "Logits: tensor([-0.3786, -0.3769, -0.3737,  ..., -6.8037, -6.7762, -6.6681],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [15/32], Loss: 0.06504559516906738\n",
      "Logits: tensor([-0.3758, -0.3729, -0.3762,  ..., -6.7965, -6.7243, -6.9363],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [16/32], Loss: 0.06538839638233185\n",
      "Logits: tensor([-0.3796, -0.3723, -0.3744,  ..., -6.8056, -6.9065, -6.6518],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [17/32], Loss: 0.06605437397956848\n",
      "Logits: tensor([-0.3814, -0.3794, -0.3772,  ..., -6.7615, -6.9365, -6.6066],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [18/32], Loss: 0.06593850255012512\n",
      "Logits: tensor([-0.3729, -0.3802, -0.3776,  ..., -6.8848, -6.7437, -6.8874],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [19/32], Loss: 0.06520603597164154\n",
      "Logits: tensor([-0.3780, -0.3760, -0.3770,  ..., -6.9820, -6.7909, -6.6094],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [20/32], Loss: 0.06587707251310349\n",
      "Logits: tensor([-0.3805, -0.3758, -0.3742,  ..., -6.8618, -6.8535, -6.8407],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [21/32], Loss: 0.06598039716482162\n",
      "Logits: tensor([-0.3769, -0.3729, -0.3754,  ..., -6.7990, -6.7771, -6.8992],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [22/32], Loss: 0.06619413197040558\n",
      "Logits: tensor([-0.3750, -0.3713, -0.3796,  ..., -6.6744, -6.8777, -7.0967],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [23/32], Loss: 0.06555888801813126\n",
      "Logits: tensor([-0.3769, -0.3786, -0.3705,  ..., -6.6528, -6.8231, -6.6446],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [24/32], Loss: 0.0652313232421875\n",
      "Logits: tensor([-0.3841, -0.3759, -0.3796,  ..., -7.0444, -6.9822, -6.8026],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [25/32], Loss: 0.06580238044261932\n",
      "Logits: tensor([-0.3745, -0.3747, -0.3785,  ..., -7.1165, -6.6485, -6.7966],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [26/32], Loss: 0.06503461301326752\n",
      "Logits: tensor([-0.3774, -0.3762, -0.3787,  ..., -6.8021, -7.1922, -6.6870],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [27/32], Loss: 0.06502951681613922\n",
      "Logits: tensor([-0.3744, -0.3788, -0.3785,  ..., -6.9760, -7.0391, -6.9162],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [28/32], Loss: 0.06554868072271347\n",
      "Logits: tensor([-0.3796, -0.3767, -0.3739,  ..., -6.8473, -6.9827, -6.8801],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [29/32], Loss: 0.06524448096752167\n",
      "Logits: tensor([-0.3819, -0.3730, -0.3738,  ..., -7.0055, -6.8901, -7.0616],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [30/32], Loss: 0.06497558951377869\n",
      "Logits: tensor([-0.3753, -0.3763, -0.3765,  ..., -7.0584, -6.9328, -6.7840],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [31/32], Loss: 0.06523189693689346\n",
      "Logits: tensor([-0.3791, -0.3742, -0.3756,  ..., -6.9325, -7.0619, -6.7994],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Batch [32/32], Loss: 0.06696206331253052\n",
      "Logits: tensor([-0.3758, -0.3814, -0.3752,  ..., -6.9920, -6.9419, -6.8761],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [11/50], Loss: 0.06696206331253052\n",
      "Epoch [12/50], Batch [1/32], Loss: 0.06488896161317825\n",
      "Logits: tensor([-0.3795, -0.3858, -0.3808,  ..., -7.0165, -7.0235, -6.7278],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [2/32], Loss: 0.06623075902462006\n",
      "Logits: tensor([-0.3769, -0.3803, -0.3824,  ..., -7.0475, -6.9436, -6.9231],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [3/32], Loss: 0.06500975787639618\n",
      "Logits: tensor([-0.3749, -0.3790, -0.3806,  ..., -6.8145, -6.8587, -6.9813],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [4/32], Loss: 0.0648757666349411\n",
      "Logits: tensor([-0.3801, -0.3717, -0.3847,  ..., -6.9402, -6.9699, -6.9305],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [5/32], Loss: 0.06483478844165802\n",
      "Logits: tensor([-0.3723, -0.3783, -0.3797,  ..., -6.7828, -6.8250, -6.8091],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [6/32], Loss: 0.06607502698898315\n",
      "Logits: tensor([-0.3700, -0.3786, -0.3819,  ..., -6.6855, -7.0113, -6.7872],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [7/32], Loss: 0.06583612412214279\n",
      "Logits: tensor([-0.3751, -0.3786, -0.3821,  ..., -6.6709, -6.5887, -6.9209],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [8/32], Loss: 0.06525520980358124\n",
      "Logits: tensor([-0.3830, -0.3841, -0.3781,  ..., -6.8351, -6.7329, -6.9203],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [9/32], Loss: 0.06578926742076874\n",
      "Logits: tensor([-0.3779, -0.3793, -0.3765,  ..., -6.9873, -6.8082, -6.7612],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [10/32], Loss: 0.0655193105340004\n",
      "Logits: tensor([-0.3759, -0.3803, -0.3800,  ..., -6.7088, -6.8675, -7.0333],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [11/32], Loss: 0.06607877463102341\n",
      "Logits: tensor([-0.3740, -0.3828, -0.3803,  ..., -7.2360, -6.9636, -7.1526],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [12/32], Loss: 0.0659773126244545\n",
      "Logits: tensor([-0.3788, -0.3828, -0.3781,  ..., -6.7367, -6.9365, -6.8894],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [13/32], Loss: 0.065053790807724\n",
      "Logits: tensor([-0.3764, -0.3772, -0.3782,  ..., -6.7292, -6.6936, -6.8380],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [14/32], Loss: 0.0657949149608612\n",
      "Logits: tensor([-0.3800, -0.3812, -0.3786,  ..., -6.8160, -6.7123, -6.8172],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [15/32], Loss: 0.06544814258813858\n",
      "Logits: tensor([-0.3810, -0.3796, -0.3773,  ..., -6.6486, -7.0088, -6.8880],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [16/32], Loss: 0.06563542783260345\n",
      "Logits: tensor([-0.3710, -0.3780, -0.3818,  ..., -6.8256, -6.8765, -6.6698],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [17/32], Loss: 0.06501327455043793\n",
      "Logits: tensor([-0.3814, -0.3794, -0.3746,  ..., -6.8437, -7.0183, -6.7899],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [18/32], Loss: 0.06559902429580688\n",
      "Logits: tensor([-0.3710, -0.3768, -0.3784,  ..., -6.5788, -7.1042, -6.9221],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [19/32], Loss: 0.06618916988372803\n",
      "Logits: tensor([-0.3745, -0.3792, -0.3789,  ..., -7.0224, -6.8599, -7.0529],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [20/32], Loss: 0.06632295995950699\n",
      "Logits: tensor([-0.3720, -0.3760, -0.3769,  ..., -7.2240, -7.0698, -6.9007],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [21/32], Loss: 0.06461765617132187\n",
      "Logits: tensor([-0.3827, -0.3772, -0.3787,  ..., -7.1630, -6.7814, -6.9213],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [22/32], Loss: 0.0661875456571579\n",
      "Logits: tensor([-0.3824, -0.3809, -0.3793,  ..., -7.0416, -7.1912, -6.5933],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [23/32], Loss: 0.06495283544063568\n",
      "Logits: tensor([-0.3841, -0.3778, -0.3776,  ..., -7.0252, -6.8109, -6.9071],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [24/32], Loss: 0.06475628912448883\n",
      "Logits: tensor([-0.3764, -0.3802, -0.3747,  ..., -6.9801, -6.9387, -6.9445],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [25/32], Loss: 0.06588271260261536\n",
      "Logits: tensor([-0.3845, -0.3816, -0.3751,  ..., -7.1027, -7.1020, -7.0857],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [26/32], Loss: 0.06515593826770782\n",
      "Logits: tensor([-0.3777, -0.3846, -0.3820,  ..., -6.7695, -6.9790, -6.9757],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [27/32], Loss: 0.06566615402698517\n",
      "Logits: tensor([-0.3734, -0.3824, -0.3800,  ..., -7.0277, -7.2622, -6.7682],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [28/32], Loss: 0.06603929400444031\n",
      "Logits: tensor([-0.3800, -0.3794, -0.3805,  ..., -7.0907, -7.1043, -6.7416],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [29/32], Loss: 0.0650462731719017\n",
      "Logits: tensor([-0.3837, -0.3790, -0.3740,  ..., -7.0439, -7.1470, -6.9333],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [30/32], Loss: 0.06569647043943405\n",
      "Logits: tensor([-0.3816, -0.3767, -0.3780,  ..., -7.1640, -7.1749, -7.0084],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [31/32], Loss: 0.06498308479785919\n",
      "Logits: tensor([-0.3784, -0.3791, -0.3775,  ..., -7.0478, -6.9499, -7.1180],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Batch [32/32], Loss: 0.06611016392707825\n",
      "Logits: tensor([-0.3853, -0.3835, -0.3832,  ..., -6.9165, -6.7530, -6.9321],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [12/50], Loss: 0.06611016392707825\n",
      "Epoch [13/50], Batch [1/32], Loss: 0.0658179447054863\n",
      "Logits: tensor([-0.3790, -0.3781, -0.3778,  ..., -6.8720, -7.0750, -6.8944],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [2/32], Loss: 0.06530191749334335\n",
      "Logits: tensor([-0.3826, -0.3810, -0.3799,  ..., -7.0528, -7.1843, -6.9174],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [3/32], Loss: 0.06581547856330872\n",
      "Logits: tensor([-0.3783, -0.3846, -0.3847,  ..., -7.0943, -7.2626, -7.0293],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [4/32], Loss: 0.06538087129592896\n",
      "Logits: tensor([-0.3797, -0.3845, -0.3787,  ..., -6.8843, -6.9169, -6.8916],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [5/32], Loss: 0.0652485117316246\n",
      "Logits: tensor([-0.3830, -0.3826, -0.3827,  ..., -6.6397, -6.8593, -7.0034],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [6/32], Loss: 0.06544376164674759\n",
      "Logits: tensor([-0.3733, -0.3797, -0.3776,  ..., -6.9905, -7.1443, -6.6775],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [7/32], Loss: 0.0647936537861824\n",
      "Logits: tensor([-0.3808, -0.3727, -0.3801,  ..., -7.1662, -7.1824, -7.1483],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [8/32], Loss: 0.06599745899438858\n",
      "Logits: tensor([-0.3834, -0.3801, -0.3850,  ..., -7.1299, -7.3650, -6.6888],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [9/32], Loss: 0.06571457535028458\n",
      "Logits: tensor([-0.3748, -0.3792, -0.3748,  ..., -6.9763, -6.8843, -6.8775],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [10/32], Loss: 0.06511175632476807\n",
      "Logits: tensor([-0.3816, -0.3750, -0.3796,  ..., -7.1907, -7.0304, -6.9600],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [11/32], Loss: 0.06621254980564117\n",
      "Logits: tensor([-0.3800, -0.3800, -0.3792,  ..., -6.9628, -6.9140, -6.9131],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [12/32], Loss: 0.06501798331737518\n",
      "Logits: tensor([-0.3845, -0.3798, -0.3748,  ..., -6.8148, -6.9762, -7.1446],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [13/32], Loss: 0.06493062525987625\n",
      "Logits: tensor([-0.3832, -0.3772, -0.3741,  ..., -6.9478, -6.8808, -7.0941],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [14/32], Loss: 0.06539894640445709\n",
      "Logits: tensor([-0.3819, -0.3781, -0.3845,  ..., -7.1018, -7.0374, -6.8601],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [15/32], Loss: 0.06592195481061935\n",
      "Logits: tensor([-0.3823, -0.3796, -0.3795,  ..., -6.8691, -7.1268, -7.2892],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [16/32], Loss: 0.06578199565410614\n",
      "Logits: tensor([-0.3835, -0.3778, -0.3815,  ..., -6.8890, -7.1566, -7.0209],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [17/32], Loss: 0.0653480738401413\n",
      "Logits: tensor([-0.3805, -0.3765, -0.3762,  ..., -7.2155, -7.2048, -7.0101],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [18/32], Loss: 0.06535183638334274\n",
      "Logits: tensor([-0.3842, -0.3732, -0.3757,  ..., -7.2087, -7.2172, -7.1972],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [19/32], Loss: 0.06564129889011383\n",
      "Logits: tensor([-0.3808, -0.3815, -0.3791,  ..., -7.2497, -7.1200, -7.0826],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [20/32], Loss: 0.0653105080127716\n",
      "Logits: tensor([-0.3840, -0.3814, -0.3833,  ..., -6.8701, -7.0302, -6.9849],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [21/32], Loss: 0.06527683138847351\n",
      "Logits: tensor([-0.3826, -0.3796, -0.3764,  ..., -6.9026, -7.0466, -7.0006],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [22/32], Loss: 0.06495462357997894\n",
      "Logits: tensor([-0.3855, -0.3807, -0.3860,  ..., -6.9953, -6.7708, -7.0867],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [23/32], Loss: 0.06579642742872238\n",
      "Logits: tensor([-0.3813, -0.3727, -0.3826,  ..., -7.1936, -7.0907, -7.0752],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [24/32], Loss: 0.06641934812068939\n",
      "Logits: tensor([-0.3868, -0.3796, -0.3835,  ..., -6.9376, -7.1473, -7.0146],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [25/32], Loss: 0.06575214862823486\n",
      "Logits: tensor([-0.3761, -0.3831, -0.3781,  ..., -7.0686, -7.1273, -7.0761],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [26/32], Loss: 0.06568051874637604\n",
      "Logits: tensor([-0.3825, -0.3810, -0.3820,  ..., -7.0758, -6.9453, -6.5956],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [27/32], Loss: 0.06470048427581787\n",
      "Logits: tensor([-0.3789, -0.3796, -0.3806,  ..., -6.6294, -6.9062, -6.8225],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [28/32], Loss: 0.06410365551710129\n",
      "Logits: tensor([-0.3810, -0.3707, -0.3838,  ..., -7.1734, -6.9738, -7.1497],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [29/32], Loss: 0.06610389798879623\n",
      "Logits: tensor([-0.3840, -0.3839, -0.3791,  ..., -6.9754, -6.9964, -7.0942],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [30/32], Loss: 0.06535142660140991\n",
      "Logits: tensor([-0.3827, -0.3809, -0.3803,  ..., -7.1999, -6.6156, -6.9980],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [31/32], Loss: 0.06457825750112534\n",
      "Logits: tensor([-0.3792, -0.3774, -0.3802,  ..., -6.9757, -6.8885, -7.0929],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Batch [32/32], Loss: 0.06588253378868103\n",
      "Logits: tensor([-0.3840, -0.3795, -0.3783,  ..., -7.1535, -7.0500, -7.0282],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [13/50], Loss: 0.06588253378868103\n",
      "Epoch [14/50], Batch [1/32], Loss: 0.06543409824371338\n",
      "Logits: tensor([-0.3785, -0.3814, -0.3842,  ..., -6.9888, -7.0488, -7.1304],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [2/32], Loss: 0.0656496062874794\n",
      "Logits: tensor([-0.3796, -0.3721, -0.3819,  ..., -6.9774, -7.3935, -6.8279],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [3/32], Loss: 0.0655112937092781\n",
      "Logits: tensor([-0.3809, -0.3784, -0.3771,  ..., -6.9545, -7.0389, -6.9862],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [4/32], Loss: 0.06542696803808212\n",
      "Logits: tensor([-0.3801, -0.3826, -0.3779,  ..., -6.9165, -7.1097, -7.0035],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [5/32], Loss: 0.06505787372589111\n",
      "Logits: tensor([-0.3805, -0.3773, -0.3825,  ..., -7.0531, -7.0554, -6.9435],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [6/32], Loss: 0.06602407246828079\n",
      "Logits: tensor([-0.3809, -0.3788, -0.3829,  ..., -7.1341, -6.9906, -7.1525],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [7/32], Loss: 0.06532759219408035\n",
      "Logits: tensor([-0.3801, -0.3807, -0.3745,  ..., -6.9520, -6.8947, -7.0541],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [8/32], Loss: 0.06483234465122223\n",
      "Logits: tensor([-0.3772, -0.3837, -0.3807,  ..., -6.9807, -6.9364, -6.9063],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [9/32], Loss: 0.06601017713546753\n",
      "Logits: tensor([-0.3802, -0.3853, -0.3812,  ..., -7.2826, -7.0815, -6.8261],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [10/32], Loss: 0.06513962894678116\n",
      "Logits: tensor([-0.3853, -0.3780, -0.3801,  ..., -7.0277, -7.0247, -6.7966],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [11/32], Loss: 0.06495682150125504\n",
      "Logits: tensor([-0.3874, -0.3773, -0.3867,  ..., -7.1224, -6.8013, -7.1615],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [12/32], Loss: 0.06554436683654785\n",
      "Logits: tensor([-0.3735, -0.3808, -0.3747,  ..., -6.9339, -7.0956, -7.0499],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [13/32], Loss: 0.06501756608486176\n",
      "Logits: tensor([-0.3859, -0.3759, -0.3856,  ..., -7.3479, -6.9939, -7.3651],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [14/32], Loss: 0.06583520025014877\n",
      "Logits: tensor([-0.3831, -0.3813, -0.3808,  ..., -7.2175, -7.3889, -7.1511],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [15/32], Loss: 0.06556382030248642\n",
      "Logits: tensor([-0.3823, -0.3854, -0.3858,  ..., -7.1993, -7.3617, -7.3160],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [16/32], Loss: 0.06549099832773209\n",
      "Logits: tensor([-0.3806, -0.3826, -0.3840,  ..., -7.3243, -7.2158, -7.1927],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [17/32], Loss: 0.06596799194812775\n",
      "Logits: tensor([-0.3806, -0.3812, -0.3916,  ..., -7.0903, -7.1174, -7.2548],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [18/32], Loss: 0.06574321538209915\n",
      "Logits: tensor([-0.3826, -0.3774, -0.3812,  ..., -7.2601, -7.0964, -6.9988],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [19/32], Loss: 0.06511067599058151\n",
      "Logits: tensor([-0.3757, -0.3842, -0.3790,  ..., -7.2037, -6.9971, -7.1141],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [20/32], Loss: 0.06541289389133453\n",
      "Logits: tensor([-0.3807, -0.3808, -0.3856,  ..., -7.2113, -7.2909, -7.0828],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [21/32], Loss: 0.06520390510559082\n",
      "Logits: tensor([-0.3815, -0.3789, -0.3869,  ..., -7.3361, -7.2473, -7.2155],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [22/32], Loss: 0.06469663977622986\n",
      "Logits: tensor([-0.3828, -0.3802, -0.3789,  ..., -7.2169, -7.2343, -7.1296],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [23/32], Loss: 0.06528448313474655\n",
      "Logits: tensor([-0.3789, -0.3780, -0.3850,  ..., -7.2135, -7.1479, -6.9678],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [24/32], Loss: 0.06574053317308426\n",
      "Logits: tensor([-0.3833, -0.3827, -0.3794,  ..., -7.2944, -7.0767, -7.2736],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [25/32], Loss: 0.06470153480768204\n",
      "Logits: tensor([-0.3770, -0.3823, -0.3825,  ..., -7.0014, -7.1113, -7.0214],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [26/32], Loss: 0.06512918323278427\n",
      "Logits: tensor([-0.3835, -0.3841, -0.3818,  ..., -7.1383, -7.1634, -7.0783],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [27/32], Loss: 0.06562420725822449\n",
      "Logits: tensor([-0.3853, -0.3795, -0.3813,  ..., -7.0846, -7.3482, -7.0814],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [28/32], Loss: 0.0659092590212822\n",
      "Logits: tensor([-0.3839, -0.3840, -0.3769,  ..., -6.9977, -7.0328, -6.9179],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [29/32], Loss: 0.06481761485338211\n",
      "Logits: tensor([-0.3741, -0.3844, -0.3793,  ..., -6.9636, -7.4234, -7.0535],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [30/32], Loss: 0.06449476629495621\n",
      "Logits: tensor([-0.3882, -0.3858, -0.3739,  ..., -7.1170, -7.3560, -6.8959],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [31/32], Loss: 0.06609580665826797\n",
      "Logits: tensor([-0.3786, -0.3812, -0.3792,  ..., -7.1632, -7.4361, -7.3078],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Batch [32/32], Loss: 0.06442258507013321\n",
      "Logits: tensor([-0.3852, -0.3822, -0.3763,  ..., -7.2096, -7.2411, -7.2989],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [14/50], Loss: 0.06442258507013321\n",
      "Epoch [15/50], Batch [1/32], Loss: 0.06559044867753983\n",
      "Logits: tensor([-0.3836, -0.3806, -0.3774,  ..., -7.2044, -7.2359, -7.4333],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [2/32], Loss: 0.06485452502965927\n",
      "Logits: tensor([-0.3869, -0.3786, -0.3871,  ..., -6.9987, -7.4829, -7.2562],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [3/32], Loss: 0.0650874674320221\n",
      "Logits: tensor([-0.3770, -0.3785, -0.3833,  ..., -7.3207, -7.0584, -7.2469],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [4/32], Loss: 0.06513289362192154\n",
      "Logits: tensor([-0.3775, -0.3835, -0.3834,  ..., -7.2555, -7.1328, -7.1779],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [5/32], Loss: 0.06559035181999207\n",
      "Logits: tensor([-0.3862, -0.3777, -0.3857,  ..., -7.2021, -6.9890, -7.2790],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [6/32], Loss: 0.06579691171646118\n",
      "Logits: tensor([-0.3808, -0.3834, -0.3777,  ..., -6.9428, -7.0898, -7.3662],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [7/32], Loss: 0.06556485593318939\n",
      "Logits: tensor([-0.3815, -0.3815, -0.3876,  ..., -7.0366, -7.3591, -7.0907],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [8/32], Loss: 0.06541086733341217\n",
      "Logits: tensor([-0.3819, -0.3789, -0.3836,  ..., -6.7952, -7.1131, -7.5078],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [9/32], Loss: 0.06568948179483414\n",
      "Logits: tensor([-0.3849, -0.3879, -0.3843,  ..., -7.1251, -7.1518, -6.9477],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [10/32], Loss: 0.06496608257293701\n",
      "Logits: tensor([-0.3840, -0.3781, -0.3787,  ..., -7.1681, -7.1683, -7.2422],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [11/32], Loss: 0.06581705063581467\n",
      "Logits: tensor([-0.3803, -0.3757, -0.3784,  ..., -7.1396, -7.0645, -7.0529],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [12/32], Loss: 0.06581515818834305\n",
      "Logits: tensor([-0.3772, -0.3827, -0.3736,  ..., -7.1481, -7.3719, -7.2382],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [13/32], Loss: 0.06555978953838348\n",
      "Logits: tensor([-0.3834, -0.3840, -0.3792,  ..., -7.3050, -7.0283, -7.0302],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [14/32], Loss: 0.06470271944999695\n",
      "Logits: tensor([-0.3811, -0.3825, -0.3834,  ..., -7.2648, -7.0430, -7.1429],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [15/32], Loss: 0.06621520221233368\n",
      "Logits: tensor([-0.3859, -0.3813, -0.3761,  ..., -7.1375, -7.0879, -7.0868],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [16/32], Loss: 0.06504633277654648\n",
      "Logits: tensor([-0.3809, -0.3826, -0.3801,  ..., -7.2150, -7.1309, -7.4876],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [17/32], Loss: 0.06552766263484955\n",
      "Logits: tensor([-0.3821, -0.3796, -0.3812,  ..., -7.0063, -6.8887, -7.2006],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [18/32], Loss: 0.06528710573911667\n",
      "Logits: tensor([-0.3822, -0.3877, -0.3828,  ..., -7.3443, -7.2392, -7.2233],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [19/32], Loss: 0.06536615639925003\n",
      "Logits: tensor([-0.3785, -0.3816, -0.3846,  ..., -7.1826, -7.3185, -7.3233],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [20/32], Loss: 0.06567453593015671\n",
      "Logits: tensor([-0.3846, -0.3831, -0.3828,  ..., -7.2564, -6.9002, -7.3563],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [21/32], Loss: 0.0650576800107956\n",
      "Logits: tensor([-0.3817, -0.3821, -0.3838,  ..., -6.5841, -7.0445, -7.1812],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [22/32], Loss: 0.06460235267877579\n",
      "Logits: tensor([-0.3802, -0.3742, -0.3841,  ..., -7.3530, -7.3099, -7.2464],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [23/32], Loss: 0.06533169746398926\n",
      "Logits: tensor([-0.3816, -0.3789, -0.3870,  ..., -7.3833, -7.4791, -7.5782],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [24/32], Loss: 0.0649048238992691\n",
      "Logits: tensor([-0.3825, -0.3828, -0.3790,  ..., -7.3496, -7.1650, -7.2130],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [25/32], Loss: 0.06538811326026917\n",
      "Logits: tensor([-0.3822, -0.3755, -0.3724,  ..., -7.3203, -7.3501, -7.6001],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [26/32], Loss: 0.06542417407035828\n",
      "Logits: tensor([-0.3822, -0.3825, -0.3745,  ..., -7.1193, -7.3888, -7.3524],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [27/32], Loss: 0.06449250876903534\n",
      "Logits: tensor([-0.3870, -0.3802, -0.3812,  ..., -7.2693, -7.2362, -7.0481],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [28/32], Loss: 0.06507586687803268\n",
      "Logits: tensor([-0.3867, -0.3806, -0.3817,  ..., -7.1262, -7.3926, -6.9987],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [29/32], Loss: 0.06554780900478363\n",
      "Logits: tensor([-0.3771, -0.3832, -0.3863,  ..., -7.0969, -7.1936, -7.1976],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [30/32], Loss: 0.06528540700674057\n",
      "Logits: tensor([-0.3808, -0.3798, -0.3845,  ..., -6.9443, -7.3392, -7.5355],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [31/32], Loss: 0.06515860557556152\n",
      "Logits: tensor([-0.3819, -0.3826, -0.3806,  ..., -7.4133, -7.3880, -7.2038],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Batch [32/32], Loss: 0.06498197466135025\n",
      "Logits: tensor([-0.3821, -0.3739, -0.3774,  ..., -7.1755, -7.1436, -7.2859],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [15/50], Loss: 0.06498197466135025\n",
      "Epoch [16/50], Batch [1/32], Loss: 0.06591590493917465\n",
      "Logits: tensor([-0.3808, -0.3800, -0.3789,  ..., -7.4651, -7.5095, -7.2880],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [2/32], Loss: 0.06532279402017593\n",
      "Logits: tensor([-0.3794, -0.3876, -0.3779,  ..., -7.3069, -7.3561, -7.4664],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [3/32], Loss: 0.06547734141349792\n",
      "Logits: tensor([-0.3798, -0.3827, -0.3840,  ..., -7.1427, -7.1585, -7.3344],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [4/32], Loss: 0.06527627259492874\n",
      "Logits: tensor([-0.3868, -0.3811, -0.3776,  ..., -7.2509, -7.2242, -7.5273],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [5/32], Loss: 0.06490732729434967\n",
      "Logits: tensor([-0.3782, -0.3784, -0.3878,  ..., -7.4889, -7.1359, -7.2194],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [6/32], Loss: 0.06566547602415085\n",
      "Logits: tensor([-0.3739, -0.3842, -0.3792,  ..., -7.2644, -7.5428, -7.2329],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [7/32], Loss: 0.06544726341962814\n",
      "Logits: tensor([-0.3814, -0.3787, -0.3869,  ..., -7.4041, -7.2798, -7.1309],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [8/32], Loss: 0.0652310773730278\n",
      "Logits: tensor([-0.3800, -0.3793, -0.3798,  ..., -7.2611, -7.2186, -7.2784],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [9/32], Loss: 0.06572410464286804\n",
      "Logits: tensor([-0.3811, -0.3809, -0.3781,  ..., -7.4164, -7.2767, -7.2999],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [10/32], Loss: 0.06494280695915222\n",
      "Logits: tensor([-0.3788, -0.3814, -0.3855,  ..., -7.4945, -7.1927, -7.3164],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [11/32], Loss: 0.06557705998420715\n",
      "Logits: tensor([-0.3814, -0.3871, -0.3733,  ..., -7.4470, -7.0953, -7.5863],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [12/32], Loss: 0.06542698293924332\n",
      "Logits: tensor([-0.3827, -0.3816, -0.3790,  ..., -7.3310, -7.4596, -7.5131],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [13/32], Loss: 0.06509523093700409\n",
      "Logits: tensor([-0.3750, -0.3809, -0.3881,  ..., -7.3600, -7.2463, -7.1294],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [14/32], Loss: 0.06501644849777222\n",
      "Logits: tensor([-0.3788, -0.3792, -0.3827,  ..., -7.3679, -7.4626, -7.2454],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [15/32], Loss: 0.06491303443908691\n",
      "Logits: tensor([-0.3822, -0.3852, -0.3758,  ..., -7.1988, -7.2588, -7.1606],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [16/32], Loss: 0.06550358980894089\n",
      "Logits: tensor([-0.3766, -0.3876, -0.3804,  ..., -7.1260, -7.1533, -7.0452],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [17/32], Loss: 0.06538566946983337\n",
      "Logits: tensor([-0.3755, -0.3785, -0.3868,  ..., -7.0625, -7.2213, -7.2720],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [18/32], Loss: 0.06471212953329086\n",
      "Logits: tensor([-0.3806, -0.3841, -0.3862,  ..., -7.3824, -7.1848, -7.3199],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [19/32], Loss: 0.065146304666996\n",
      "Logits: tensor([-0.3810, -0.3852, -0.3817,  ..., -7.6244, -7.2878, -7.5787],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [20/32], Loss: 0.06553290784358978\n",
      "Logits: tensor([-0.3786, -0.3830, -0.3822,  ..., -7.5196, -7.0773, -7.1880],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [21/32], Loss: 0.06492850929498672\n",
      "Logits: tensor([-0.3814, -0.3787, -0.3838,  ..., -7.3677, -6.8711, -7.0605],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [22/32], Loss: 0.06524960696697235\n",
      "Logits: tensor([-0.3773, -0.3796, -0.3832,  ..., -6.9446, -7.3127, -7.1551],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [23/32], Loss: 0.06605211645364761\n",
      "Logits: tensor([-0.3794, -0.3757, -0.3855,  ..., -7.2773, -7.3182, -7.2038],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [24/32], Loss: 0.0650312528014183\n",
      "Logits: tensor([-0.3804, -0.3833, -0.3755,  ..., -7.2799, -7.2615, -7.4008],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [25/32], Loss: 0.06457652896642685\n",
      "Logits: tensor([-0.3847, -0.3735, -0.3760,  ..., -7.4041, -7.2368, -7.4228],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [26/32], Loss: 0.06550117582082748\n",
      "Logits: tensor([-0.3862, -0.3798, -0.3846,  ..., -7.5518, -7.4110, -7.4035],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [27/32], Loss: 0.06468215584754944\n",
      "Logits: tensor([-0.3764, -0.3831, -0.3771,  ..., -7.2524, -7.2288, -7.2771],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [28/32], Loss: 0.06535447388887405\n",
      "Logits: tensor([-0.3791, -0.3825, -0.3821,  ..., -7.4481, -7.5037, -7.1719],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [29/32], Loss: 0.06522713601589203\n",
      "Logits: tensor([-0.3743, -0.3878, -0.3846,  ..., -7.2731, -7.3667, -6.9789],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [30/32], Loss: 0.06548803299665451\n",
      "Logits: tensor([-0.3790, -0.3822, -0.3799,  ..., -7.3489, -7.1275, -7.4252],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [31/32], Loss: 0.06478586047887802\n",
      "Logits: tensor([-0.3787, -0.3790, -0.3828,  ..., -7.1257, -7.5059, -7.2810],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Batch [32/32], Loss: 0.06641644239425659\n",
      "Logits: tensor([-0.3816, -0.3845, -0.3816,  ..., -7.3902, -7.0863, -7.4403],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [16/50], Loss: 0.06641644239425659\n",
      "Epoch [17/50], Batch [1/32], Loss: 0.06545257568359375\n",
      "Logits: tensor([-0.3797, -0.3764, -0.3770,  ..., -7.0917, -7.2511, -7.3022],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [2/32], Loss: 0.06542935967445374\n",
      "Logits: tensor([-0.3822, -0.3843, -0.3819,  ..., -7.4489, -7.3448, -7.1731],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [3/32], Loss: 0.0646345391869545\n",
      "Logits: tensor([-0.3831, -0.3803, -0.3788,  ..., -7.4685, -7.2133, -7.1123],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [4/32], Loss: 0.06534022092819214\n",
      "Logits: tensor([-0.3863, -0.3844, -0.3841,  ..., -7.3621, -7.4988, -7.0641],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [5/32], Loss: 0.06495267152786255\n",
      "Logits: tensor([-0.3845, -0.3866, -0.3828,  ..., -7.3442, -7.2549, -7.2186],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [6/32], Loss: 0.06517942249774933\n",
      "Logits: tensor([-0.3772, -0.3743, -0.3778,  ..., -7.3713, -7.1115, -6.8816],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [7/32], Loss: 0.06459562480449677\n",
      "Logits: tensor([-0.3837, -0.3756, -0.3784,  ..., -6.9953, -7.2223, -7.4300],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [8/32], Loss: 0.06490595638751984\n",
      "Logits: tensor([-0.3763, -0.3839, -0.3844,  ..., -7.3049, -7.2236, -7.3741],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [9/32], Loss: 0.0650297999382019\n",
      "Logits: tensor([-0.3807, -0.3780, -0.3831,  ..., -7.1098, -7.3455, -7.2103],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [10/32], Loss: 0.06520485132932663\n",
      "Logits: tensor([-0.3795, -0.3790, -0.3792,  ..., -7.4456, -7.2936, -7.3573],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [11/32], Loss: 0.06560691446065903\n",
      "Logits: tensor([-0.3852, -0.3734, -0.3840,  ..., -7.4311, -7.3274, -6.9062],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [12/32], Loss: 0.06470749527215958\n",
      "Logits: tensor([-0.3804, -0.3788, -0.3824,  ..., -7.2468, -7.3770, -7.2872],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [13/32], Loss: 0.0651923343539238\n",
      "Logits: tensor([-0.3735, -0.3800, -0.3780,  ..., -7.2805, -7.5244, -7.5443],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [14/32], Loss: 0.06538956612348557\n",
      "Logits: tensor([-0.3740, -0.3799, -0.3872,  ..., -7.2196, -7.2719, -7.2046],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [15/32], Loss: 0.06510904431343079\n",
      "Logits: tensor([-0.3726, -0.3852, -0.3814,  ..., -7.4840, -7.2934, -7.3435],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [16/32], Loss: 0.06543011218309402\n",
      "Logits: tensor([-0.3794, -0.3854, -0.3802,  ..., -7.0280, -7.3728, -7.2000],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [17/32], Loss: 0.06621531397104263\n",
      "Logits: tensor([-0.3808, -0.3812, -0.3818,  ..., -7.2628, -7.1485, -7.4370],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [18/32], Loss: 0.06533850729465485\n",
      "Logits: tensor([-0.3795, -0.3762, -0.3722,  ..., -7.7179, -7.2651, -7.3493],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [19/32], Loss: 0.06505776196718216\n",
      "Logits: tensor([-0.3785, -0.3767, -0.3817,  ..., -7.3357, -7.4842, -7.2601],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [20/32], Loss: 0.0651395171880722\n",
      "Logits: tensor([-0.3787, -0.3834, -0.3820,  ..., -7.5106, -7.3423, -7.2689],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [21/32], Loss: 0.06515717506408691\n",
      "Logits: tensor([-0.3824, -0.3808, -0.3818,  ..., -7.5183, -7.2490, -7.2402],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [22/32], Loss: 0.06495315581560135\n",
      "Logits: tensor([-0.3823, -0.3840, -0.3821,  ..., -7.2815, -7.3267, -7.0132],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [23/32], Loss: 0.06592127680778503\n",
      "Logits: tensor([-0.3808, -0.3777, -0.3776,  ..., -7.4855, -7.3658, -7.2618],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [24/32], Loss: 0.06485649943351746\n",
      "Logits: tensor([-0.3800, -0.3836, -0.3847,  ..., -7.4863, -7.3830, -7.2331],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [25/32], Loss: 0.06522876769304276\n",
      "Logits: tensor([-0.3807, -0.3837, -0.3743,  ..., -7.1867, -7.3297, -7.2085],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [26/32], Loss: 0.06531059741973877\n",
      "Logits: tensor([-0.3824, -0.3801, -0.3814,  ..., -7.3355, -7.3048, -7.1877],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [27/32], Loss: 0.06580014526844025\n",
      "Logits: tensor([-0.3822, -0.3816, -0.3778,  ..., -7.5090, -7.3277, -6.9708],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [28/32], Loss: 0.06522870808839798\n",
      "Logits: tensor([-0.3830, -0.3773, -0.3782,  ..., -7.4159, -7.3786, -7.1714],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [29/32], Loss: 0.06497137248516083\n",
      "Logits: tensor([-0.3750, -0.3804, -0.3713,  ..., -7.2184, -7.3398, -7.3876],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [30/32], Loss: 0.0654778704047203\n",
      "Logits: tensor([-0.3797, -0.3794, -0.3823,  ..., -7.4388, -7.2734, -7.1562],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [31/32], Loss: 0.06515078991651535\n",
      "Logits: tensor([-0.3781, -0.3744, -0.3828,  ..., -6.8551, -7.5317, -7.0734],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Batch [32/32], Loss: 0.06524040549993515\n",
      "Logits: tensor([-0.3745, -0.3840, -0.3792,  ..., -7.3386, -7.4695, -7.5202],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [17/50], Loss: 0.06524040549993515\n",
      "Epoch [18/50], Batch [1/32], Loss: 0.0653838962316513\n",
      "Logits: tensor([-0.3768, -0.3803, -0.3760,  ..., -6.9101, -7.7573, -7.4584],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [2/32], Loss: 0.0650831013917923\n",
      "Logits: tensor([-0.3732, -0.3822, -0.3799,  ..., -7.4255, -7.0454, -7.1781],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [3/32], Loss: 0.06538185477256775\n",
      "Logits: tensor([-0.3812, -0.3780, -0.3824,  ..., -7.5167, -7.4558, -7.3615],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [4/32], Loss: 0.06524152308702469\n",
      "Logits: tensor([-0.3712, -0.3785, -0.3724,  ..., -7.3321, -7.4612, -7.5448],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [5/32], Loss: 0.06561128050088882\n",
      "Logits: tensor([-0.3737, -0.3828, -0.3813,  ..., -7.1079, -7.0198, -7.3743],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [6/32], Loss: 0.06501271575689316\n",
      "Logits: tensor([-0.3775, -0.3734, -0.3813,  ..., -7.4830, -7.6128, -7.4196],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [7/32], Loss: 0.06478025764226913\n",
      "Logits: tensor([-0.3799, -0.3764, -0.3809,  ..., -7.4592, -7.5490, -7.1614],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [8/32], Loss: 0.06410856544971466\n",
      "Logits: tensor([-0.3791, -0.3788, -0.3817,  ..., -7.2147, -7.5279, -7.3039],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [9/32], Loss: 0.06502687186002731\n",
      "Logits: tensor([-0.3843, -0.3806, -0.3753,  ..., -7.6598, -7.1336, -7.3939],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [10/32], Loss: 0.06571595370769501\n",
      "Logits: tensor([-0.3753, -0.3738, -0.3781,  ..., -7.2720, -7.1889, -7.3740],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [11/32], Loss: 0.06562241911888123\n",
      "Logits: tensor([-0.3803, -0.3830, -0.3810,  ..., -7.4971, -7.4359, -7.2034],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [12/32], Loss: 0.06556404381990433\n",
      "Logits: tensor([-0.3785, -0.3798, -0.3866,  ..., -7.4627, -7.2566, -7.5238],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [13/32], Loss: 0.06567446887493134\n",
      "Logits: tensor([-0.3822, -0.3765, -0.3776,  ..., -7.4831, -7.2827, -7.4197],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [14/32], Loss: 0.06429222971200943\n",
      "Logits: tensor([-0.3813, -0.3773, -0.3754,  ..., -7.5726, -7.4372, -7.2777],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [15/32], Loss: 0.06495288759469986\n",
      "Logits: tensor([-0.3760, -0.3803, -0.3796,  ..., -7.4321, -7.1974, -7.4743],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [16/32], Loss: 0.06544370204210281\n",
      "Logits: tensor([-0.3804, -0.3753, -0.3731,  ..., -7.2713, -7.4222, -7.5824],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [17/32], Loss: 0.06480170786380768\n",
      "Logits: tensor([-0.3823, -0.3750, -0.3796,  ..., -7.1593, -7.4355, -7.2918],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [18/32], Loss: 0.0654740035533905\n",
      "Logits: tensor([-0.3770, -0.3777, -0.3761,  ..., -7.2862, -7.1764, -7.2157],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [19/32], Loss: 0.06521905958652496\n",
      "Logits: tensor([-0.3787, -0.3736, -0.3797,  ..., -7.3100, -7.3748, -7.4519],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [20/32], Loss: 0.0653395801782608\n",
      "Logits: tensor([-0.3768, -0.3759, -0.3805,  ..., -7.1814, -7.3066, -7.3204],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [21/32], Loss: 0.06473153829574585\n",
      "Logits: tensor([-0.3805, -0.3753, -0.3878,  ..., -7.2602, -7.5599, -7.3265],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [22/32], Loss: 0.06501446664333344\n",
      "Logits: tensor([-0.3776, -0.3823, -0.3795,  ..., -7.5254, -7.4815, -7.4165],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [23/32], Loss: 0.06509828567504883\n",
      "Logits: tensor([-0.3820, -0.3858, -0.3791,  ..., -7.5904, -7.2852, -7.3686],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [24/32], Loss: 0.0653378814458847\n",
      "Logits: tensor([-0.3756, -0.3773, -0.3801,  ..., -7.2421, -7.4498, -7.5582],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [25/32], Loss: 0.06473349779844284\n",
      "Logits: tensor([-0.3746, -0.3801, -0.3806,  ..., -7.6285, -7.5334, -7.4044],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [26/32], Loss: 0.06508950889110565\n",
      "Logits: tensor([-0.3777, -0.3842, -0.3814,  ..., -7.2463, -7.3341, -7.2793],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [27/32], Loss: 0.06537101417779922\n",
      "Logits: tensor([-0.3801, -0.3787, -0.3777,  ..., -7.3485, -7.2954, -7.4205],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [28/32], Loss: 0.06545868515968323\n",
      "Logits: tensor([-0.3769, -0.3760, -0.3757,  ..., -7.1809, -7.3315, -7.3352],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [29/32], Loss: 0.06523679196834564\n",
      "Logits: tensor([-0.3797, -0.3777, -0.3788,  ..., -6.8956, -7.5767, -7.1154],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [30/32], Loss: 0.06589681655168533\n",
      "Logits: tensor([-0.3788, -0.3762, -0.3749,  ..., -7.5115, -7.3104, -7.4528],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [31/32], Loss: 0.06523451209068298\n",
      "Logits: tensor([-0.3801, -0.3788, -0.3802,  ..., -7.4173, -7.3063, -6.9356],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Batch [32/32], Loss: 0.06390465795993805\n",
      "Logits: tensor([-0.3739, -0.3772, -0.3760,  ..., -7.3117, -7.2412, -7.4660],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [18/50], Loss: 0.06390465795993805\n",
      "Epoch [19/50], Batch [1/32], Loss: 0.06525599211454391\n",
      "Logits: tensor([-0.3821, -0.3791, -0.3751,  ..., -7.4565, -7.5399, -7.4798],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [2/32], Loss: 0.06527330726385117\n",
      "Logits: tensor([-0.3761, -0.3761, -0.3763,  ..., -7.1515, -7.4667, -7.6629],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [3/32], Loss: 0.0651828870177269\n",
      "Logits: tensor([-0.3769, -0.3767, -0.3783,  ..., -7.3495, -7.3685, -7.3853],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [4/32], Loss: 0.06613148748874664\n",
      "Logits: tensor([-0.3784, -0.3822, -0.3770,  ..., -7.2999, -7.1780, -7.2634],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [5/32], Loss: 0.06534039229154587\n",
      "Logits: tensor([-0.3792, -0.3802, -0.3735,  ..., -7.0227, -7.6704, -7.3761],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [6/32], Loss: 0.06469488143920898\n",
      "Logits: tensor([-0.3806, -0.3750, -0.3767,  ..., -7.6342, -7.5309, -7.0340],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [7/32], Loss: 0.06563360244035721\n",
      "Logits: tensor([-0.3698, -0.3800, -0.3803,  ..., -7.2650, -7.4060, -7.3007],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [8/32], Loss: 0.0645027756690979\n",
      "Logits: tensor([-0.3798, -0.3752, -0.3733,  ..., -7.3592, -7.3318, -7.2439],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [9/32], Loss: 0.06528156995773315\n",
      "Logits: tensor([-0.3784, -0.3790, -0.3742,  ..., -7.1330, -7.2622, -7.3062],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [10/32], Loss: 0.06546676158905029\n",
      "Logits: tensor([-0.3720, -0.3767, -0.3768,  ..., -7.4835, -7.3382, -7.3268],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [11/32], Loss: 0.0645630732178688\n",
      "Logits: tensor([-0.3770, -0.3768, -0.3765,  ..., -7.4882, -7.5380, -7.4962],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [12/32], Loss: 0.06531752645969391\n",
      "Logits: tensor([-0.3754, -0.3767, -0.3738,  ..., -7.2861, -7.4137, -7.4514],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [13/32], Loss: 0.06476785987615585\n",
      "Logits: tensor([-0.3718, -0.3791, -0.3753,  ..., -7.5071, -7.3961, -7.3112],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [14/32], Loss: 0.06484723091125488\n",
      "Logits: tensor([-0.3741, -0.3816, -0.3728,  ..., -7.4948, -7.3031, -7.2515],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [15/32], Loss: 0.06497795134782791\n",
      "Logits: tensor([-0.3750, -0.3896, -0.3789,  ..., -7.5249, -7.4286, -7.5016],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [16/32], Loss: 0.06498263776302338\n",
      "Logits: tensor([-0.3713, -0.3774, -0.3786,  ..., -7.1243, -7.5323, -7.2027],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [17/32], Loss: 0.06505592912435532\n",
      "Logits: tensor([-0.3769, -0.3774, -0.3762,  ..., -7.5623, -7.4950, -7.4822],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [18/32], Loss: 0.06521224975585938\n",
      "Logits: tensor([-0.3757, -0.3737, -0.3791,  ..., -7.6060, -7.5946, -7.3884],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [19/32], Loss: 0.06550692766904831\n",
      "Logits: tensor([-0.3772, -0.3755, -0.3647,  ..., -7.4608, -7.4939, -7.6986],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [20/32], Loss: 0.06536106020212173\n",
      "Logits: tensor([-0.3767, -0.3701, -0.3786,  ..., -7.3153, -7.4006, -7.4769],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [21/32], Loss: 0.06492476165294647\n",
      "Logits: tensor([-0.3776, -0.3789, -0.3761,  ..., -7.3556, -7.3220, -7.3509],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [22/32], Loss: 0.06524790823459625\n",
      "Logits: tensor([-0.3722, -0.3772, -0.3747,  ..., -7.7139, -7.6196, -7.2935],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [23/32], Loss: 0.06542081385850906\n",
      "Logits: tensor([-0.3692, -0.3770, -0.3826,  ..., -7.2318, -7.2569, -7.4633],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [24/32], Loss: 0.06526239216327667\n",
      "Logits: tensor([-0.3783, -0.3809, -0.3735,  ..., -7.3891, -7.4337, -7.5999],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [25/32], Loss: 0.06458812952041626\n",
      "Logits: tensor([-0.3783, -0.3765, -0.3765,  ..., -7.6109, -7.5561, -7.5315],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [26/32], Loss: 0.06507471948862076\n",
      "Logits: tensor([-0.3765, -0.3738, -0.3765,  ..., -7.4789, -7.3162, -7.1664],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [27/32], Loss: 0.06504569202661514\n",
      "Logits: tensor([-0.3806, -0.3760, -0.3745,  ..., -7.2708, -7.3131, -7.3724],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [28/32], Loss: 0.0650959238409996\n",
      "Logits: tensor([-0.3746, -0.3720, -0.3754,  ..., -7.4047, -7.5894, -7.5879],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [29/32], Loss: 0.06558140367269516\n",
      "Logits: tensor([-0.3811, -0.3759, -0.3742,  ..., -7.4530, -7.4088, -7.4154],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [30/32], Loss: 0.0651092380285263\n",
      "Logits: tensor([-0.3788, -0.3731, -0.3743,  ..., -7.6984, -7.5527, -7.2763],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [31/32], Loss: 0.06477713584899902\n",
      "Logits: tensor([-0.3781, -0.3778, -0.3703,  ..., -7.5262, -7.6060, -7.4218],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Batch [32/32], Loss: 0.06436879932880402\n",
      "Logits: tensor([-0.3753, -0.3768, -0.3763,  ..., -7.3402, -7.6608, -7.5445],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [19/50], Loss: 0.06436879932880402\n",
      "Epoch [20/50], Batch [1/32], Loss: 0.06566167622804642\n",
      "Logits: tensor([-0.3683, -0.3729, -0.3723,  ..., -7.3508, -7.4149, -7.5430],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [2/32], Loss: 0.06472361832857132\n",
      "Logits: tensor([-0.3793, -0.3701, -0.3729,  ..., -7.4061, -7.6010, -7.5321],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [3/32], Loss: 0.06537429988384247\n",
      "Logits: tensor([-0.3803, -0.3755, -0.3756,  ..., -7.3480, -7.2819, -7.2781],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [4/32], Loss: 0.06486924737691879\n",
      "Logits: tensor([-0.3780, -0.3736, -0.3743,  ..., -7.1977, -7.1461, -7.3532],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [5/32], Loss: 0.06546719372272491\n",
      "Logits: tensor([-0.3757, -0.3710, -0.3748,  ..., -7.3532, -7.6036, -7.1724],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [6/32], Loss: 0.064913809299469\n",
      "Logits: tensor([-0.3762, -0.3737, -0.3731,  ..., -7.6696, -7.5373, -7.4746],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [7/32], Loss: 0.06409534066915512\n",
      "Logits: tensor([-0.3743, -0.3740, -0.3756,  ..., -7.5098, -7.4312, -7.5879],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [8/32], Loss: 0.065399169921875\n",
      "Logits: tensor([-0.3745, -0.3735, -0.3724,  ..., -7.3450, -7.5127, -7.3799],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [9/32], Loss: 0.06477171182632446\n",
      "Logits: tensor([-0.3747, -0.3734, -0.3727,  ..., -7.5678, -7.4395, -7.1268],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [10/32], Loss: 0.06463409215211868\n",
      "Logits: tensor([-0.3719, -0.3746, -0.3753,  ..., -7.1239, -7.4004, -7.1980],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [11/32], Loss: 0.06591857969760895\n",
      "Logits: tensor([-0.3710, -0.3706, -0.3745,  ..., -7.5493, -7.2780, -7.3874],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [12/32], Loss: 0.06433381885290146\n",
      "Logits: tensor([-0.3749, -0.3781, -0.3786,  ..., -7.3758, -7.3362, -7.3926],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [13/32], Loss: 0.06555306911468506\n",
      "Logits: tensor([-0.3800, -0.3773, -0.3743,  ..., -7.6638, -7.4885, -7.5426],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [14/32], Loss: 0.06553851068019867\n",
      "Logits: tensor([-0.3782, -0.3748, -0.3692,  ..., -7.2031, -7.4906, -7.3632],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [15/32], Loss: 0.06546702235937119\n",
      "Logits: tensor([-0.3697, -0.3748, -0.3771,  ..., -7.3525, -7.3298, -7.2542],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [16/32], Loss: 0.0653567835688591\n",
      "Logits: tensor([-0.3739, -0.3722, -0.3657,  ..., -7.6089, -7.4871, -7.3812],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [17/32], Loss: 0.06544455140829086\n",
      "Logits: tensor([-0.3730, -0.3689, -0.3761,  ..., -7.2587, -7.4748, -7.4251],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [18/32], Loss: 0.06476172804832458\n",
      "Logits: tensor([-0.3721, -0.3681, -0.3760,  ..., -7.3602, -7.4423, -7.4985],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [19/32], Loss: 0.06568346917629242\n",
      "Logits: tensor([-0.3747, -0.3700, -0.3712,  ..., -7.4491, -7.5162, -7.1756],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [20/32], Loss: 0.0652761459350586\n",
      "Logits: tensor([-0.3783, -0.3734, -0.3789,  ..., -7.2595, -7.7408, -7.3532],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [21/32], Loss: 0.06482671946287155\n",
      "Logits: tensor([-0.3731, -0.3705, -0.3692,  ..., -7.3719, -7.5010, -7.4371],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [22/32], Loss: 0.06515874713659286\n",
      "Logits: tensor([-0.3774, -0.3778, -0.3738,  ..., -7.4741, -7.1280, -7.4818],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [23/32], Loss: 0.06447748094797134\n",
      "Logits: tensor([-0.3750, -0.3715, -0.3772,  ..., -7.3963, -7.4582, -7.3568],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [24/32], Loss: 0.06516440957784653\n",
      "Logits: tensor([-0.3763, -0.3746, -0.3704,  ..., -7.4608, -6.8917, -7.7177],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [25/32], Loss: 0.06507386267185211\n",
      "Logits: tensor([-0.3747, -0.3723, -0.3714,  ..., -7.1539, -7.5256, -7.2428],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [26/32], Loss: 0.06502840667963028\n",
      "Logits: tensor([-0.3746, -0.3727, -0.3708,  ..., -7.8410, -7.6839, -7.6247],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [27/32], Loss: 0.06520944833755493\n",
      "Logits: tensor([-0.3720, -0.3731, -0.3782,  ..., -7.4424, -7.5833, -7.5890],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [28/32], Loss: 0.06498100608587265\n",
      "Logits: tensor([-0.3729, -0.3710, -0.3727,  ..., -7.3075, -7.4771, -7.8274],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [29/32], Loss: 0.06478974968194962\n",
      "Logits: tensor([-0.3710, -0.3773, -0.3775,  ..., -7.0833, -7.3802, -7.4162],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [30/32], Loss: 0.06474531441926956\n",
      "Logits: tensor([-0.3753, -0.3743, -0.3775,  ..., -7.2560, -7.4817, -7.4784],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [31/32], Loss: 0.06585542112588882\n",
      "Logits: tensor([-0.3727, -0.3732, -0.3708,  ..., -7.6207, -6.9934, -7.4859],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Batch [32/32], Loss: 0.06281980872154236\n",
      "Logits: tensor([-0.3740, -0.3812, -0.3752,  ..., -7.5346, -7.5137, -7.5762],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [20/50], Loss: 0.06281980872154236\n",
      "Epoch [21/50], Batch [1/32], Loss: 0.06500580161809921\n",
      "Logits: tensor([-0.3728, -0.3747, -0.3663,  ..., -7.6512, -7.7582, -7.2809],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [2/32], Loss: 0.06478654593229294\n",
      "Logits: tensor([-0.3724, -0.3736, -0.3734,  ..., -7.4812, -7.6079, -7.3247],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [3/32], Loss: 0.0648701936006546\n",
      "Logits: tensor([-0.3757, -0.3700, -0.3712,  ..., -7.3441, -7.2696, -7.1175],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [4/32], Loss: 0.06591136753559113\n",
      "Logits: tensor([-0.3714, -0.3715, -0.3764,  ..., -7.5497, -6.9921, -7.7308],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [5/32], Loss: 0.06544041633605957\n",
      "Logits: tensor([-0.3762, -0.3682, -0.3665,  ..., -7.2343, -7.1824, -7.3907],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [6/32], Loss: 0.0647425577044487\n",
      "Logits: tensor([-0.3735, -0.3720, -0.3727,  ..., -7.6263, -7.5169, -7.5003],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [7/32], Loss: 0.06537608802318573\n",
      "Logits: tensor([-0.3676, -0.3727, -0.3750,  ..., -7.8930, -7.5518, -7.4042],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [8/32], Loss: 0.06567776948213577\n",
      "Logits: tensor([-0.3734, -0.3740, -0.3692,  ..., -7.4338, -7.5634, -7.5210],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [9/32], Loss: 0.0654122531414032\n",
      "Logits: tensor([-0.3708, -0.3659, -0.3717,  ..., -7.5666, -7.5856, -7.4759],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [10/32], Loss: 0.0653892308473587\n",
      "Logits: tensor([-0.3700, -0.3623, -0.3723,  ..., -7.3960, -7.4696, -7.4715],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [11/32], Loss: 0.06480523198843002\n",
      "Logits: tensor([-0.3689, -0.3705, -0.3717,  ..., -7.5235, -6.9277, -7.2512],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [12/32], Loss: 0.0652780756354332\n",
      "Logits: tensor([-0.3816, -0.3736, -0.3682,  ..., -7.4263, -7.3934, -7.5407],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [13/32], Loss: 0.06538576632738113\n",
      "Logits: tensor([-0.3767, -0.3696, -0.3692,  ..., -7.5036, -7.4167, -7.7818],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [14/32], Loss: 0.06547237932682037\n",
      "Logits: tensor([-0.3757, -0.3676, -0.3685,  ..., -7.3954, -7.5065, -7.7209],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [15/32], Loss: 0.06490534543991089\n",
      "Logits: tensor([-0.3695, -0.3636, -0.3705,  ..., -7.6985, -7.4448, -7.3295],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [16/32], Loss: 0.06521464884281158\n",
      "Logits: tensor([-0.3737, -0.3716, -0.3667,  ..., -7.6876, -7.3231, -7.8316],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [17/32], Loss: 0.06453725695610046\n",
      "Logits: tensor([-0.3682, -0.3719, -0.3779,  ..., -7.2052, -7.3182, -7.2427],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [18/32], Loss: 0.06440915167331696\n",
      "Logits: tensor([-0.3738, -0.3694, -0.3731,  ..., -7.6634, -7.4007, -7.2973],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [19/32], Loss: 0.06514306366443634\n",
      "Logits: tensor([-0.3722, -0.3716, -0.3739,  ..., -7.5844, -7.6647, -7.4789],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [20/32], Loss: 0.06476181000471115\n",
      "Logits: tensor([-0.3698, -0.3725, -0.3747,  ..., -7.6641, -7.5187, -7.5433],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [21/32], Loss: 0.06436530500650406\n",
      "Logits: tensor([-0.3637, -0.3702, -0.3691,  ..., -7.3000, -7.4900, -7.2069],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [22/32], Loss: 0.06460370123386383\n",
      "Logits: tensor([-0.3672, -0.3652, -0.3752,  ..., -7.6861, -7.6179, -7.2391],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [23/32], Loss: 0.06517883390188217\n",
      "Logits: tensor([-0.3630, -0.3706, -0.3682,  ..., -7.4048, -7.5899, -7.7661],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [24/32], Loss: 0.06490525603294373\n",
      "Logits: tensor([-0.3700, -0.3668, -0.3670,  ..., -7.4945, -6.9223, -7.7528],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [25/32], Loss: 0.065033458173275\n",
      "Logits: tensor([-0.3704, -0.3731, -0.3771,  ..., -7.5106, -7.5473, -7.3420],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [26/32], Loss: 0.06513410061597824\n",
      "Logits: tensor([-0.3701, -0.3631, -0.3714,  ..., -7.7987, -7.6320, -7.4479],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [27/32], Loss: 0.06534510850906372\n",
      "Logits: tensor([-0.3674, -0.3743, -0.3720,  ..., -7.4496, -7.6300, -7.4001],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [28/32], Loss: 0.06487742811441422\n",
      "Logits: tensor([-0.3676, -0.3670, -0.3744,  ..., -7.7360, -7.8730, -7.4947],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [29/32], Loss: 0.06536035984754562\n",
      "Logits: tensor([-0.3688, -0.3636, -0.3691,  ..., -7.6774, -7.5966, -7.4721],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [30/32], Loss: 0.06458396464586258\n",
      "Logits: tensor([-0.3741, -0.3650, -0.3672,  ..., -7.2252, -7.2470, -7.4584],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [31/32], Loss: 0.06515749543905258\n",
      "Logits: tensor([-0.3646, -0.3738, -0.3715,  ..., -7.4412, -7.6563, -7.3850],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Batch [32/32], Loss: 0.06355379521846771\n",
      "Logits: tensor([-0.3778, -0.3667, -0.3655,  ..., -7.5266, -7.3928, -7.5091],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [21/50], Loss: 0.06355379521846771\n",
      "Epoch [22/50], Batch [1/32], Loss: 0.06487124413251877\n",
      "Logits: tensor([-0.3686, -0.3640, -0.3742,  ..., -7.5457, -6.9478, -7.2725],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [2/32], Loss: 0.06524606794118881\n",
      "Logits: tensor([-0.3716, -0.3644, -0.3665,  ..., -7.7621, -7.6022, -7.5447],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [3/32], Loss: 0.06600233167409897\n",
      "Logits: tensor([-0.3690, -0.3672, -0.3730,  ..., -7.5962, -7.4684, -7.5153],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [4/32], Loss: 0.06466330587863922\n",
      "Logits: tensor([-0.3641, -0.3734, -0.3711,  ..., -7.0973, -7.6054, -7.5275],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [5/32], Loss: 0.06514883041381836\n",
      "Logits: tensor([-0.3678, -0.3731, -0.3688,  ..., -7.3639, -7.7969, -7.5613],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [6/32], Loss: 0.06449132412672043\n",
      "Logits: tensor([-0.3681, -0.3733, -0.3635,  ..., -7.5730, -7.6439, -7.5215],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [7/32], Loss: 0.06446850299835205\n",
      "Logits: tensor([-0.3661, -0.3707, -0.3676,  ..., -7.9274, -7.5845, -7.4361],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [8/32], Loss: 0.06539201736450195\n",
      "Logits: tensor([-0.3730, -0.3713, -0.3713,  ..., -7.4977, -7.5179, -7.5477],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [9/32], Loss: 0.06470955163240433\n",
      "Logits: tensor([-0.3659, -0.3651, -0.3723,  ..., -7.6740, -7.7563, -7.0429],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [10/32], Loss: 0.0637328177690506\n",
      "Logits: tensor([-0.3617, -0.3705, -0.3613,  ..., -7.5495, -7.5426, -7.5255],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [11/32], Loss: 0.06492525339126587\n",
      "Logits: tensor([-0.3701, -0.3707, -0.3659,  ..., -7.7043, -7.6487, -7.6238],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [12/32], Loss: 0.06515597552061081\n",
      "Logits: tensor([-0.3685, -0.3671, -0.3667,  ..., -7.5179, -7.4838, -7.7073],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [13/32], Loss: 0.06542487442493439\n",
      "Logits: tensor([-0.3695, -0.3647, -0.3660,  ..., -7.4169, -7.5715, -7.7352],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [14/32], Loss: 0.06601478159427643\n",
      "Logits: tensor([-0.3708, -0.3720, -0.3627,  ..., -7.1934, -7.4731, -7.2685],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [15/32], Loss: 0.06538239866495132\n",
      "Logits: tensor([-0.3619, -0.3681, -0.3636,  ..., -7.4637, -7.4134, -7.3981],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [16/32], Loss: 0.06499569863080978\n",
      "Logits: tensor([-0.3739, -0.3603, -0.3645,  ..., -7.4546, -7.7326, -7.4516],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [17/32], Loss: 0.06495989114046097\n",
      "Logits: tensor([-0.3694, -0.3616, -0.3633,  ..., -7.2454, -7.3770, -7.4218],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [18/32], Loss: 0.06505808979272842\n",
      "Logits: tensor([-0.3617, -0.3684, -0.3622,  ..., -7.2840, -7.4934, -7.6998],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [19/32], Loss: 0.06577035784721375\n",
      "Logits: tensor([-0.3610, -0.3655, -0.3650,  ..., -7.9127, -7.6384, -7.9281],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [20/32], Loss: 0.06570474803447723\n",
      "Logits: tensor([-0.3653, -0.3648, -0.3721,  ..., -7.3979, -7.3493, -7.3182],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [21/32], Loss: 0.06443134695291519\n",
      "Logits: tensor([-0.3707, -0.3674, -0.3669,  ..., -7.6238, -7.5556, -7.2743],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [22/32], Loss: 0.06453634053468704\n",
      "Logits: tensor([-0.3701, -0.3691, -0.3723,  ..., -7.3590, -7.7055, -7.6388],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [23/32], Loss: 0.06511839479207993\n",
      "Logits: tensor([-0.3701, -0.3665, -0.3690,  ..., -7.6393, -7.7377, -7.5118],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [24/32], Loss: 0.06432999670505524\n",
      "Logits: tensor([-0.3694, -0.3634, -0.3639,  ..., -7.4413, -7.5872, -7.5710],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [25/32], Loss: 0.06465426087379456\n",
      "Logits: tensor([-0.3637, -0.3682, -0.3688,  ..., -7.6915, -7.4755, -7.5895],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [26/32], Loss: 0.065509133040905\n",
      "Logits: tensor([-0.3645, -0.3724, -0.3665,  ..., -7.2722, -7.3466, -7.6059],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [27/32], Loss: 0.06545121967792511\n",
      "Logits: tensor([-0.3612, -0.3647, -0.3668,  ..., -7.7163, -7.7879, -7.4435],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [28/32], Loss: 0.06532089412212372\n",
      "Logits: tensor([-0.3624, -0.3657, -0.3687,  ..., -7.6453, -7.6698, -7.5172],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [29/32], Loss: 0.06457813084125519\n",
      "Logits: tensor([-0.3667, -0.3666, -0.3677,  ..., -7.4105, -7.6487, -7.5306],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [30/32], Loss: 0.06474380195140839\n",
      "Logits: tensor([-0.3677, -0.3611, -0.3657,  ..., -7.3008, -7.3618, -7.7126],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [31/32], Loss: 0.06483707576990128\n",
      "Logits: tensor([-0.3660, -0.3691, -0.3648,  ..., -7.5979, -7.5768, -7.6397],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Batch [32/32], Loss: 0.064114049077034\n",
      "Logits: tensor([-0.3662, -0.3645, -0.3579,  ..., -7.0837, -7.5572, -7.5691],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [22/50], Loss: 0.064114049077034\n",
      "Epoch [23/50], Batch [1/32], Loss: 0.06521841138601303\n",
      "Logits: tensor([-0.3661, -0.3558, -0.3689,  ..., -7.5857, -7.5518, -7.3549],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [2/32], Loss: 0.06507416069507599\n",
      "Logits: tensor([-0.3661, -0.3619, -0.3598,  ..., -7.5322, -7.4837, -7.4521],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [3/32], Loss: 0.06524822115898132\n",
      "Logits: tensor([-0.3655, -0.3670, -0.3662,  ..., -7.7800, -7.6208, -7.5051],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [4/32], Loss: 0.06480664014816284\n",
      "Logits: tensor([-0.3578, -0.3667, -0.3697,  ..., -7.7129, -7.6072, -7.8157],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [5/32], Loss: 0.06454053521156311\n",
      "Logits: tensor([-0.3685, -0.3669, -0.3658,  ..., -7.7117, -7.5004, -7.4586],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [6/32], Loss: 0.06518050283193588\n",
      "Logits: tensor([-0.3640, -0.3624, -0.3619,  ..., -7.3797, -7.7609, -7.5755],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [7/32], Loss: 0.06587260961532593\n",
      "Logits: tensor([-0.3613, -0.3646, -0.3676,  ..., -7.4388, -7.7188, -7.3065],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [8/32], Loss: 0.06576208025217056\n",
      "Logits: tensor([-0.3710, -0.3634, -0.3632,  ..., -7.5923, -7.7857, -7.4547],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [9/32], Loss: 0.0644925981760025\n",
      "Logits: tensor([-0.3673, -0.3677, -0.3604,  ..., -7.4315, -7.6442, -7.5854],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [10/32], Loss: 0.06465858966112137\n",
      "Logits: tensor([-0.3626, -0.3668, -0.3662,  ..., -7.3350, -7.5798, -7.4398],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [11/32], Loss: 0.0650070384144783\n",
      "Logits: tensor([-0.3630, -0.3654, -0.3659,  ..., -7.5885, -7.6366, -7.2645],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [12/32], Loss: 0.0651756152510643\n",
      "Logits: tensor([-0.3647, -0.3626, -0.3630,  ..., -7.4231, -7.7312, -7.4913],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [13/32], Loss: 0.0655057281255722\n",
      "Logits: tensor([-0.3619, -0.3598, -0.3618,  ..., -7.4701, -7.3824, -7.1859],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [14/32], Loss: 0.06531497091054916\n",
      "Logits: tensor([-0.3654, -0.3655, -0.3665,  ..., -7.3983, -7.6599, -7.5345],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [15/32], Loss: 0.06485796719789505\n",
      "Logits: tensor([-0.3538, -0.3624, -0.3616,  ..., -7.6972, -7.2814, -7.6297],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [16/32], Loss: 0.06508069485425949\n",
      "Logits: tensor([-0.3611, -0.3636, -0.3625,  ..., -7.4238, -7.4178, -7.4453],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [17/32], Loss: 0.06519555300474167\n",
      "Logits: tensor([-0.3634, -0.3627, -0.3657,  ..., -7.6603, -7.7590, -7.5324],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [18/32], Loss: 0.06471516937017441\n",
      "Logits: tensor([-0.3662, -0.3658, -0.3621,  ..., -7.2362, -7.8740, -7.1883],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [19/32], Loss: 0.06508452445268631\n",
      "Logits: tensor([-0.3617, -0.3689, -0.3629,  ..., -7.7232, -7.5764, -7.6014],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [20/32], Loss: 0.06509613990783691\n",
      "Logits: tensor([-0.3591, -0.3703, -0.3629,  ..., -7.5654, -7.2206, -7.6066],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [21/32], Loss: 0.06481505930423737\n",
      "Logits: tensor([-0.3673, -0.3639, -0.3684,  ..., -7.4887, -7.4123, -7.2709],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [22/32], Loss: 0.0642615333199501\n",
      "Logits: tensor([-0.3643, -0.3619, -0.3656,  ..., -7.8094, -7.3478, -7.4638],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [23/32], Loss: 0.06444002687931061\n",
      "Logits: tensor([-0.3591, -0.3627, -0.3627,  ..., -7.5922, -7.3530, -7.3619],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [24/32], Loss: 0.06494332104921341\n",
      "Logits: tensor([-0.3575, -0.3632, -0.3648,  ..., -7.7342, -7.4460, -7.7422],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [25/32], Loss: 0.06544771790504456\n",
      "Logits: tensor([-0.3628, -0.3668, -0.3644,  ..., -7.5039, -7.5237, -7.5407],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [26/32], Loss: 0.06521610915660858\n",
      "Logits: tensor([-0.3629, -0.3619, -0.3652,  ..., -7.5343, -7.4665, -7.7091],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [27/32], Loss: 0.06448917835950851\n",
      "Logits: tensor([-0.3623, -0.3577, -0.3679,  ..., -7.4703, -7.6120, -7.3161],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [28/32], Loss: 0.064322330057621\n",
      "Logits: tensor([-0.3560, -0.3651, -0.3670,  ..., -7.6604, -7.6009, -7.6280],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [29/32], Loss: 0.06459502130746841\n",
      "Logits: tensor([-0.3655, -0.3640, -0.3575,  ..., -7.4791, -7.5786, -7.6141],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [30/32], Loss: 0.06498551368713379\n",
      "Logits: tensor([-0.3611, -0.3680, -0.3655,  ..., -7.9101, -7.6095, -7.8186],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [31/32], Loss: 0.06495673954486847\n",
      "Logits: tensor([-0.3648, -0.3598, -0.3611,  ..., -7.3841, -7.6624, -7.8379],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Batch [32/32], Loss: 0.06397734582424164\n",
      "Logits: tensor([-0.3644, -0.3618, -0.3645,  ..., -7.4436, -7.7935, -7.5880],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [23/50], Loss: 0.06397734582424164\n",
      "Epoch [24/50], Batch [1/32], Loss: 0.06529369950294495\n",
      "Logits: tensor([-0.3652, -0.3579, -0.3619,  ..., -7.7193, -7.3523, -7.6467],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [2/32], Loss: 0.06485289335250854\n",
      "Logits: tensor([-0.3628, -0.3625, -0.3579,  ..., -7.4049, -7.6942, -7.5472],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [3/32], Loss: 0.06520868092775345\n",
      "Logits: tensor([-0.3600, -0.3606, -0.3655,  ..., -7.6479, -7.4618, -7.5468],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [4/32], Loss: 0.0648459643125534\n",
      "Logits: tensor([-0.3565, -0.3618, -0.3605,  ..., -7.5719, -7.7256, -7.4937],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [5/32], Loss: 0.06465993821620941\n",
      "Logits: tensor([-0.3648, -0.3625, -0.3637,  ..., -7.5517, -7.5043, -7.6670],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [6/32], Loss: 0.06466743350028992\n",
      "Logits: tensor([-0.3625, -0.3631, -0.3636,  ..., -7.3595, -7.2661, -7.4861],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [7/32], Loss: 0.0647665336728096\n",
      "Logits: tensor([-0.3650, -0.3623, -0.3552,  ..., -7.4802, -7.5487, -7.7127],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [8/32], Loss: 0.06442097574472427\n",
      "Logits: tensor([-0.3624, -0.3596, -0.3668,  ..., -7.5553, -7.4926, -7.8107],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [9/32], Loss: 0.06496769934892654\n",
      "Logits: tensor([-0.3694, -0.3610, -0.3649,  ..., -7.5039, -7.7644, -7.4223],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [10/32], Loss: 0.06465914100408554\n",
      "Logits: tensor([-0.3713, -0.3580, -0.3611,  ..., -7.3566, -7.5445, -7.3478],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [11/32], Loss: 0.06529831141233444\n",
      "Logits: tensor([-0.3507, -0.3593, -0.3585,  ..., -7.4640, -7.7441, -7.5503],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [12/32], Loss: 0.06529136747121811\n",
      "Logits: tensor([-0.3571, -0.3661, -0.3567,  ..., -7.5150, -7.5749, -7.4630],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [13/32], Loss: 0.06476336717605591\n",
      "Logits: tensor([-0.3545, -0.3610, -0.3599,  ..., -7.7365, -7.4558, -7.5143],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [14/32], Loss: 0.06504680216312408\n",
      "Logits: tensor([-0.3642, -0.3579, -0.3573,  ..., -7.6559, -7.5326, -7.8272],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [15/32], Loss: 0.06451099365949631\n",
      "Logits: tensor([-0.3602, -0.3627, -0.3589,  ..., -7.4512, -7.6909, -7.5721],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [16/32], Loss: 0.06526961922645569\n",
      "Logits: tensor([-0.3635, -0.3594, -0.3600,  ..., -7.7033, -7.5555, -7.7190],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [17/32], Loss: 0.06459347903728485\n",
      "Logits: tensor([-0.3656, -0.3552, -0.3637,  ..., -7.6603, -7.6606, -7.7583],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [18/32], Loss: 0.06412148475646973\n",
      "Logits: tensor([-0.3604, -0.3670, -0.3642,  ..., -7.5910, -7.4821, -7.5446],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [19/32], Loss: 0.06519752740859985\n",
      "Logits: tensor([-0.3525, -0.3614, -0.3644,  ..., -7.4919, -7.6342, -7.3373],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [20/32], Loss: 0.06438066065311432\n",
      "Logits: tensor([-0.3653, -0.3604, -0.3659,  ..., -7.5540, -7.6972, -7.7033],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [21/32], Loss: 0.06451192498207092\n",
      "Logits: tensor([-0.3658, -0.3620, -0.3568,  ..., -7.3851, -7.4914, -7.5054],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [22/32], Loss: 0.06459492444992065\n",
      "Logits: tensor([-0.3580, -0.3595, -0.3614,  ..., -7.9335, -7.6318, -7.8417],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [23/32], Loss: 0.06476651877164841\n",
      "Logits: tensor([-0.3593, -0.3587, -0.3622,  ..., -7.5566, -7.5061, -7.1794],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [24/32], Loss: 0.0648835152387619\n",
      "Logits: tensor([-0.3602, -0.3559, -0.3538,  ..., -7.7995, -7.5837, -7.5952],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [25/32], Loss: 0.06566465646028519\n",
      "Logits: tensor([-0.3591, -0.3611, -0.3625,  ..., -7.8806, -7.7199, -7.6624],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [26/32], Loss: 0.06553774327039719\n",
      "Logits: tensor([-0.3644, -0.3541, -0.3640,  ..., -7.5097, -7.5067, -7.6594],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [27/32], Loss: 0.06476303189992905\n",
      "Logits: tensor([-0.3562, -0.3533, -0.3570,  ..., -7.3503, -7.4121, -7.7658],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [28/32], Loss: 0.06582246720790863\n",
      "Logits: tensor([-0.3530, -0.3665, -0.3633,  ..., -7.4832, -7.5065, -7.6109],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [29/32], Loss: 0.06562013179063797\n",
      "Logits: tensor([-0.3604, -0.3568, -0.3614,  ..., -7.5851, -7.6282, -7.5085],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [30/32], Loss: 0.0651252418756485\n",
      "Logits: tensor([-0.3620, -0.3579, -0.3579,  ..., -7.6545, -7.5740, -7.7342],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [31/32], Loss: 0.06453274190425873\n",
      "Logits: tensor([-0.3618, -0.3557, -0.3600,  ..., -7.7656, -7.6524, -7.6285],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Batch [32/32], Loss: 0.06559592485427856\n",
      "Logits: tensor([-0.3612, -0.3633, -0.3557,  ..., -7.3994, -7.3468, -7.6583],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [24/50], Loss: 0.06559592485427856\n",
      "Epoch [25/50], Batch [1/32], Loss: 0.06454548984766006\n",
      "Logits: tensor([-0.3621, -0.3603, -0.3600,  ..., -7.4367, -7.5241, -7.6205],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [2/32], Loss: 0.06540504097938538\n",
      "Logits: tensor([-0.3589, -0.3547, -0.3590,  ..., -7.3531, -7.3281, -7.7622],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [3/32], Loss: 0.06422004103660583\n",
      "Logits: tensor([-0.3612, -0.3571, -0.3577,  ..., -7.5208, -7.5874, -7.6670],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [4/32], Loss: 0.06481297314167023\n",
      "Logits: tensor([-0.3511, -0.3542, -0.3568,  ..., -7.8864, -7.4668, -7.6240],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [5/32], Loss: 0.06487590819597244\n",
      "Logits: tensor([-0.3528, -0.3590, -0.3604,  ..., -7.7205, -7.6052, -7.7629],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [6/32], Loss: 0.06415765732526779\n",
      "Logits: tensor([-0.3561, -0.3575, -0.3564,  ..., -7.4970, -7.4299, -7.4257],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [7/32], Loss: 0.06466227769851685\n",
      "Logits: tensor([-0.3625, -0.3534, -0.3556,  ..., -7.7539, -7.4807, -7.7276],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [8/32], Loss: 0.06513424217700958\n",
      "Logits: tensor([-0.3595, -0.3597, -0.3661,  ..., -7.5534, -7.8070, -7.8282],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [9/32], Loss: 0.06476227194070816\n",
      "Logits: tensor([-0.3583, -0.3553, -0.3586,  ..., -7.7304, -7.6672, -7.4264],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [10/32], Loss: 0.064662404358387\n",
      "Logits: tensor([-0.3606, -0.3560, -0.3548,  ..., -7.8269, -7.6499, -7.7488],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [11/32], Loss: 0.06504867225885391\n",
      "Logits: tensor([-0.3547, -0.3606, -0.3557,  ..., -7.8370, -7.8834, -7.6499],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [12/32], Loss: 0.06429692357778549\n",
      "Logits: tensor([-0.3579, -0.3587, -0.3595,  ..., -7.4471, -7.2220, -7.9149],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [13/32], Loss: 0.06579618901014328\n",
      "Logits: tensor([-0.3613, -0.3550, -0.3543,  ..., -7.3482, -7.5048, -7.7993],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [14/32], Loss: 0.06504525989294052\n",
      "Logits: tensor([-0.3611, -0.3551, -0.3616,  ..., -7.5951, -7.6382, -7.5183],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [15/32], Loss: 0.06463728100061417\n",
      "Logits: tensor([-0.3585, -0.3591, -0.3507,  ..., -7.4762, -7.8718, -7.4143],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [16/32], Loss: 0.0648794174194336\n",
      "Logits: tensor([-0.3575, -0.3609, -0.3614,  ..., -7.6945, -7.5224, -7.4015],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [17/32], Loss: 0.06469836831092834\n",
      "Logits: tensor([-0.3613, -0.3501, -0.3607,  ..., -7.5957, -7.3884, -7.4906],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [18/32], Loss: 0.06451167911291122\n",
      "Logits: tensor([-0.3524, -0.3541, -0.3584,  ..., -7.6861, -7.3315, -7.7214],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [19/32], Loss: 0.06506215780973434\n",
      "Logits: tensor([-0.3558, -0.3556, -0.3561,  ..., -7.3888, -7.6360, -7.4947],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [20/32], Loss: 0.06456080079078674\n",
      "Logits: tensor([-0.3532, -0.3572, -0.3580,  ..., -7.7755, -7.3651, -7.4365],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [21/32], Loss: 0.06536053866147995\n",
      "Logits: tensor([-0.3622, -0.3594, -0.3565,  ..., -7.4500, -7.4720, -7.7503],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [22/32], Loss: 0.0646708533167839\n",
      "Logits: tensor([-0.3587, -0.3546, -0.3499,  ..., -7.9057, -7.7183, -7.7458],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [23/32], Loss: 0.06531180441379547\n",
      "Logits: tensor([-0.3528, -0.3563, -0.3542,  ..., -7.6586, -7.7665, -7.4872],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [24/32], Loss: 0.06468541920185089\n",
      "Logits: tensor([-0.3576, -0.3633, -0.3602,  ..., -7.5454, -7.4479, -7.8280],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [25/32], Loss: 0.0650302916765213\n",
      "Logits: tensor([-0.3542, -0.3649, -0.3583,  ..., -7.6663, -7.7650, -7.5870],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [26/32], Loss: 0.06493081897497177\n",
      "Logits: tensor([-0.3582, -0.3594, -0.3501,  ..., -7.6325, -7.4592, -7.3180],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [27/32], Loss: 0.06499945372343063\n",
      "Logits: tensor([-0.3591, -0.3534, -0.3545,  ..., -7.5798, -7.5290, -7.2011],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [28/32], Loss: 0.06524547189474106\n",
      "Logits: tensor([-0.3534, -0.3541, -0.3580,  ..., -7.5758, -7.4503, -7.4848],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [29/32], Loss: 0.06518352776765823\n",
      "Logits: tensor([-0.3539, -0.3532, -0.3527,  ..., -7.8823, -7.5996, -7.6814],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [30/32], Loss: 0.06534530967473984\n",
      "Logits: tensor([-0.3567, -0.3587, -0.3540,  ..., -7.3517, -7.7198, -7.6471],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [31/32], Loss: 0.06486083567142487\n",
      "Logits: tensor([-0.3607, -0.3565, -0.3532,  ..., -7.4011, -7.2765, -7.6070],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Batch [32/32], Loss: 0.06528696417808533\n",
      "Logits: tensor([-0.3574, -0.3589, -0.3496,  ..., -7.4230, -7.5838, -7.7316],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [25/50], Loss: 0.06528696417808533\n",
      "Epoch [26/50], Batch [1/32], Loss: 0.0649690330028534\n",
      "Logits: tensor([-0.3523, -0.3534, -0.3529,  ..., -7.2876, -7.4491, -7.3979],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [2/32], Loss: 0.06408841907978058\n",
      "Logits: tensor([-0.3537, -0.3496, -0.3568,  ..., -7.7655, -7.6571, -7.4758],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [3/32], Loss: 0.06488059461116791\n",
      "Logits: tensor([-0.3518, -0.3499, -0.3541,  ..., -7.7047, -7.8149, -7.6947],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [4/32], Loss: 0.06528352200984955\n",
      "Logits: tensor([-0.3540, -0.3558, -0.3564,  ..., -7.4606, -7.2349, -7.9295],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [5/32], Loss: 0.06453225761651993\n",
      "Logits: tensor([-0.3586, -0.3530, -0.3591,  ..., -7.5988, -7.7322, -7.6264],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [6/32], Loss: 0.06458619982004166\n",
      "Logits: tensor([-0.3568, -0.3523, -0.3530,  ..., -7.7100, -7.7660, -7.1391],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [7/32], Loss: 0.0650012344121933\n",
      "Logits: tensor([-0.3505, -0.3545, -0.3486,  ..., -7.5476, -7.5393, -7.5437],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [8/32], Loss: 0.06470496207475662\n",
      "Logits: tensor([-0.3585, -0.3536, -0.3592,  ..., -7.4702, -7.7072, -7.8646],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [9/32], Loss: 0.06442617624998093\n",
      "Logits: tensor([-0.3539, -0.3538, -0.3533,  ..., -7.0874, -7.6823, -7.5416],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [10/32], Loss: 0.06473837047815323\n",
      "Logits: tensor([-0.3566, -0.3503, -0.3568,  ..., -7.4025, -7.7269, -7.5855],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [11/32], Loss: 0.06454113870859146\n",
      "Logits: tensor([-0.3500, -0.3525, -0.3514,  ..., -7.8246, -7.6842, -7.5187],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [12/32], Loss: 0.06579149514436722\n",
      "Logits: tensor([-0.3523, -0.3535, -0.3585,  ..., -7.6902, -7.7233, -7.5163],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [13/32], Loss: 0.06420942395925522\n",
      "Logits: tensor([-0.3566, -0.3515, -0.3534,  ..., -7.4127, -7.2200, -7.4079],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [14/32], Loss: 0.06553276628255844\n",
      "Logits: tensor([-0.3488, -0.3503, -0.3517,  ..., -7.8294, -7.6921, -7.4082],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [15/32], Loss: 0.06538061052560806\n",
      "Logits: tensor([-0.3510, -0.3503, -0.3482,  ..., -7.6093, -7.3374, -7.5315],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [16/32], Loss: 0.06455177813768387\n",
      "Logits: tensor([-0.3525, -0.3534, -0.3562,  ..., -7.6780, -7.9127, -7.5510],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [17/32], Loss: 0.06489865481853485\n",
      "Logits: tensor([-0.3484, -0.3503, -0.3554,  ..., -7.4142, -7.6358, -7.5853],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [18/32], Loss: 0.06373948603868484\n",
      "Logits: tensor([-0.3478, -0.3569, -0.3545,  ..., -7.7409, -7.5328, -7.6806],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [19/32], Loss: 0.06505081057548523\n",
      "Logits: tensor([-0.3534, -0.3478, -0.3518,  ..., -7.5805, -7.1734, -7.7438],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [20/32], Loss: 0.06468696147203445\n",
      "Logits: tensor([-0.3550, -0.3488, -0.3458,  ..., -7.7212, -7.5576, -7.8330],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [21/32], Loss: 0.06548867374658585\n",
      "Logits: tensor([-0.3534, -0.3555, -0.3532,  ..., -8.0123, -7.7341, -8.0282],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [22/32], Loss: 0.06478805094957352\n",
      "Logits: tensor([-0.3526, -0.3596, -0.3531,  ..., -7.6135, -7.4881, -7.5906],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [23/32], Loss: 0.06442981213331223\n",
      "Logits: tensor([-0.3533, -0.3546, -0.3518,  ..., -7.6757, -7.4359, -7.8593],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [24/32], Loss: 0.06525789946317673\n",
      "Logits: tensor([-0.3538, -0.3525, -0.3514,  ..., -7.7076, -7.5254, -7.7407],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [25/32], Loss: 0.06572025269269943\n",
      "Logits: tensor([-0.3486, -0.3502, -0.3560,  ..., -7.5177, -7.6905, -7.5540],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [26/32], Loss: 0.06494392454624176\n",
      "Logits: tensor([-0.3522, -0.3544, -0.3499,  ..., -7.8998, -8.0441, -7.8255],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [27/32], Loss: 0.0652722418308258\n",
      "Logits: tensor([-0.3537, -0.3521, -0.3498,  ..., -7.7956, -7.5809, -7.3762],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [28/32], Loss: 0.06471742689609528\n",
      "Logits: tensor([-0.3540, -0.3526, -0.3492,  ..., -7.6762, -7.7712, -7.5131],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [29/32], Loss: 0.06439772993326187\n",
      "Logits: tensor([-0.3568, -0.3547, -0.3523,  ..., -7.6127, -7.7457, -7.9729],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [30/32], Loss: 0.06479749828577042\n",
      "Logits: tensor([-0.3423, -0.3455, -0.3542,  ..., -7.4079, -7.5973, -7.3990],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [31/32], Loss: 0.06471336632966995\n",
      "Logits: tensor([-0.3555, -0.3495, -0.3505,  ..., -7.6362, -7.6735, -7.4646],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Batch [32/32], Loss: 0.06510216742753983\n",
      "Logits: tensor([-0.3499, -0.3480, -0.3520,  ..., -7.5686, -7.5483, -7.6085],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [26/50], Loss: 0.06510216742753983\n",
      "Epoch [27/50], Batch [1/32], Loss: 0.06440085172653198\n",
      "Logits: tensor([-0.3526, -0.3529, -0.3525,  ..., -7.4998, -7.5458, -7.5289],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [2/32], Loss: 0.06474091857671738\n",
      "Logits: tensor([-0.3494, -0.3554, -0.3522,  ..., -7.4839, -7.5720, -7.6345],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [3/32], Loss: 0.06429511308670044\n",
      "Logits: tensor([-0.3449, -0.3447, -0.3544,  ..., -7.7897, -7.5551, -7.7667],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [4/32], Loss: 0.0642947107553482\n",
      "Logits: tensor([-0.3552, -0.3504, -0.3514,  ..., -7.8675, -7.7328, -7.6312],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [5/32], Loss: 0.06463215500116348\n",
      "Logits: tensor([-0.3464, -0.3494, -0.3594,  ..., -7.8456, -7.5337, -7.4658],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [6/32], Loss: 0.06472355127334595\n",
      "Logits: tensor([-0.3543, -0.3496, -0.3482,  ..., -7.6587, -7.4694, -7.4536],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [7/32], Loss: 0.0646456927061081\n",
      "Logits: tensor([-0.3511, -0.3496, -0.3505,  ..., -7.8627, -7.3207, -7.4067],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [8/32], Loss: 0.06408035755157471\n",
      "Logits: tensor([-0.3477, -0.3437, -0.3516,  ..., -7.6197, -7.7529, -7.9804],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [9/32], Loss: 0.06492671370506287\n",
      "Logits: tensor([-0.3531, -0.3557, -0.3490,  ..., -7.6141, -7.6235, -7.4965],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [10/32], Loss: 0.06465782225131989\n",
      "Logits: tensor([-0.3423, -0.3510, -0.3418,  ..., -7.5861, -7.8256, -7.6824],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [11/32], Loss: 0.06471706926822662\n",
      "Logits: tensor([-0.3514, -0.3453, -0.3459,  ..., -7.4496, -7.6059, -7.4082],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [12/32], Loss: 0.06470946222543716\n",
      "Logits: tensor([-0.3576, -0.3501, -0.3422,  ..., -7.8165, -7.5245, -7.8245],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [13/32], Loss: 0.06476112455129623\n",
      "Logits: tensor([-0.3455, -0.3451, -0.3489,  ..., -7.6298, -7.3845, -7.7029],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [14/32], Loss: 0.06493083387613297\n",
      "Logits: tensor([-0.3484, -0.3480, -0.3486,  ..., -7.8932, -7.5186, -7.5192],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [15/32], Loss: 0.06473298370838165\n",
      "Logits: tensor([-0.3460, -0.3502, -0.3496,  ..., -7.6665, -7.5500, -7.4115],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [16/32], Loss: 0.06487737596035004\n",
      "Logits: tensor([-0.3512, -0.3452, -0.3467,  ..., -7.5607, -7.3797, -7.5772],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [17/32], Loss: 0.06521720439195633\n",
      "Logits: tensor([-0.3469, -0.3462, -0.3457,  ..., -7.9203, -7.7725, -7.7635],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [18/32], Loss: 0.06531152129173279\n",
      "Logits: tensor([-0.3486, -0.3494, -0.3497,  ..., -7.7400, -7.7483, -7.5009],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [19/32], Loss: 0.06546033173799515\n",
      "Logits: tensor([-0.3469, -0.3513, -0.3478,  ..., -7.3756, -7.4513, -7.7154],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [20/32], Loss: 0.06482289731502533\n",
      "Logits: tensor([-0.3504, -0.3473, -0.3476,  ..., -7.5068, -7.8207, -7.5052],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [21/32], Loss: 0.065260149538517\n",
      "Logits: tensor([-0.3478, -0.3483, -0.3465,  ..., -7.6344, -7.5849, -7.5528],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [22/32], Loss: 0.06559377163648605\n",
      "Logits: tensor([-0.3425, -0.3517, -0.3496,  ..., -7.6323, -7.8522, -7.5751],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [23/32], Loss: 0.06487786024808884\n",
      "Logits: tensor([-0.3431, -0.3522, -0.3428,  ..., -7.6435, -7.2071, -7.2825],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [24/32], Loss: 0.06436942517757416\n",
      "Logits: tensor([-0.3446, -0.3433, -0.3511,  ..., -7.4600, -7.8386, -7.5857],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [25/32], Loss: 0.06519486755132675\n",
      "Logits: tensor([-0.3497, -0.3467, -0.3452,  ..., -7.6091, -7.5338, -7.5736],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [26/32], Loss: 0.06480243057012558\n",
      "Logits: tensor([-0.3476, -0.3477, -0.3487,  ..., -7.6391, -7.6826, -7.5618],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [27/32], Loss: 0.06517620384693146\n",
      "Logits: tensor([-0.3458, -0.3471, -0.3521,  ..., -7.7683, -7.8186, -7.6683],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [28/32], Loss: 0.06456660479307175\n",
      "Logits: tensor([-0.3472, -0.3388, -0.3424,  ..., -7.7844, -7.9010, -7.7970],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [29/32], Loss: 0.06500022113323212\n",
      "Logits: tensor([-0.3460, -0.3428, -0.3466,  ..., -7.6024, -7.8580, -7.8794],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [30/32], Loss: 0.06456873565912247\n",
      "Logits: tensor([-0.3431, -0.3477, -0.3419,  ..., -7.6519, -7.6810, -7.5008],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [31/32], Loss: 0.06472998857498169\n",
      "Logits: tensor([-0.3501, -0.3509, -0.3434,  ..., -7.7389, -7.8498, -7.7290],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Batch [32/32], Loss: 0.06395094096660614\n",
      "Logits: tensor([-0.3487, -0.3448, -0.3427,  ..., -7.3767, -7.7916, -7.5238],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [27/50], Loss: 0.06395094096660614\n",
      "Epoch [28/50], Batch [1/32], Loss: 0.06562099605798721\n",
      "Logits: tensor([-0.3492, -0.3496, -0.3429,  ..., -7.2815, -7.5557, -7.6974],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [2/32], Loss: 0.06451773643493652\n",
      "Logits: tensor([-0.3480, -0.3480, -0.3448,  ..., -7.7453, -7.7536, -7.5060],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [3/32], Loss: 0.06504332274198532\n",
      "Logits: tensor([-0.3387, -0.3473, -0.3381,  ..., -7.5613, -7.5826, -7.5554],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [4/32], Loss: 0.06478381901979446\n",
      "Logits: tensor([-0.3426, -0.3404, -0.3465,  ..., -7.8072, -7.5139, -7.4689],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [5/32], Loss: 0.06527518481016159\n",
      "Logits: tensor([-0.3485, -0.3395, -0.3460,  ..., -7.7987, -7.1535, -7.6607],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [6/32], Loss: 0.06465919315814972\n",
      "Logits: tensor([-0.3513, -0.3461, -0.3476,  ..., -7.5630, -7.7328, -7.6469],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [7/32], Loss: 0.06480815261602402\n",
      "Logits: tensor([-0.3435, -0.3513, -0.3452,  ..., -7.4782, -7.5343, -7.4549],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [8/32], Loss: 0.06405013054609299\n",
      "Logits: tensor([-0.3450, -0.3472, -0.3388,  ..., -7.4446, -7.4356, -7.7004],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [9/32], Loss: 0.06513038277626038\n",
      "Logits: tensor([-0.3484, -0.3373, -0.3416,  ..., -7.6103, -7.5989, -7.9245],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [10/32], Loss: 0.06512308865785599\n",
      "Logits: tensor([-0.3431, -0.3412, -0.3456,  ..., -7.7830, -7.8868, -7.4371],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [11/32], Loss: 0.0645226463675499\n",
      "Logits: tensor([-0.3451, -0.3505, -0.3355,  ..., -7.4841, -7.8187, -7.5280],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [12/32], Loss: 0.06520406156778336\n",
      "Logits: tensor([-0.3358, -0.3424, -0.3479,  ..., -7.4384, -7.3439, -7.5670],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [13/32], Loss: 0.06436765938997269\n",
      "Logits: tensor([-0.3474, -0.3441, -0.3465,  ..., -7.4543, -7.6311, -7.7203],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [14/32], Loss: 0.06477627158164978\n",
      "Logits: tensor([-0.3418, -0.3427, -0.3387,  ..., -7.7440, -7.7139, -7.4498],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [15/32], Loss: 0.06448497623205185\n",
      "Logits: tensor([-0.3441, -0.3393, -0.3417,  ..., -7.5632, -7.5589, -7.3105],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [16/32], Loss: 0.06459317356348038\n",
      "Logits: tensor([-0.3435, -0.3460, -0.3449,  ..., -7.2347, -7.5760, -8.0009],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [17/32], Loss: 0.06472381949424744\n",
      "Logits: tensor([-0.3428, -0.3429, -0.3398,  ..., -7.7343, -7.5165, -7.4876],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [18/32], Loss: 0.06428465247154236\n",
      "Logits: tensor([-0.3442, -0.3411, -0.3408,  ..., -7.5586, -7.5477, -7.7713],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [19/32], Loss: 0.06435734033584595\n",
      "Logits: tensor([-0.3444, -0.3396, -0.3408,  ..., -7.9331, -7.5894, -7.5308],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [20/32], Loss: 0.06464242190122604\n",
      "Logits: tensor([-0.3451, -0.3453, -0.3463,  ..., -7.5366, -7.8636, -7.8246],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [21/32], Loss: 0.06445638835430145\n",
      "Logits: tensor([-0.3441, -0.3422, -0.3464,  ..., -7.6748, -7.4917, -7.9345],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [22/32], Loss: 0.06522806733846664\n",
      "Logits: tensor([-0.3421, -0.3448, -0.3471,  ..., -7.6668, -7.6748, -7.6011],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [23/32], Loss: 0.0649154782295227\n",
      "Logits: tensor([-0.3447, -0.3417, -0.3387,  ..., -7.6652, -7.6944, -7.5138],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [24/32], Loss: 0.06465895473957062\n",
      "Logits: tensor([-0.3420, -0.3402, -0.3426,  ..., -7.3034, -7.6952, -7.5268],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [25/32], Loss: 0.06460389494895935\n",
      "Logits: tensor([-0.3362, -0.3479, -0.3419,  ..., -7.8632, -7.6814, -7.4032],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [26/32], Loss: 0.06461860984563828\n",
      "Logits: tensor([-0.3390, -0.3434, -0.3441,  ..., -7.9355, -7.6503, -7.7329],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [27/32], Loss: 0.06478024274110794\n",
      "Logits: tensor([-0.3439, -0.3410, -0.3378,  ..., -7.5773, -7.4625, -7.6743],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [28/32], Loss: 0.06541960686445236\n",
      "Logits: tensor([-0.3407, -0.3453, -0.3440,  ..., -7.5667, -7.8261, -7.3796],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [29/32], Loss: 0.06405437737703323\n",
      "Logits: tensor([-0.3364, -0.3457, -0.3441,  ..., -7.9048, -7.7334, -7.6451],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [30/32], Loss: 0.06509578973054886\n",
      "Logits: tensor([-0.3400, -0.3506, -0.3397,  ..., -7.1422, -7.8537, -7.3722],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [31/32], Loss: 0.0646185576915741\n",
      "Logits: tensor([-0.3391, -0.3470, -0.3410,  ..., -7.2629, -8.0002, -7.8272],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Batch [32/32], Loss: 0.06531026214361191\n",
      "Logits: tensor([-0.3353, -0.3391, -0.3378,  ..., -7.3312, -7.5714, -7.7905],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [28/50], Loss: 0.06531026214361191\n",
      "Epoch [29/50], Batch [1/32], Loss: 0.06400849670171738\n",
      "Logits: tensor([-0.3425, -0.3450, -0.3371,  ..., -7.8740, -7.7724, -7.7860],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [2/32], Loss: 0.06461397558450699\n",
      "Logits: tensor([-0.3450, -0.3419, -0.3445,  ..., -7.7372, -7.4990, -7.6073],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [3/32], Loss: 0.06499473750591278\n",
      "Logits: tensor([-0.3427, -0.3408, -0.3365,  ..., -7.4508, -7.4339, -7.7291],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [4/32], Loss: 0.06422383338212967\n",
      "Logits: tensor([-0.3361, -0.3380, -0.3375,  ..., -7.7084, -7.7440, -7.5454],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [5/32], Loss: 0.06536013633012772\n",
      "Logits: tensor([-0.3351, -0.3429, -0.3437,  ..., -7.8584, -7.8313, -7.6346],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [6/32], Loss: 0.06495755165815353\n",
      "Logits: tensor([-0.3405, -0.3399, -0.3447,  ..., -7.6339, -7.7937, -7.7820],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [7/32], Loss: 0.06349871307611465\n",
      "Logits: tensor([-0.3376, -0.3413, -0.3434,  ..., -7.7509, -7.9449, -7.6466],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [8/32], Loss: 0.06465858966112137\n",
      "Logits: tensor([-0.3390, -0.3347, -0.3425,  ..., -7.4303, -7.3627, -7.3416],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [9/32], Loss: 0.06523371487855911\n",
      "Logits: tensor([-0.3440, -0.3457, -0.3397,  ..., -7.4630, -7.2683, -7.4582],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [10/32], Loss: 0.06526606529951096\n",
      "Logits: tensor([-0.3441, -0.3383, -0.3399,  ..., -7.8687, -7.8150, -7.5098],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [11/32], Loss: 0.06527236849069595\n",
      "Logits: tensor([-0.3375, -0.3424, -0.3352,  ..., -7.4459, -7.8497, -7.4414],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [12/32], Loss: 0.06484050303697586\n",
      "Logits: tensor([-0.3322, -0.3405, -0.3351,  ..., -7.7326, -7.7279, -7.3813],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [13/32], Loss: 0.06391491740942001\n",
      "Logits: tensor([-0.3346, -0.3409, -0.3419,  ..., -7.4614, -7.9596, -7.5579],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [14/32], Loss: 0.06500149518251419\n",
      "Logits: tensor([-0.3394, -0.3420, -0.3398,  ..., -7.8027, -7.8933, -7.6842],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [15/32], Loss: 0.0642901360988617\n",
      "Logits: tensor([-0.3407, -0.3440, -0.3466,  ..., -7.6216, -7.9647, -7.7172],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [16/32], Loss: 0.06525113433599472\n",
      "Logits: tensor([-0.3372, -0.3388, -0.3356,  ..., -7.5223, -7.7394, -7.8531],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [17/32], Loss: 0.06474418193101883\n",
      "Logits: tensor([-0.3414, -0.3430, -0.3396,  ..., -7.8421, -7.6520, -7.4858],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [18/32], Loss: 0.0654190331697464\n",
      "Logits: tensor([-0.3452, -0.3340, -0.3431,  ..., -7.5468, -7.6754, -7.6466],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [19/32], Loss: 0.06479065865278244\n",
      "Logits: tensor([-0.3341, -0.3433, -0.3409,  ..., -7.7216, -7.9194, -7.5814],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [20/32], Loss: 0.06534484773874283\n",
      "Logits: tensor([-0.3319, -0.3399, -0.3454,  ..., -7.5724, -7.9056, -7.7846],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [21/32], Loss: 0.06487074494361877\n",
      "Logits: tensor([-0.3352, -0.3379, -0.3357,  ..., -7.5520, -7.9084, -7.6154],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [22/32], Loss: 0.06421630829572678\n",
      "Logits: tensor([-0.3381, -0.3351, -0.3401,  ..., -7.9480, -7.3299, -7.6837],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [23/32], Loss: 0.06456983834505081\n",
      "Logits: tensor([-0.3369, -0.3440, -0.3410,  ..., -7.3614, -7.6881, -7.8883],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [24/32], Loss: 0.06491410732269287\n",
      "Logits: tensor([-0.3363, -0.3469, -0.3404,  ..., -7.8844, -7.6968, -7.6825],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [25/32], Loss: 0.06420421600341797\n",
      "Logits: tensor([-0.3348, -0.3366, -0.3372,  ..., -7.8869, -7.8908, -7.7396],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [26/32], Loss: 0.06461901217699051\n",
      "Logits: tensor([-0.3425, -0.3376, -0.3411,  ..., -7.5233, -7.7692, -7.6189],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [27/32], Loss: 0.06461617350578308\n",
      "Logits: tensor([-0.3435, -0.3381, -0.3389,  ..., -7.5474, -7.5850, -7.5557],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [28/32], Loss: 0.06464851647615433\n",
      "Logits: tensor([-0.3435, -0.3343, -0.3376,  ..., -7.4983, -7.6211, -8.0576],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [29/32], Loss: 0.06421135365962982\n",
      "Logits: tensor([-0.3364, -0.3339, -0.3345,  ..., -7.7765, -7.5605, -7.8411],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [30/32], Loss: 0.06459781527519226\n",
      "Logits: tensor([-0.3390, -0.3361, -0.3389,  ..., -7.7549, -7.6423, -7.4450],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [31/32], Loss: 0.06510846316814423\n",
      "Logits: tensor([-0.3348, -0.3430, -0.3381,  ..., -7.4834, -7.6979, -7.5802],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Batch [32/32], Loss: 0.06462635099887848\n",
      "Logits: tensor([-0.3351, -0.3345, -0.3412,  ..., -7.7462, -7.4708, -7.2276],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [29/50], Loss: 0.06462635099887848\n",
      "Epoch [30/50], Batch [1/32], Loss: 0.06512700766324997\n",
      "Logits: tensor([-0.3393, -0.3334, -0.3349,  ..., -7.7527, -7.8683, -7.6299],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [2/32], Loss: 0.064249187707901\n",
      "Logits: tensor([-0.3294, -0.3386, -0.3406,  ..., -7.6096, -7.4601, -7.5472],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [3/32], Loss: 0.06451184302568436\n",
      "Logits: tensor([-0.3353, -0.3368, -0.3314,  ..., -7.4184, -7.3254, -7.6981],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [4/32], Loss: 0.0645219013094902\n",
      "Logits: tensor([-0.3361, -0.3351, -0.3370,  ..., -7.6709, -7.9853, -7.2541],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [5/32], Loss: 0.06506407260894775\n",
      "Logits: tensor([-0.3396, -0.3322, -0.3368,  ..., -7.7336, -7.6271, -7.8213],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [6/32], Loss: 0.06467411667108536\n",
      "Logits: tensor([-0.3385, -0.3392, -0.3328,  ..., -7.8403, -7.9641, -7.6475],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [7/32], Loss: 0.06565774232149124\n",
      "Logits: tensor([-0.3355, -0.3366, -0.3440,  ..., -7.7605, -7.3594, -7.4990],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [8/32], Loss: 0.06473638117313385\n",
      "Logits: tensor([-0.3375, -0.3368, -0.3331,  ..., -7.5087, -7.8647, -7.7964],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [9/32], Loss: 0.06497970968484879\n",
      "Logits: tensor([-0.3387, -0.3369, -0.3395,  ..., -7.6552, -7.8475, -7.8452],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [10/32], Loss: 0.06496371328830719\n",
      "Logits: tensor([-0.3278, -0.3309, -0.3336,  ..., -7.8661, -7.8757, -7.8541],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [11/32], Loss: 0.0650712177157402\n",
      "Logits: tensor([-0.3333, -0.3295, -0.3396,  ..., -7.8336, -7.5729, -7.8170],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [12/32], Loss: 0.06410666555166245\n",
      "Logits: tensor([-0.3354, -0.3385, -0.3395,  ..., -7.7585, -7.5805, -7.5711],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [13/32], Loss: 0.0646928995847702\n",
      "Logits: tensor([-0.3407, -0.3387, -0.3335,  ..., -7.7621, -7.6493, -7.4518],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [14/32], Loss: 0.06454651802778244\n",
      "Logits: tensor([-0.3380, -0.3402, -0.3280,  ..., -7.7847, -7.5685, -7.8495],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [15/32], Loss: 0.06459419429302216\n",
      "Logits: tensor([-0.3363, -0.3412, -0.3331,  ..., -7.8057, -7.8241, -7.7203],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [16/32], Loss: 0.06420842558145523\n",
      "Logits: tensor([-0.3340, -0.3340, -0.3356,  ..., -7.3996, -7.3849, -7.6402],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [17/32], Loss: 0.06407833099365234\n",
      "Logits: tensor([-0.3343, -0.3358, -0.3352,  ..., -7.7088, -7.7335, -7.3956],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [18/32], Loss: 0.06494393199682236\n",
      "Logits: tensor([-0.3288, -0.3281, -0.3286,  ..., -7.4676, -7.6590, -7.4588],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [19/32], Loss: 0.0648791491985321\n",
      "Logits: tensor([-0.3332, -0.3359, -0.3382,  ..., -7.9993, -7.6023, -7.5402],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [20/32], Loss: 0.06471738964319229\n",
      "Logits: tensor([-0.3324, -0.3280, -0.3359,  ..., -7.4925, -7.6789, -7.6418],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [21/32], Loss: 0.06525106728076935\n",
      "Logits: tensor([-0.3397, -0.3344, -0.3320,  ..., -7.6977, -7.7270, -7.5454],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [22/32], Loss: 0.06463628262281418\n",
      "Logits: tensor([-0.3314, -0.3319, -0.3334,  ..., -7.6238, -7.3773, -7.7244],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [23/32], Loss: 0.06465785205364227\n",
      "Logits: tensor([-0.3244, -0.3315, -0.3350,  ..., -7.7726, -7.5531, -7.5241],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [24/32], Loss: 0.06546890735626221\n",
      "Logits: tensor([-0.3293, -0.3357, -0.3343,  ..., -7.6920, -8.0286, -7.6817],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [25/32], Loss: 0.06374717503786087\n",
      "Logits: tensor([-0.3321, -0.3333, -0.3350,  ..., -7.6441, -7.8057, -7.6498],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [26/32], Loss: 0.06446109712123871\n",
      "Logits: tensor([-0.3351, -0.3333, -0.3332,  ..., -7.9294, -7.2763, -7.6075],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [27/32], Loss: 0.06428935378789902\n",
      "Logits: tensor([-0.3323, -0.3294, -0.3283,  ..., -7.6857, -7.6533, -7.5292],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [28/32], Loss: 0.0644230842590332\n",
      "Logits: tensor([-0.3322, -0.3328, -0.3319,  ..., -7.4646, -7.7555, -7.6045],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [29/32], Loss: 0.06443490833044052\n",
      "Logits: tensor([-0.3274, -0.3389, -0.3313,  ..., -7.7658, -7.9892, -7.7485],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [30/32], Loss: 0.06511840224266052\n",
      "Logits: tensor([-0.3354, -0.3338, -0.3295,  ..., -7.4555, -7.3876, -7.3664],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [31/32], Loss: 0.06409609317779541\n",
      "Logits: tensor([-0.3365, -0.3289, -0.3320,  ..., -7.6323, -7.5985, -7.7505],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Batch [32/32], Loss: 0.06475147604942322\n",
      "Logits: tensor([-0.3303, -0.3333, -0.3301,  ..., -7.8099, -7.8350, -7.6785],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [30/50], Loss: 0.06475147604942322\n",
      "Epoch [31/50], Batch [1/32], Loss: 0.06491043418645859\n",
      "Logits: tensor([-0.3270, -0.3289, -0.3284,  ..., -7.8478, -7.6067, -7.5820],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [2/32], Loss: 0.06454145908355713\n",
      "Logits: tensor([-0.3299, -0.3338, -0.3318,  ..., -7.5013, -7.5510, -7.6400],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [3/32], Loss: 0.0652817115187645\n",
      "Logits: tensor([-0.3230, -0.3296, -0.3314,  ..., -8.0482, -7.6071, -7.8594],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [4/32], Loss: 0.0644809827208519\n",
      "Logits: tensor([-0.3272, -0.3287, -0.3301,  ..., -7.6992, -8.0363, -7.6889],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [5/32], Loss: 0.064356230199337\n",
      "Logits: tensor([-0.3284, -0.3320, -0.3342,  ..., -7.7715, -7.8875, -7.6484],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [6/32], Loss: 0.06473275274038315\n",
      "Logits: tensor([-0.3347, -0.3295, -0.3314,  ..., -7.1645, -7.7689, -7.6260],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [7/32], Loss: 0.06283412128686905\n",
      "Logits: tensor([-0.3348, -0.3278, -0.3257,  ..., -7.8805, -7.9846, -8.0910],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [8/32], Loss: 0.06493474543094635\n",
      "Logits: tensor([-0.3328, -0.3257, -0.3302,  ..., -7.7491, -7.7149, -7.5125],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [9/32], Loss: 0.06516589969396591\n",
      "Logits: tensor([-0.3356, -0.3266, -0.3273,  ..., -8.0899, -7.9263, -7.8651],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [10/32], Loss: 0.06476309150457382\n",
      "Logits: tensor([-0.3348, -0.3385, -0.3317,  ..., -7.5353, -7.5784, -7.6413],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [11/32], Loss: 0.06462903320789337\n",
      "Logits: tensor([-0.3305, -0.3287, -0.3220,  ..., -7.4608, -7.3846, -7.6119],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [12/32], Loss: 0.06461174041032791\n",
      "Logits: tensor([-0.3330, -0.3289, -0.3293,  ..., -7.6214, -7.8029, -7.6658],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [13/32], Loss: 0.06512358784675598\n",
      "Logits: tensor([-0.3318, -0.3299, -0.3278,  ..., -7.6568, -7.3222, -7.8034],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [14/32], Loss: 0.06551172584295273\n",
      "Logits: tensor([-0.3292, -0.3311, -0.3268,  ..., -7.5130, -7.5547, -7.8652],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [15/32], Loss: 0.06491003185510635\n",
      "Logits: tensor([-0.3318, -0.3289, -0.3318,  ..., -7.6782, -7.5732, -7.5889],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [16/32], Loss: 0.0651717409491539\n",
      "Logits: tensor([-0.3299, -0.3227, -0.3316,  ..., -7.5716, -7.5207, -7.4895],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [17/32], Loss: 0.06450889259576797\n",
      "Logits: tensor([-0.3269, -0.3312, -0.3309,  ..., -7.8824, -7.6045, -7.4964],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [18/32], Loss: 0.06392577290534973\n",
      "Logits: tensor([-0.3283, -0.3273, -0.3320,  ..., -7.6325, -7.6261, -7.6693],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [19/32], Loss: 0.06424304097890854\n",
      "Logits: tensor([-0.3345, -0.3297, -0.3308,  ..., -7.6784, -7.8245, -7.8313],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [20/32], Loss: 0.0648496001958847\n",
      "Logits: tensor([-0.3335, -0.3282, -0.3351,  ..., -7.8957, -7.7170, -7.6390],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [21/32], Loss: 0.06478246301412582\n",
      "Logits: tensor([-0.3270, -0.3335, -0.3337,  ..., -7.8142, -7.7916, -7.8659],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [22/32], Loss: 0.06473913043737411\n",
      "Logits: tensor([-0.3335, -0.3291, -0.3275,  ..., -7.7466, -7.5895, -7.7671],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [23/32], Loss: 0.06396111100912094\n",
      "Logits: tensor([-0.3286, -0.3284, -0.3328,  ..., -7.6461, -7.5322, -7.2685],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [24/32], Loss: 0.06426315754652023\n",
      "Logits: tensor([-0.3302, -0.3259, -0.3326,  ..., -7.6646, -7.6412, -7.7792],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [25/32], Loss: 0.0640515387058258\n",
      "Logits: tensor([-0.3282, -0.3259, -0.3321,  ..., -7.7068, -7.6718, -7.9024],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [26/32], Loss: 0.0652087852358818\n",
      "Logits: tensor([-0.3276, -0.3258, -0.3297,  ..., -7.5945, -7.7009, -7.7657],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [27/32], Loss: 0.06453998386859894\n",
      "Logits: tensor([-0.3293, -0.3244, -0.3257,  ..., -7.6574, -7.7111, -7.7810],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [28/32], Loss: 0.06451188027858734\n",
      "Logits: tensor([-0.3273, -0.3274, -0.3322,  ..., -7.9103, -7.5588, -7.8576],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [29/32], Loss: 0.06442958861589432\n",
      "Logits: tensor([-0.3262, -0.3330, -0.3306,  ..., -7.8450, -7.9362, -7.7258],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [30/32], Loss: 0.06481121480464935\n",
      "Logits: tensor([-0.3326, -0.3265, -0.3276,  ..., -7.7691, -7.7473, -7.8187],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [31/32], Loss: 0.06497561931610107\n",
      "Logits: tensor([-0.3258, -0.3260, -0.3287,  ..., -7.5797, -7.8125, -7.9510],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Batch [32/32], Loss: 0.06404627859592438\n",
      "Logits: tensor([-0.3248, -0.3273, -0.3259,  ..., -7.7405, -7.5640, -7.4201],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [31/50], Loss: 0.06404627859592438\n",
      "Epoch [32/50], Batch [1/32], Loss: 0.06476806849241257\n",
      "Logits: tensor([-0.3269, -0.3227, -0.3293,  ..., -7.8689, -7.7395, -7.9337],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [2/32], Loss: 0.06413612514734268\n",
      "Logits: tensor([-0.3267, -0.3313, -0.3284,  ..., -7.8556, -7.7099, -7.2430],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [3/32], Loss: 0.06411638110876083\n",
      "Logits: tensor([-0.3293, -0.3232, -0.3237,  ..., -7.7329, -7.7978, -7.8223],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [4/32], Loss: 0.06491680443286896\n",
      "Logits: tensor([-0.3295, -0.3264, -0.3249,  ..., -7.5523, -7.6203, -7.5442],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [5/32], Loss: 0.06470243632793427\n",
      "Logits: tensor([-0.3257, -0.3278, -0.3248,  ..., -7.4458, -7.8665, -7.5955],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [6/32], Loss: 0.06455972045660019\n",
      "Logits: tensor([-0.3236, -0.3230, -0.3303,  ..., -7.7669, -7.3812, -7.8772],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [7/32], Loss: 0.06526518613100052\n",
      "Logits: tensor([-0.3251, -0.3311, -0.3280,  ..., -7.7273, -7.7567, -7.5743],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [8/32], Loss: 0.06409119069576263\n",
      "Logits: tensor([-0.3226, -0.3298, -0.3316,  ..., -7.6394, -7.5978, -7.6567],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [9/32], Loss: 0.06409437954425812\n",
      "Logits: tensor([-0.3276, -0.3205, -0.3261,  ..., -7.7927, -8.0171, -7.7753],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [10/32], Loss: 0.06457257270812988\n",
      "Logits: tensor([-0.3282, -0.3279, -0.3249,  ..., -7.7412, -7.6025, -7.7234],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [11/32], Loss: 0.06486670672893524\n",
      "Logits: tensor([-0.3230, -0.3263, -0.3337,  ..., -7.8242, -7.7537, -7.3191],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [12/32], Loss: 0.06491836160421371\n",
      "Logits: tensor([-0.3246, -0.3194, -0.3250,  ..., -8.1248, -7.7814, -7.5608],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [13/32], Loss: 0.06452389806509018\n",
      "Logits: tensor([-0.3226, -0.3263, -0.3285,  ..., -7.8597, -7.7074, -7.6546],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [14/32], Loss: 0.064349465072155\n",
      "Logits: tensor([-0.3249, -0.3280, -0.3322,  ..., -7.8524, -7.6130, -7.7492],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [15/32], Loss: 0.0647568553686142\n",
      "Logits: tensor([-0.3230, -0.3254, -0.3198,  ..., -7.8755, -7.4475, -7.8060],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [16/32], Loss: 0.06486184895038605\n",
      "Logits: tensor([-0.3200, -0.3291, -0.3272,  ..., -7.5875, -7.7184, -7.7686],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [17/32], Loss: 0.06438229978084564\n",
      "Logits: tensor([-0.3261, -0.3228, -0.3251,  ..., -8.0530, -7.8169, -7.7766],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [18/32], Loss: 0.0645965039730072\n",
      "Logits: tensor([-0.3266, -0.3211, -0.3259,  ..., -7.6538, -7.7216, -7.8030],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [19/32], Loss: 0.06426377594470978\n",
      "Logits: tensor([-0.3237, -0.3193, -0.3270,  ..., -7.7362, -7.8342, -7.7416],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [20/32], Loss: 0.06486693769693375\n",
      "Logits: tensor([-0.3255, -0.3276, -0.3243,  ..., -7.7296, -7.7581, -8.0123],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [21/32], Loss: 0.06521628797054291\n",
      "Logits: tensor([-0.3286, -0.3235, -0.3293,  ..., -8.1252, -7.8423, -8.1415],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [22/32], Loss: 0.0647735595703125\n",
      "Logits: tensor([-0.3238, -0.3220, -0.3187,  ..., -7.7185, -7.4413, -7.6396],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [23/32], Loss: 0.06479625403881073\n",
      "Logits: tensor([-0.3204, -0.3138, -0.3249,  ..., -7.6209, -7.5847, -7.7223],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [24/32], Loss: 0.06390467286109924\n",
      "Logits: tensor([-0.3241, -0.3284, -0.3247,  ..., -7.5980, -7.6363, -7.6064],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [25/32], Loss: 0.06468807905912399\n",
      "Logits: tensor([-0.3191, -0.3312, -0.3183,  ..., -7.8019, -7.9186, -7.6782],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [26/32], Loss: 0.06507200747728348\n",
      "Logits: tensor([-0.3246, -0.3222, -0.3157,  ..., -7.6435, -7.7883, -7.5692],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [27/32], Loss: 0.06432835012674332\n",
      "Logits: tensor([-0.3218, -0.3286, -0.3262,  ..., -8.1074, -7.7473, -8.0588],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [28/32], Loss: 0.06458434462547302\n",
      "Logits: tensor([-0.3318, -0.3243, -0.3165,  ..., -7.6278, -7.7750, -7.6267],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [29/32], Loss: 0.06390713900327682\n",
      "Logits: tensor([-0.3216, -0.3173, -0.3252,  ..., -7.6855, -7.5843, -7.5771],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [30/32], Loss: 0.06534779816865921\n",
      "Logits: tensor([-0.3255, -0.3214, -0.3213,  ..., -7.6395, -7.7103, -7.8784],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [31/32], Loss: 0.0641593411564827\n",
      "Logits: tensor([-0.3306, -0.3205, -0.3229,  ..., -7.6759, -7.5932, -7.5816],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Batch [32/32], Loss: 0.06430985033512115\n",
      "Logits: tensor([-0.3276, -0.3290, -0.3225,  ..., -7.5273, -7.7253, -7.4323],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [32/50], Loss: 0.06430985033512115\n",
      "Epoch [33/50], Batch [1/32], Loss: 0.06464912742376328\n",
      "Logits: tensor([-0.3261, -0.3205, -0.3202,  ..., -8.0228, -7.7712, -7.7321],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [2/32], Loss: 0.0650859847664833\n",
      "Logits: tensor([-0.3202, -0.3228, -0.3176,  ..., -7.8917, -8.0167, -7.6974],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [3/32], Loss: 0.06454208493232727\n",
      "Logits: tensor([-0.3186, -0.3241, -0.3274,  ..., -7.6358, -7.7518, -7.9753],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [4/32], Loss: 0.06438975781202316\n",
      "Logits: tensor([-0.3231, -0.3212, -0.3255,  ..., -7.5509, -7.6496, -7.5993],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [5/32], Loss: 0.06502965837717056\n",
      "Logits: tensor([-0.3224, -0.3209, -0.3227,  ..., -7.5817, -7.6240, -7.7439],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [6/32], Loss: 0.06442498415708542\n",
      "Logits: tensor([-0.3250, -0.3221, -0.3161,  ..., -7.4666, -7.5121, -7.7205],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [7/32], Loss: 0.06400211900472641\n",
      "Logits: tensor([-0.3192, -0.3204, -0.3131,  ..., -7.7195, -7.8378, -7.5163],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [8/32], Loss: 0.06429751217365265\n",
      "Logits: tensor([-0.3264, -0.3201, -0.3099,  ..., -7.4999, -7.6817, -7.8683],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [9/32], Loss: 0.06441926211118698\n",
      "Logits: tensor([-0.3211, -0.3205, -0.3253,  ..., -7.5387, -7.7558, -7.6367],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [10/32], Loss: 0.06492838263511658\n",
      "Logits: tensor([-0.3184, -0.3170, -0.3250,  ..., -7.6681, -7.5841, -7.3863],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [11/32], Loss: 0.06448271125555038\n",
      "Logits: tensor([-0.3218, -0.3220, -0.3184,  ..., -7.7579, -7.8149, -7.5872],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [12/32], Loss: 0.06481969356536865\n",
      "Logits: tensor([-0.3222, -0.3180, -0.3211,  ..., -7.6292, -7.9193, -7.4937],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [13/32], Loss: 0.06409304589033127\n",
      "Logits: tensor([-0.3157, -0.3177, -0.3171,  ..., -7.9819, -7.8449, -7.7411],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [14/32], Loss: 0.06454315781593323\n",
      "Logits: tensor([-0.3207, -0.3178, -0.3232,  ..., -7.7267, -7.7694, -7.6170],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [15/32], Loss: 0.06471001356840134\n",
      "Logits: tensor([-0.3175, -0.3176, -0.3219,  ..., -7.7674, -7.5403, -7.9896],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [16/32], Loss: 0.06496841460466385\n",
      "Logits: tensor([-0.3156, -0.3235, -0.3203,  ..., -7.9377, -7.8668, -7.4723],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [17/32], Loss: 0.06437952071428299\n",
      "Logits: tensor([-0.3134, -0.3198, -0.3183,  ..., -7.9172, -7.6792, -7.9196],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [18/32], Loss: 0.06499762088060379\n",
      "Logits: tensor([-0.3226, -0.3212, -0.3227,  ..., -7.4336, -7.6741, -7.6617],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [19/32], Loss: 0.06424637883901596\n",
      "Logits: tensor([-0.3177, -0.3215, -0.3189,  ..., -7.9583, -7.8188, -7.5281],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [20/32], Loss: 0.06469108164310455\n",
      "Logits: tensor([-0.3222, -0.3192, -0.3177,  ..., -7.8364, -8.0055, -7.3273],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [21/32], Loss: 0.06458459794521332\n",
      "Logits: tensor([-0.3229, -0.3178, -0.3196,  ..., -7.0004, -7.7284, -7.6365],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [22/32], Loss: 0.06487958878278732\n",
      "Logits: tensor([-0.3244, -0.3178, -0.3229,  ..., -7.8979, -7.7681, -7.9631],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [23/32], Loss: 0.06423850357532501\n",
      "Logits: tensor([-0.3210, -0.3202, -0.3165,  ..., -7.8944, -7.8487, -7.7799],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [24/32], Loss: 0.06442800164222717\n",
      "Logits: tensor([-0.3176, -0.3200, -0.3219,  ..., -7.5627, -7.9181, -7.7891],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [25/32], Loss: 0.06445520371198654\n",
      "Logits: tensor([-0.3192, -0.3139, -0.3204,  ..., -7.9982, -7.6100, -8.0193],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [26/32], Loss: 0.06443870812654495\n",
      "Logits: tensor([-0.3115, -0.3181, -0.3164,  ..., -7.7529, -7.8183, -7.7617],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [27/32], Loss: 0.06495551019906998\n",
      "Logits: tensor([-0.3144, -0.3182, -0.3164,  ..., -7.5522, -7.5943, -7.9070],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [28/32], Loss: 0.06405340135097504\n",
      "Logits: tensor([-0.3112, -0.3266, -0.3162,  ..., -7.7600, -7.4024, -7.8021],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [29/32], Loss: 0.06538481265306473\n",
      "Logits: tensor([-0.3208, -0.3193, -0.3180,  ..., -7.8068, -7.5026, -7.4480],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [30/32], Loss: 0.06415949016809464\n",
      "Logits: tensor([-0.3196, -0.3220, -0.3191,  ..., -7.6380, -7.8149, -7.6756],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [31/32], Loss: 0.06396215409040451\n",
      "Logits: tensor([-0.3172, -0.3204, -0.3184,  ..., -7.5964, -7.6894, -7.6314],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Batch [32/32], Loss: 0.06369363516569138\n",
      "Logits: tensor([-0.3151, -0.3184, -0.3126,  ..., -7.7266, -7.4874, -7.7763],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [33/50], Loss: 0.06369363516569138\n",
      "Epoch [34/50], Batch [1/32], Loss: 0.06490969657897949\n",
      "Logits: tensor([-0.3142, -0.3153, -0.3107,  ..., -7.7315, -7.8179, -7.8141],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [2/32], Loss: 0.06413295865058899\n",
      "Logits: tensor([-0.3221, -0.3174, -0.3199,  ..., -7.9267, -7.7749, -7.8014],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [3/32], Loss: 0.06537597626447678\n",
      "Logits: tensor([-0.3176, -0.3129, -0.3151,  ..., -7.9273, -7.7336, -7.3507],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [4/32], Loss: 0.06435361504554749\n",
      "Logits: tensor([-0.3126, -0.3191, -0.3163,  ..., -7.4138, -7.6580, -7.8803],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [5/32], Loss: 0.06431393325328827\n",
      "Logits: tensor([-0.3176, -0.3179, -0.3157,  ..., -7.5993, -7.8195, -7.9348],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [6/32], Loss: 0.06424681097269058\n",
      "Logits: tensor([-0.3175, -0.3105, -0.3168,  ..., -7.5288, -7.4175, -7.5985],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [7/32], Loss: 0.06438443064689636\n",
      "Logits: tensor([-0.3090, -0.3173, -0.3119,  ..., -7.6945, -8.0637, -7.5773],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [8/32], Loss: 0.06492511183023453\n",
      "Logits: tensor([-0.3215, -0.3165, -0.3149,  ..., -7.6364, -7.7439, -7.8092],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [9/32], Loss: 0.0646352767944336\n",
      "Logits: tensor([-0.3182, -0.3145, -0.3203,  ..., -7.6248, -7.5531, -8.1740],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [10/32], Loss: 0.06483004242181778\n",
      "Logits: tensor([-0.3134, -0.3135, -0.3178,  ..., -7.5416, -7.4139, -7.7525],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [11/32], Loss: 0.06420086324214935\n",
      "Logits: tensor([-0.3261, -0.3208, -0.3152,  ..., -7.8367, -7.9066, -7.7716],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [12/32], Loss: 0.06436262279748917\n",
      "Logits: tensor([-0.3183, -0.3122, -0.3167,  ..., -7.6529, -7.8008, -7.6518],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [13/32], Loss: 0.06426864117383957\n",
      "Logits: tensor([-0.3154, -0.3136, -0.3102,  ..., -7.6870, -7.7771, -7.9673],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [14/32], Loss: 0.06459321826696396\n",
      "Logits: tensor([-0.3157, -0.3179, -0.3161,  ..., -7.6452, -7.7110, -7.7917],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [15/32], Loss: 0.06369534879922867\n",
      "Logits: tensor([-0.3184, -0.3170, -0.3205,  ..., -7.6613, -7.5424, -7.5864],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [16/32], Loss: 0.06462855637073517\n",
      "Logits: tensor([-0.3209, -0.3188, -0.3187,  ..., -7.4492, -7.6905, -7.6780],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [17/32], Loss: 0.06436441838741302\n",
      "Logits: tensor([-0.3169, -0.3142, -0.3145,  ..., -7.9178, -7.7900, -7.6790],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [18/32], Loss: 0.06424728780984879\n",
      "Logits: tensor([-0.3099, -0.3121, -0.3185,  ..., -7.8955, -7.7373, -7.8450],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [19/32], Loss: 0.06426118314266205\n",
      "Logits: tensor([-0.3200, -0.3178, -0.3148,  ..., -7.9199, -7.8094, -7.6224],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [20/32], Loss: 0.06450741738080978\n",
      "Logits: tensor([-0.3197, -0.3137, -0.3147,  ..., -7.5440, -7.8764, -7.7317],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [21/32], Loss: 0.0646737590432167\n",
      "Logits: tensor([-0.3131, -0.3125, -0.3119,  ..., -7.3201, -7.7150, -7.8357],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [22/32], Loss: 0.06387109309434891\n",
      "Logits: tensor([-0.3097, -0.3149, -0.3139,  ..., -7.7727, -7.8903, -7.8646],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [23/32], Loss: 0.06505720317363739\n",
      "Logits: tensor([-0.3151, -0.3067, -0.3146,  ..., -8.0720, -7.7450, -7.5818],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [24/32], Loss: 0.06443628668785095\n",
      "Logits: tensor([-0.3169, -0.3148, -0.3109,  ..., -7.6218, -7.7244, -7.7303],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [25/32], Loss: 0.06418196111917496\n",
      "Logits: tensor([-0.3156, -0.3086, -0.3141,  ..., -8.0695, -7.9039, -7.8444],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [26/32], Loss: 0.0654342845082283\n",
      "Logits: tensor([-0.3131, -0.3132, -0.3181,  ..., -7.8271, -7.9747, -7.5060],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [27/32], Loss: 0.06427288055419922\n",
      "Logits: tensor([-0.3099, -0.3149, -0.3128,  ..., -7.3323, -7.8641, -7.7830],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [28/32], Loss: 0.0652269572019577\n",
      "Logits: tensor([-0.3159, -0.3172, -0.3082,  ..., -7.8046, -7.7583, -7.7109],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [29/32], Loss: 0.06401633471250534\n",
      "Logits: tensor([-0.3060, -0.3120, -0.3085,  ..., -7.5851, -7.9940, -7.7524],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [30/32], Loss: 0.06433913856744766\n",
      "Logits: tensor([-0.3123, -0.3185, -0.3131,  ..., -7.9765, -7.7915, -7.5077],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [31/32], Loss: 0.06509946286678314\n",
      "Logits: tensor([-0.3159, -0.3222, -0.3104,  ..., -7.8348, -7.4752, -7.6403],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Batch [32/32], Loss: 0.06408174335956573\n",
      "Logits: tensor([-0.3149, -0.3152, -0.3092,  ..., -7.7314, -7.6887, -7.9034],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [34/50], Loss: 0.06408174335956573\n",
      "Epoch [35/50], Batch [1/32], Loss: 0.06476330012083054\n",
      "Logits: tensor([-0.3159, -0.3114, -0.3150,  ..., -7.4543, -7.6352, -7.9722],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [2/32], Loss: 0.06465593725442886\n",
      "Logits: tensor([-0.3149, -0.3167, -0.3139,  ..., -7.5834, -7.9708, -7.7123],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [3/32], Loss: 0.0647391751408577\n",
      "Logits: tensor([-0.3125, -0.3121, -0.3090,  ..., -7.5030, -7.8490, -7.7918],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [4/32], Loss: 0.06420700252056122\n",
      "Logits: tensor([-0.3125, -0.3092, -0.3107,  ..., -7.6978, -7.4030, -7.8379],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [5/32], Loss: 0.06448838859796524\n",
      "Logits: tensor([-0.3123, -0.3082, -0.3146,  ..., -7.6311, -7.9629, -7.9252],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [6/32], Loss: 0.06434277445077896\n",
      "Logits: tensor([-0.3166, -0.3136, -0.3097,  ..., -7.9372, -7.7117, -7.8310],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [7/32], Loss: 0.06452982872724533\n",
      "Logits: tensor([-0.3102, -0.3098, -0.3043,  ..., -7.5549, -7.4589, -7.6869],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [8/32], Loss: 0.06430763751268387\n",
      "Logits: tensor([-0.3152, -0.3097, -0.3113,  ..., -7.9566, -7.6577, -7.9643],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [9/32], Loss: 0.0648437887430191\n",
      "Logits: tensor([-0.3075, -0.3059, -0.3131,  ..., -7.9024, -7.7519, -7.7629],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [10/32], Loss: 0.06509144604206085\n",
      "Logits: tensor([-0.3103, -0.3106, -0.3160,  ..., -7.5809, -7.7465, -7.8987],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [11/32], Loss: 0.06501085311174393\n",
      "Logits: tensor([-0.3156, -0.3104, -0.3171,  ..., -8.0379, -7.5585, -7.6795],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [12/32], Loss: 0.06406628340482712\n",
      "Logits: tensor([-0.3152, -0.3097, -0.3105,  ..., -7.7744, -7.7393, -7.7617],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [13/32], Loss: 0.06447815150022507\n",
      "Logits: tensor([-0.3109, -0.3163, -0.3037,  ..., -7.4989, -7.9628, -7.6058],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [14/32], Loss: 0.06462402641773224\n",
      "Logits: tensor([-0.3111, -0.3145, -0.3115,  ..., -7.9512, -7.7567, -7.3721],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [15/32], Loss: 0.0642959475517273\n",
      "Logits: tensor([-0.3144, -0.3031, -0.3074,  ..., -7.6049, -7.9469, -7.6501],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [16/32], Loss: 0.06434908509254456\n",
      "Logits: tensor([-0.3116, -0.3057, -0.3099,  ..., -7.7949, -7.7481, -7.7555],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [17/32], Loss: 0.0647081807255745\n",
      "Logits: tensor([-0.3012, -0.3083, -0.3119,  ..., -7.7913, -7.8569, -7.8818],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [18/32], Loss: 0.06452751904726028\n",
      "Logits: tensor([-0.3076, -0.3089, -0.3015,  ..., -7.6164, -7.9701, -7.6767],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [19/32], Loss: 0.06445253640413284\n",
      "Logits: tensor([-0.3131, -0.3113, -0.3107,  ..., -7.6018, -7.6397, -7.5138],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [20/32], Loss: 0.06450562924146652\n",
      "Logits: tensor([-0.3131, -0.3073, -0.3085,  ..., -8.0797, -7.6473, -7.8092],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [21/32], Loss: 0.06404651701450348\n",
      "Logits: tensor([-0.3111, -0.3125, -0.3093,  ..., -7.6472, -7.7788, -7.7488],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [22/32], Loss: 0.06545274704694748\n",
      "Logits: tensor([-0.3115, -0.3102, -0.3091,  ..., -7.8647, -7.7960, -7.7682],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [23/32], Loss: 0.06390028446912766\n",
      "Logits: tensor([-0.3100, -0.3139, -0.3066,  ..., -7.7193, -7.6814, -7.8627],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [24/32], Loss: 0.06448264420032501\n",
      "Logits: tensor([-0.3006, -0.3039, -0.3126,  ..., -7.5105, -7.8136, -7.6799],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [25/32], Loss: 0.06420491635799408\n",
      "Logits: tensor([-0.3112, -0.3102, -0.3066,  ..., -7.7410, -7.9464, -7.8749],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [26/32], Loss: 0.06481026113033295\n",
      "Logits: tensor([-0.3087, -0.3067, -0.3040,  ..., -7.9508, -7.8857, -7.7852],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [27/32], Loss: 0.06413278728723526\n",
      "Logits: tensor([-0.3032, -0.3103, -0.3125,  ..., -7.5146, -7.9392, -7.6687],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [28/32], Loss: 0.06459616869688034\n",
      "Logits: tensor([-0.3095, -0.3031, -0.3123,  ..., -7.8889, -7.9219, -8.1928],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [29/32], Loss: 0.06403399258852005\n",
      "Logits: tensor([-0.3080, -0.3058, -0.3118,  ..., -7.8055, -7.5297, -7.8868],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [30/32], Loss: 0.06358667463064194\n",
      "Logits: tensor([-0.3074, -0.3095, -0.3036,  ..., -7.6294, -7.7798, -7.6692],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [31/32], Loss: 0.06448749452829361\n",
      "Logits: tensor([-0.3133, -0.3079, -0.3034,  ..., -7.6152, -7.7782, -7.7255],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Batch [32/32], Loss: 0.06349514424800873\n",
      "Logits: tensor([-0.3087, -0.3063, -0.2998,  ..., -7.9775, -7.9705, -8.0859],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [35/50], Loss: 0.06349514424800873\n",
      "Epoch [36/50], Batch [1/32], Loss: 0.06380879133939743\n",
      "Logits: tensor([-0.3089, -0.3030, -0.3072,  ..., -7.8728, -7.9021, -8.0007],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [2/32], Loss: 0.06404826045036316\n",
      "Logits: tensor([-0.3043, -0.3090, -0.3105,  ..., -7.7763, -7.6711, -7.8252],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [3/32], Loss: 0.06439296901226044\n",
      "Logits: tensor([-0.3104, -0.3043, -0.3109,  ..., -7.7487, -7.9544, -7.8828],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [4/32], Loss: 0.06457330286502838\n",
      "Logits: tensor([-0.3040, -0.3071, -0.3168,  ..., -7.7474, -7.7769, -7.9280],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [5/32], Loss: 0.06415696442127228\n",
      "Logits: tensor([-0.3034, -0.3079, -0.3100,  ..., -7.9749, -7.7451, -7.4585],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [6/32], Loss: 0.06397645175457001\n",
      "Logits: tensor([-0.3045, -0.3064, -0.3095,  ..., -7.9670, -7.8142, -7.8409],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [7/32], Loss: 0.065193772315979\n",
      "Logits: tensor([-0.3085, -0.3086, -0.3152,  ..., -7.5361, -8.0285, -7.7897],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [8/32], Loss: 0.0646754801273346\n",
      "Logits: tensor([-0.3096, -0.3080, -0.3067,  ..., -7.9027, -7.6832, -7.8780],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [9/32], Loss: 0.06393987685441971\n",
      "Logits: tensor([-0.3054, -0.3065, -0.3028,  ..., -7.7596, -7.7953, -7.7817],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [10/32], Loss: 0.06454026699066162\n",
      "Logits: tensor([-0.2995, -0.3061, -0.3045,  ..., -7.5634, -7.6424, -7.9827],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [11/32], Loss: 0.06450873613357544\n",
      "Logits: tensor([-0.3063, -0.3087, -0.3078,  ..., -7.8160, -7.9348, -7.7512],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [12/32], Loss: 0.06425133347511292\n",
      "Logits: tensor([-0.3099, -0.3083, -0.3059,  ..., -7.9056, -7.8559, -7.8727],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [13/32], Loss: 0.06449169665575027\n",
      "Logits: tensor([-0.3112, -0.3068, -0.3080,  ..., -7.9822, -7.6571, -7.9755],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [14/32], Loss: 0.0647505670785904\n",
      "Logits: tensor([-0.3021, -0.3071, -0.3026,  ..., -7.5259, -8.0266, -7.8287],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [15/32], Loss: 0.06403374671936035\n",
      "Logits: tensor([-0.3009, -0.3115, -0.3011,  ..., -7.3096, -7.9511, -7.6943],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [16/32], Loss: 0.06424378603696823\n",
      "Logits: tensor([-0.3100, -0.3045, -0.3053,  ..., -7.7849, -7.9215, -7.8125],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [17/32], Loss: 0.06447355449199677\n",
      "Logits: tensor([-0.3055, -0.3033, -0.3093,  ..., -7.7166, -7.6744, -7.7340],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [18/32], Loss: 0.0641731321811676\n",
      "Logits: tensor([-0.3023, -0.3105, -0.3008,  ..., -7.9999, -7.6432, -7.9465],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [19/32], Loss: 0.06427869945764542\n",
      "Logits: tensor([-0.3054, -0.3054, -0.3064,  ..., -7.6490, -7.9116, -7.8430],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [20/32], Loss: 0.06400109082460403\n",
      "Logits: tensor([-0.3085, -0.3030, -0.3038,  ..., -7.7771, -7.7798, -7.7946],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [21/32], Loss: 0.06451868265867233\n",
      "Logits: tensor([-0.3078, -0.3084, -0.3020,  ..., -8.0672, -7.5855, -7.7071],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [22/32], Loss: 0.06419297307729721\n",
      "Logits: tensor([-0.3016, -0.3044, -0.3037,  ..., -7.7730, -7.9214, -7.9286],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [23/32], Loss: 0.06428045779466629\n",
      "Logits: tensor([-0.3022, -0.3088, -0.2998,  ..., -7.8805, -7.7668, -7.7394],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [24/32], Loss: 0.06452825665473938\n",
      "Logits: tensor([-0.3104, -0.3036, -0.3071,  ..., -7.7848, -7.9415, -7.8202],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [25/32], Loss: 0.06465091556310654\n",
      "Logits: tensor([-0.3045, -0.3060, -0.3058,  ..., -7.8277, -7.7098, -7.3126],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [26/32], Loss: 0.06437378376722336\n",
      "Logits: tensor([-0.3033, -0.3055, -0.3010,  ..., -7.7722, -7.9360, -7.9241],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [27/32], Loss: 0.06462892144918442\n",
      "Logits: tensor([-0.3023, -0.3027, -0.3019,  ..., -8.0478, -7.8234, -7.7679],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [28/32], Loss: 0.06483445316553116\n",
      "Logits: tensor([-0.3015, -0.3067, -0.2989,  ..., -7.8342, -7.8150, -7.5559],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [29/32], Loss: 0.0650252178311348\n",
      "Logits: tensor([-0.3045, -0.3050, -0.3060,  ..., -8.2098, -7.9235, -8.2266],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [30/32], Loss: 0.06474665552377701\n",
      "Logits: tensor([-0.3046, -0.3039, -0.3106,  ..., -7.7498, -7.8139, -7.6149],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [31/32], Loss: 0.0647200420498848\n",
      "Logits: tensor([-0.3062, -0.3033, -0.3014,  ..., -7.8159, -7.5441, -7.9759],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Batch [32/32], Loss: 0.06519031524658203\n",
      "Logits: tensor([-0.3021, -0.2997, -0.3045,  ..., -7.8294, -7.9786, -7.9345],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [36/50], Loss: 0.06519031524658203\n",
      "Epoch [37/50], Batch [1/32], Loss: 0.0640811175107956\n",
      "Logits: tensor([-0.3011, -0.3049, -0.3019,  ..., -8.1098, -7.9385, -7.9376],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [2/32], Loss: 0.0643729493021965\n",
      "Logits: tensor([-0.3051, -0.3050, -0.3022,  ..., -8.0053, -7.9928, -7.7728],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [3/32], Loss: 0.06426886469125748\n",
      "Logits: tensor([-0.3042, -0.2982, -0.3024,  ..., -7.9859, -7.7903, -7.4034],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [4/32], Loss: 0.06474613398313522\n",
      "Logits: tensor([-0.3069, -0.3050, -0.3052,  ..., -7.5369, -7.7394, -7.7929],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [5/32], Loss: 0.06419529765844345\n",
      "Logits: tensor([-0.3005, -0.3070, -0.3072,  ..., -7.5522, -7.4936, -7.8716],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [6/32], Loss: 0.06457135081291199\n",
      "Logits: tensor([-0.3023, -0.3023, -0.3033,  ..., -7.7055, -7.8614, -8.2080],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [7/32], Loss: 0.06426618248224258\n",
      "Logits: tensor([-0.3011, -0.3029, -0.2988,  ..., -7.9299, -7.7157, -7.8628],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [8/32], Loss: 0.06416603922843933\n",
      "Logits: tensor([-0.3022, -0.3057, -0.3028,  ..., -7.7382, -7.7942, -7.8370],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [9/32], Loss: 0.06419195234775543\n",
      "Logits: tensor([-0.2974, -0.3017, -0.3032,  ..., -7.7174, -7.7126, -7.4569],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [10/32], Loss: 0.06457582861185074\n",
      "Logits: tensor([-0.2969, -0.3049, -0.3017,  ..., -7.7217, -7.8964, -7.8090],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [11/32], Loss: 0.06412000209093094\n",
      "Logits: tensor([-0.2968, -0.3034, -0.2968,  ..., -7.9698, -8.0966, -7.7731],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [12/32], Loss: 0.06445084512233734\n",
      "Logits: tensor([-0.2978, -0.3024, -0.3037,  ..., -7.9261, -7.9516, -7.7925],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [13/32], Loss: 0.06364602595567703\n",
      "Logits: tensor([-0.3037, -0.2988, -0.2992,  ..., -7.7910, -8.1531, -7.7841],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [14/32], Loss: 0.06369966268539429\n",
      "Logits: tensor([-0.3038, -0.2962, -0.2999,  ..., -7.5727, -7.7004, -7.7228],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [15/32], Loss: 0.06458932906389236\n",
      "Logits: tensor([-0.3019, -0.3010, -0.2978,  ..., -7.7130, -7.5373, -7.9840],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [16/32], Loss: 0.0639415979385376\n",
      "Logits: tensor([-0.3033, -0.2992, -0.2994,  ..., -8.0432, -7.8975, -7.8537],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [17/32], Loss: 0.06461451947689056\n",
      "Logits: tensor([-0.3040, -0.3005, -0.3028,  ..., -7.9253, -7.8755, -7.8923],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [18/32], Loss: 0.06465119868516922\n",
      "Logits: tensor([-0.3038, -0.2937, -0.3084,  ..., -7.4381, -7.7212, -7.8672],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [19/32], Loss: 0.06478343904018402\n",
      "Logits: tensor([-0.2990, -0.3064, -0.2974,  ..., -7.6961, -7.5254, -7.5816],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [20/32], Loss: 0.06421511620283127\n",
      "Logits: tensor([-0.3017, -0.2997, -0.2989,  ..., -7.5053, -7.5421, -8.1507],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [21/32], Loss: 0.06485357135534286\n",
      "Logits: tensor([-0.3027, -0.3011, -0.2945,  ..., -7.9334, -7.8304, -7.9086],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [22/32], Loss: 0.06448134034872055\n",
      "Logits: tensor([-0.2971, -0.3022, -0.2947,  ..., -7.9957, -7.8846, -8.1031],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [23/32], Loss: 0.06467312574386597\n",
      "Logits: tensor([-0.2972, -0.3024, -0.2943,  ..., -7.8692, -7.9259, -7.5725],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [24/32], Loss: 0.06453844904899597\n",
      "Logits: tensor([-0.3043, -0.3000, -0.2983,  ..., -8.1150, -7.8218, -7.8476],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [25/32], Loss: 0.06423406302928925\n",
      "Logits: tensor([-0.2943, -0.3000, -0.3002,  ..., -8.1093, -7.8873, -7.7916],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [26/32], Loss: 0.06446470320224762\n",
      "Logits: tensor([-0.2999, -0.2975, -0.2938,  ..., -7.9245, -7.7094, -7.8314],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [27/32], Loss: 0.06487344205379486\n",
      "Logits: tensor([-0.3016, -0.2996, -0.2945,  ..., -7.7520, -7.7813, -7.5558],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [28/32], Loss: 0.06391435861587524\n",
      "Logits: tensor([-0.2944, -0.3005, -0.2968,  ..., -7.7754, -7.6609, -7.6878],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [29/32], Loss: 0.06464888900518417\n",
      "Logits: tensor([-0.2918, -0.3002, -0.2910,  ..., -7.6044, -7.8064, -7.6427],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [30/32], Loss: 0.06486887484788895\n",
      "Logits: tensor([-0.2995, -0.2975, -0.3019,  ..., -7.7516, -7.6712, -7.5227],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [31/32], Loss: 0.06401718407869339\n",
      "Logits: tensor([-0.2983, -0.2938, -0.2917,  ..., -7.6232, -7.8637, -8.1190],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Batch [32/32], Loss: 0.06533639878034592\n",
      "Logits: tensor([-0.2963, -0.2945, -0.3044,  ..., -7.7780, -7.5745, -8.0025],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [37/50], Loss: 0.06533639878034592\n",
      "Epoch [38/50], Batch [1/32], Loss: 0.06410694122314453\n",
      "Logits: tensor([-0.2985, -0.2983, -0.2932,  ..., -7.9081, -7.8518, -7.4941],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [2/32], Loss: 0.06428621709346771\n",
      "Logits: tensor([-0.2974, -0.3010, -0.2962,  ..., -7.6043, -7.4752, -7.8174],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [3/32], Loss: 0.06457888334989548\n",
      "Logits: tensor([-0.2981, -0.3052, -0.2985,  ..., -7.7232, -7.9291, -7.6169],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [4/32], Loss: 0.06457675248384476\n",
      "Logits: tensor([-0.2946, -0.2998, -0.2921,  ..., -7.6001, -7.9563, -7.5906],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [5/32], Loss: 0.06435029208660126\n",
      "Logits: tensor([-0.2947, -0.3033, -0.2982,  ..., -7.9240, -7.8524, -7.4102],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [6/32], Loss: 0.06445036828517914\n",
      "Logits: tensor([-0.2942, -0.2971, -0.2906,  ..., -7.7985, -7.5562, -7.8486],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [7/32], Loss: 0.06421088427305222\n",
      "Logits: tensor([-0.2943, -0.2965, -0.2945,  ..., -7.6706, -7.8101, -7.9223],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [8/32], Loss: 0.06392417848110199\n",
      "Logits: tensor([-0.2996, -0.2950, -0.2998,  ..., -7.6541, -7.9565, -7.8028],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [9/32], Loss: 0.06442912667989731\n",
      "Logits: tensor([-0.2977, -0.2982, -0.2914,  ..., -7.7137, -7.5336, -8.0083],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [10/32], Loss: 0.06439986824989319\n",
      "Logits: tensor([-0.2922, -0.2947, -0.2941,  ..., -7.5555, -7.8925, -8.1032],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [11/32], Loss: 0.0645938292145729\n",
      "Logits: tensor([-0.2941, -0.2980, -0.2959,  ..., -8.0165, -7.9580, -7.9318],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [12/32], Loss: 0.06397097557783127\n",
      "Logits: tensor([-0.2984, -0.2964, -0.2922,  ..., -7.9352, -7.6269, -7.8795],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [13/32], Loss: 0.06460769474506378\n",
      "Logits: tensor([-0.2882, -0.2987, -0.2991,  ..., -7.5918, -8.0063, -7.5871],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [14/32], Loss: 0.0638837218284607\n",
      "Logits: tensor([-0.2983, -0.2950, -0.2951,  ..., -8.0564, -7.7775, -7.7893],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [15/32], Loss: 0.0635027140378952\n",
      "Logits: tensor([-0.2918, -0.2972, -0.2966,  ..., -7.6221, -7.8433, -7.7220],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [16/32], Loss: 0.06435475498437881\n",
      "Logits: tensor([-0.2970, -0.2925, -0.2994,  ..., -8.0267, -7.6683, -7.9730],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [17/32], Loss: 0.06421668082475662\n",
      "Logits: tensor([-0.2986, -0.2934, -0.2952,  ..., -7.5682, -7.7897, -8.0073],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [18/32], Loss: 0.06468982249498367\n",
      "Logits: tensor([-0.2972, -0.2946, -0.2945,  ..., -7.7560, -7.8121, -7.8551],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [19/32], Loss: 0.06419884413480759\n",
      "Logits: tensor([-0.2982, -0.2927, -0.2943,  ..., -8.0765, -7.7012, -7.7383],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [20/32], Loss: 0.06439891457557678\n",
      "Logits: tensor([-0.2913, -0.2959, -0.2903,  ..., -7.8590, -7.4784, -7.7172],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [21/32], Loss: 0.06450427323579788\n",
      "Logits: tensor([-0.2967, -0.2951, -0.2949,  ..., -7.7506, -7.5470, -7.6023],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [22/32], Loss: 0.064249187707901\n",
      "Logits: tensor([-0.2910, -0.2914, -0.2932,  ..., -7.9032, -8.0221, -7.7775],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [23/32], Loss: 0.06451108306646347\n",
      "Logits: tensor([-0.2936, -0.2918, -0.2942,  ..., -8.0872, -8.0324, -8.0912],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [24/32], Loss: 0.06462907046079636\n",
      "Logits: tensor([-0.2975, -0.2918, -0.2938,  ..., -7.9842, -7.7386, -7.7121],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [25/32], Loss: 0.06453999876976013\n",
      "Logits: tensor([-0.2914, -0.2966, -0.2885,  ..., -7.5644, -7.9932, -7.7202],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [26/32], Loss: 0.06444350630044937\n",
      "Logits: tensor([-0.2977, -0.2942, -0.2934,  ..., -7.8126, -7.9865, -7.4551],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [27/32], Loss: 0.06455936282873154\n",
      "Logits: tensor([-0.2934, -0.2931, -0.2947,  ..., -7.7251, -7.6721, -7.6696],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [28/32], Loss: 0.06452587991952896\n",
      "Logits: tensor([-0.2885, -0.2950, -0.2984,  ..., -7.4898, -7.7377, -7.9629],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [29/32], Loss: 0.06413591653108597\n",
      "Logits: tensor([-0.2887, -0.2902, -0.2917,  ..., -7.8160, -7.7663, -7.9361],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [30/32], Loss: 0.06430163979530334\n",
      "Logits: tensor([-0.2947, -0.2941, -0.2926,  ..., -7.7759, -7.5429, -7.9825],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [31/32], Loss: 0.06440043449401855\n",
      "Logits: tensor([-0.2973, -0.2931, -0.2953,  ..., -7.8133, -7.8226, -7.6908],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Batch [32/32], Loss: 0.0650058165192604\n",
      "Logits: tensor([-0.2908, -0.2863, -0.2907,  ..., -7.9076, -7.7409, -8.0087],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [38/50], Loss: 0.0650058165192604\n",
      "Epoch [39/50], Batch [1/32], Loss: 0.06417431682348251\n",
      "Logits: tensor([-0.2938, -0.2921, -0.2930,  ..., -8.0492, -8.0134, -8.1975],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [2/32], Loss: 0.06469694525003433\n",
      "Logits: tensor([-0.2952, -0.2927, -0.2907,  ..., -7.9059, -8.0279, -7.8127],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [3/32], Loss: 0.06398991495370865\n",
      "Logits: tensor([-0.2913, -0.2970, -0.2940,  ..., -7.8121, -7.5692, -7.8623],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [4/32], Loss: 0.06468919664621353\n",
      "Logits: tensor([-0.2921, -0.2897, -0.2913,  ..., -7.7801, -7.9192, -7.8740],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [5/32], Loss: 0.06386169791221619\n",
      "Logits: tensor([-0.2896, -0.2901, -0.2917,  ..., -7.7076, -7.7238, -7.9510],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [6/32], Loss: 0.06475791335105896\n",
      "Logits: tensor([-0.2923, -0.2905, -0.2861,  ..., -8.0628, -7.9920, -7.7809],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [7/32], Loss: 0.0641898512840271\n",
      "Logits: tensor([-0.2923, -0.2893, -0.2886,  ..., -7.5508, -7.8143, -7.9538],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [8/32], Loss: 0.06443285197019577\n",
      "Logits: tensor([-0.2905, -0.2932, -0.2955,  ..., -7.4704, -7.9087, -7.8538],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [9/32], Loss: 0.06428149342536926\n",
      "Logits: tensor([-0.2899, -0.2925, -0.2864,  ..., -7.8618, -7.5834, -7.9440],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [10/32], Loss: 0.06418709456920624\n",
      "Logits: tensor([-0.2934, -0.2897, -0.2868,  ..., -7.9380, -7.6481, -7.7654],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [11/32], Loss: 0.0646648183465004\n",
      "Logits: tensor([-0.2890, -0.2884, -0.2878,  ..., -7.7763, -7.9507, -7.6654],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [12/32], Loss: 0.06465186923742294\n",
      "Logits: tensor([-0.2918, -0.2945, -0.2924,  ..., -7.5664, -7.4705, -7.8534],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [13/32], Loss: 0.06437799334526062\n",
      "Logits: tensor([-0.2917, -0.2880, -0.2934,  ..., -7.7394, -7.6183, -7.6634],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [14/32], Loss: 0.06453797966241837\n",
      "Logits: tensor([-0.2945, -0.2834, -0.2940,  ..., -7.9803, -7.8149, -7.8854],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [15/32], Loss: 0.064592644572258\n",
      "Logits: tensor([-0.2952, -0.2924, -0.2952,  ..., -7.7158, -7.7901, -7.9317],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [16/32], Loss: 0.06412146985530853\n",
      "Logits: tensor([-0.2925, -0.2876, -0.2856,  ..., -7.9742, -7.8142, -7.9233],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [17/32], Loss: 0.06429626792669296\n",
      "Logits: tensor([-0.2888, -0.2877, -0.2871,  ..., -7.8399, -7.8694, -8.1282],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [18/32], Loss: 0.06407348066568375\n",
      "Logits: tensor([-0.2924, -0.2878, -0.2927,  ..., -7.6841, -7.7786, -7.7196],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [19/32], Loss: 0.06454020738601685\n",
      "Logits: tensor([-0.2839, -0.2897, -0.2885,  ..., -7.8929, -8.0051, -7.7986],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [20/32], Loss: 0.06370680779218674\n",
      "Logits: tensor([-0.2806, -0.2841, -0.2927,  ..., -7.8662, -8.0211, -7.8937],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [21/32], Loss: 0.06372915208339691\n",
      "Logits: tensor([-0.2887, -0.2929, -0.2892,  ..., -8.1473, -7.9797, -7.9193],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [22/32], Loss: 0.06466483324766159\n",
      "Logits: tensor([-0.2967, -0.2894, -0.2911,  ..., -8.0287, -7.7969, -7.5075],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [23/32], Loss: 0.06385280191898346\n",
      "Logits: tensor([-0.2852, -0.2898, -0.2842,  ..., -7.7967, -7.8289, -7.5655],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [24/32], Loss: 0.06420625746250153\n",
      "Logits: tensor([-0.2906, -0.2876, -0.2895,  ..., -7.7836, -7.9762, -7.7320],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [25/32], Loss: 0.06450492143630981\n",
      "Logits: tensor([-0.2902, -0.2873, -0.2841,  ..., -7.6980, -7.5434, -8.0604],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [26/32], Loss: 0.0648202896118164\n",
      "Logits: tensor([-0.2902, -0.2917, -0.2826,  ..., -7.9608, -7.8572, -7.9358],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [27/32], Loss: 0.06436961889266968\n",
      "Logits: tensor([-0.2824, -0.2918, -0.2901,  ..., -8.1617, -7.9482, -8.2684],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [28/32], Loss: 0.06391995400190353\n",
      "Logits: tensor([-0.2863, -0.2897, -0.2872,  ..., -8.1556, -7.8242, -7.6587],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [29/32], Loss: 0.06417056918144226\n",
      "Logits: tensor([-0.2895, -0.2850, -0.2918,  ..., -8.0853, -7.7106, -7.8154],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [30/32], Loss: 0.0643632784485817\n",
      "Logits: tensor([-0.2880, -0.2870, -0.2906,  ..., -7.8259, -7.7402, -7.9886],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [31/32], Loss: 0.06407046318054199\n",
      "Logits: tensor([-0.2857, -0.2854, -0.2924,  ..., -8.0655, -7.9242, -7.6275],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Batch [32/32], Loss: 0.06417030841112137\n",
      "Logits: tensor([-0.2912, -0.2877, -0.2868,  ..., -7.7976, -7.9638, -7.8035],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [39/50], Loss: 0.06417030841112137\n",
      "Epoch [40/50], Batch [1/32], Loss: 0.06425308436155319\n",
      "Logits: tensor([-0.2910, -0.2820, -0.2949,  ..., -7.9772, -8.0297, -7.8745],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [2/32], Loss: 0.06393381208181381\n",
      "Logits: tensor([-0.2836, -0.2870, -0.2848,  ..., -7.8503, -8.1535, -8.0121],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [3/32], Loss: 0.06484317779541016\n",
      "Logits: tensor([-0.2889, -0.2860, -0.2847,  ..., -7.3999, -7.8011, -7.9234],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [4/32], Loss: 0.06440555304288864\n",
      "Logits: tensor([-0.2865, -0.2866, -0.2835,  ..., -8.0052, -7.7046, -7.9621],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [5/32], Loss: 0.06423693895339966\n",
      "Logits: tensor([-0.2875, -0.2852, -0.2829,  ..., -7.8585, -8.1031, -7.5767],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [6/32], Loss: 0.0644332617521286\n",
      "Logits: tensor([-0.2907, -0.2843, -0.2865,  ..., -8.0170, -7.7127, -7.6667],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [7/32], Loss: 0.06425215303897858\n",
      "Logits: tensor([-0.2812, -0.2823, -0.2814,  ..., -8.1098, -8.0547, -8.1138],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [8/32], Loss: 0.06408140063285828\n",
      "Logits: tensor([-0.2900, -0.2865, -0.2856,  ..., -7.9558, -7.8613, -7.9242],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [9/32], Loss: 0.06463812291622162\n",
      "Logits: tensor([-0.2897, -0.2870, -0.2817,  ..., -7.7704, -7.7277, -7.7879],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [10/32], Loss: 0.06439585238695145\n",
      "Logits: tensor([-0.2852, -0.2833, -0.2899,  ..., -7.7551, -7.8605, -7.7831],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [11/32], Loss: 0.0641181692481041\n",
      "Logits: tensor([-0.2890, -0.2839, -0.2857,  ..., -8.0516, -7.8789, -7.9547],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [12/32], Loss: 0.06429417431354523\n",
      "Logits: tensor([-0.2853, -0.2859, -0.2879,  ..., -7.8429, -7.8006, -8.0572],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [13/32], Loss: 0.06421306729316711\n",
      "Logits: tensor([-0.2867, -0.2903, -0.2881,  ..., -7.7735, -7.5854, -7.7904],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [14/32], Loss: 0.06410019099712372\n",
      "Logits: tensor([-0.2900, -0.2836, -0.2733,  ..., -8.0308, -7.9025, -7.7364],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [15/32], Loss: 0.06481225788593292\n",
      "Logits: tensor([-0.2824, -0.2883, -0.2856,  ..., -7.6402, -7.7752, -7.7920],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [16/32], Loss: 0.06408429145812988\n",
      "Logits: tensor([-0.2765, -0.2847, -0.2769,  ..., -7.7620, -8.0859, -7.7497],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [17/32], Loss: 0.06424356997013092\n",
      "Logits: tensor([-0.2756, -0.2826, -0.2862,  ..., -8.1475, -7.9951, -7.9848],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [18/32], Loss: 0.06404595077037811\n",
      "Logits: tensor([-0.2820, -0.2913, -0.2850,  ..., -8.0106, -7.9642, -7.8942],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [19/32], Loss: 0.06399620324373245\n",
      "Logits: tensor([-0.2868, -0.2806, -0.2851,  ..., -8.0435, -8.0044, -7.5031],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [20/32], Loss: 0.0646522268652916\n",
      "Logits: tensor([-0.2829, -0.2806, -0.2838,  ..., -7.9111, -7.7368, -7.5743],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [21/32], Loss: 0.06385394930839539\n",
      "Logits: tensor([-0.2789, -0.2807, -0.2850,  ..., -8.2580, -8.0895, -8.0269],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [22/32], Loss: 0.0641026496887207\n",
      "Logits: tensor([-0.2805, -0.2867, -0.2883,  ..., -7.7550, -7.9059, -7.7542],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [23/32], Loss: 0.06423567980527878\n",
      "Logits: tensor([-0.2835, -0.2732, -0.2861,  ..., -7.8671, -7.8839, -7.9807],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [24/32], Loss: 0.06411521136760712\n",
      "Logits: tensor([-0.2838, -0.2775, -0.2854,  ..., -7.8743, -7.7103, -7.5598],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [25/32], Loss: 0.06415379792451859\n",
      "Logits: tensor([-0.2809, -0.2824, -0.2833,  ..., -7.8687, -7.8109, -7.7648],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [26/32], Loss: 0.06424649059772491\n",
      "Logits: tensor([-0.2839, -0.2862, -0.2830,  ..., -7.7668, -7.8460, -7.8492],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [27/32], Loss: 0.06447109580039978\n",
      "Logits: tensor([-0.2838, -0.2892, -0.2742,  ..., -7.6049, -7.7808, -8.0313],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [28/32], Loss: 0.06385695189237595\n",
      "Logits: tensor([-0.2755, -0.2850, -0.2870,  ..., -8.1084, -7.8820, -7.8258],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [29/32], Loss: 0.06477133184671402\n",
      "Logits: tensor([-0.2792, -0.2845, -0.2880,  ..., -7.4383, -7.7268, -7.8391],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [30/32], Loss: 0.06435605138540268\n",
      "Logits: tensor([-0.2810, -0.2914, -0.2849,  ..., -7.6841, -7.5234, -8.0077],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [31/32], Loss: 0.06417255103588104\n",
      "Logits: tensor([-0.2749, -0.2808, -0.2772,  ..., -7.8681, -7.5607, -7.6954],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Batch [32/32], Loss: 0.0637611672282219\n",
      "Logits: tensor([-0.2867, -0.2807, -0.2870,  ..., -7.6339, -7.8762, -7.8723],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [40/50], Loss: 0.0637611672282219\n",
      "Epoch [41/50], Batch [1/32], Loss: 0.06405056267976761\n",
      "Logits: tensor([-0.2864, -0.2771, -0.2796,  ..., -7.9831, -8.0874, -7.8483],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [2/32], Loss: 0.06407559663057327\n",
      "Logits: tensor([-0.2827, -0.2811, -0.2827,  ..., -7.8195, -7.9863, -7.8254],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [3/32], Loss: 0.06444846093654633\n",
      "Logits: tensor([-0.2769, -0.2832, -0.2839,  ..., -7.7200, -7.8821, -7.7579],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [4/32], Loss: 0.06338304281234741\n",
      "Logits: tensor([-0.2808, -0.2801, -0.2841,  ..., -7.8143, -7.8362, -7.8532],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [5/32], Loss: 0.06407330930233002\n",
      "Logits: tensor([-0.2771, -0.2786, -0.2801,  ..., -7.9549, -8.0262, -7.8883],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [6/32], Loss: 0.06437321752309799\n",
      "Logits: tensor([-0.2855, -0.2741, -0.2783,  ..., -7.6621, -7.6891, -7.9096],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [7/32], Loss: 0.06408634036779404\n",
      "Logits: tensor([-0.2850, -0.2722, -0.2835,  ..., -7.9047, -8.0279, -8.0341],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [8/32], Loss: 0.06413474678993225\n",
      "Logits: tensor([-0.2860, -0.2835, -0.2864,  ..., -7.5325, -7.9173, -7.8794],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [9/32], Loss: 0.06430251896381378\n",
      "Logits: tensor([-0.2802, -0.2799, -0.2808,  ..., -7.8846, -7.7386, -7.3397],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [10/32], Loss: 0.0643424540758133\n",
      "Logits: tensor([-0.2793, -0.2803, -0.2845,  ..., -8.0827, -8.0610, -7.5673],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [11/32], Loss: 0.06486789882183075\n",
      "Logits: tensor([-0.2756, -0.2836, -0.2796,  ..., -8.0317, -7.9583, -7.9458],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [12/32], Loss: 0.06432238221168518\n",
      "Logits: tensor([-0.2855, -0.2785, -0.2869,  ..., -7.8894, -7.9525, -7.8291],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [13/32], Loss: 0.06448078155517578\n",
      "Logits: tensor([-0.2779, -0.2782, -0.2745,  ..., -7.9634, -7.9450, -7.8800],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [14/32], Loss: 0.06442537158727646\n",
      "Logits: tensor([-0.2830, -0.2799, -0.2781,  ..., -7.6613, -8.1787, -7.7613],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [15/32], Loss: 0.06490759551525116\n",
      "Logits: tensor([-0.2779, -0.2774, -0.2847,  ..., -7.6461, -8.0645, -7.6413],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [16/32], Loss: 0.06403970718383789\n",
      "Logits: tensor([-0.2747, -0.2839, -0.2808,  ..., -7.7622, -8.0594, -7.8540],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [17/32], Loss: 0.06388438493013382\n",
      "Logits: tensor([-0.2846, -0.2798, -0.2820,  ..., -8.1741, -8.2706, -7.6305],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [18/32], Loss: 0.06400976330041885\n",
      "Logits: tensor([-0.2722, -0.2818, -0.2838,  ..., -8.0365, -7.7346, -7.9934],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [19/32], Loss: 0.06424728035926819\n",
      "Logits: tensor([-0.2794, -0.2851, -0.2723,  ..., -7.9781, -8.0625, -7.8644],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [20/32], Loss: 0.06429978460073471\n",
      "Logits: tensor([-0.2769, -0.2777, -0.2820,  ..., -7.6650, -7.8005, -7.8174],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [21/32], Loss: 0.06400593370199203\n",
      "Logits: tensor([-0.2853, -0.2832, -0.2804,  ..., -7.6778, -7.8713, -7.8328],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [22/32], Loss: 0.06469196081161499\n",
      "Logits: tensor([-0.2754, -0.2786, -0.2771,  ..., -8.0365, -7.9900, -7.9198],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [23/32], Loss: 0.06415852904319763\n",
      "Logits: tensor([-0.2828, -0.2737, -0.2866,  ..., -7.8015, -7.6126, -7.8185],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [24/32], Loss: 0.06419084221124649\n",
      "Logits: tensor([-0.2715, -0.2781, -0.2765,  ..., -8.0233, -7.8770, -7.4830],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [25/32], Loss: 0.06455323845148087\n",
      "Logits: tensor([-0.2785, -0.2726, -0.2786,  ..., -8.1091, -8.0103, -7.9755],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [26/32], Loss: 0.06410156190395355\n",
      "Logits: tensor([-0.2820, -0.2831, -0.2756,  ..., -7.9446, -8.0425, -7.7870],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [27/32], Loss: 0.06434405595064163\n",
      "Logits: tensor([-0.2811, -0.2782, -0.2714,  ..., -7.9420, -7.9191, -7.9932],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [28/32], Loss: 0.06423933058977127\n",
      "Logits: tensor([-0.2827, -0.2853, -0.2862,  ..., -8.1525, -7.9828, -7.9217],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [29/32], Loss: 0.06405584514141083\n",
      "Logits: tensor([-0.2849, -0.2818, -0.2785,  ..., -7.8165, -7.9254, -7.5864],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [30/32], Loss: 0.0643720030784607\n",
      "Logits: tensor([-0.2802, -0.2794, -0.2775,  ..., -7.9536, -7.6683, -7.4169],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [31/32], Loss: 0.06369518488645554\n",
      "Logits: tensor([-0.2831, -0.2762, -0.2799,  ..., -8.0488, -7.9367, -7.7447],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Batch [32/32], Loss: 0.06379002332687378\n",
      "Logits: tensor([-0.2789, -0.2761, -0.2764,  ..., -8.0938, -8.0372, -7.7208],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [41/50], Loss: 0.06379002332687378\n",
      "Epoch [42/50], Batch [1/32], Loss: 0.06436499208211899\n",
      "Logits: tensor([-0.2767, -0.2778, -0.2851,  ..., -8.0676, -7.8240, -8.0706],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [2/32], Loss: 0.06385054439306259\n",
      "Logits: tensor([-0.2793, -0.2736, -0.2752,  ..., -7.9889, -7.9174, -7.6176],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [3/32], Loss: 0.06431835889816284\n",
      "Logits: tensor([-0.2775, -0.2703, -0.2768,  ..., -7.9848, -7.8207, -7.6760],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [4/32], Loss: 0.06418503820896149\n",
      "Logits: tensor([-0.2742, -0.2744, -0.2752,  ..., -7.9047, -7.5372, -7.9475],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [5/32], Loss: 0.06398015469312668\n",
      "Logits: tensor([-0.2777, -0.2812, -0.2761,  ..., -7.8307, -7.7658, -7.4908],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [6/32], Loss: 0.06441500037908554\n",
      "Logits: tensor([-0.2788, -0.2762, -0.2706,  ..., -7.8578, -7.6710, -7.9552],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [7/32], Loss: 0.06455006450414658\n",
      "Logits: tensor([-0.2686, -0.2765, -0.2742,  ..., -8.0590, -7.9352, -8.0787],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [8/32], Loss: 0.06409715116024017\n",
      "Logits: tensor([-0.2810, -0.2695, -0.2741,  ..., -8.1504, -8.0087, -7.9221],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [9/32], Loss: 0.06403850764036179\n",
      "Logits: tensor([-0.2800, -0.2799, -0.2831,  ..., -7.7465, -8.0576, -8.1163],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [10/32], Loss: 0.0640161857008934\n",
      "Logits: tensor([-0.2802, -0.2742, -0.2805,  ..., -7.9993, -7.9268, -7.4791],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [11/32], Loss: 0.06413298100233078\n",
      "Logits: tensor([-0.2741, -0.2695, -0.2758,  ..., -7.8801, -7.9251, -7.7683],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [12/32], Loss: 0.06412966549396515\n",
      "Logits: tensor([-0.2783, -0.2731, -0.2738,  ..., -7.9599, -7.9542, -7.5941],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [13/32], Loss: 0.0640060156583786\n",
      "Logits: tensor([-0.2783, -0.2737, -0.2773,  ..., -7.8907, -7.6719, -7.7805],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [14/32], Loss: 0.06501597911119461\n",
      "Logits: tensor([-0.2742, -0.2735, -0.2740,  ..., -8.1949, -7.9372, -7.8959],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [15/32], Loss: 0.0647284984588623\n",
      "Logits: tensor([-0.2740, -0.2783, -0.2760,  ..., -7.9601, -7.6473, -7.5910],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [16/32], Loss: 0.06411097198724747\n",
      "Logits: tensor([-0.2694, -0.2754, -0.2741,  ..., -8.1689, -8.0658, -7.8763],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [17/32], Loss: 0.06412020325660706\n",
      "Logits: tensor([-0.2777, -0.2731, -0.2780,  ..., -7.7402, -7.8239, -7.8754],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [18/32], Loss: 0.06448914855718613\n",
      "Logits: tensor([-0.2756, -0.2691, -0.2783,  ..., -7.7858, -8.0848, -7.6477],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [19/32], Loss: 0.06394834816455841\n",
      "Logits: tensor([-0.2802, -0.2711, -0.2745,  ..., -7.8337, -7.7164, -7.4434],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [20/32], Loss: 0.06403123587369919\n",
      "Logits: tensor([-0.2779, -0.2722, -0.2716,  ..., -7.9626, -7.7067, -8.0087],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [21/32], Loss: 0.06411603838205338\n",
      "Logits: tensor([-0.2759, -0.2732, -0.2725,  ..., -7.6574, -7.7868, -7.8102],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [22/32], Loss: 0.0641443058848381\n",
      "Logits: tensor([-0.2733, -0.2661, -0.2746,  ..., -7.8586, -7.8330, -7.8870],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [23/32], Loss: 0.06415190547704697\n",
      "Logits: tensor([-0.2778, -0.2701, -0.2733,  ..., -7.9075, -7.9244, -8.0218],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [24/32], Loss: 0.06433239579200745\n",
      "Logits: tensor([-0.2653, -0.2735, -0.2656,  ..., -7.5965, -7.1762, -7.9678],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [25/32], Loss: 0.06342389434576035\n",
      "Logits: tensor([-0.2712, -0.2738, -0.2687,  ..., -7.6991, -8.0287, -7.8547],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [26/32], Loss: 0.0643458440899849\n",
      "Logits: tensor([-0.2678, -0.2702, -0.2767,  ..., -8.0690, -7.9049, -7.7546],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [27/32], Loss: 0.06411638110876083\n",
      "Logits: tensor([-0.2634, -0.2779, -0.2720,  ..., -8.0470, -7.8964, -7.4139],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [28/32], Loss: 0.06429728120565414\n",
      "Logits: tensor([-0.2759, -0.2776, -0.2764,  ..., -7.9255, -7.7822, -7.9074],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [29/32], Loss: 0.06390932202339172\n",
      "Logits: tensor([-0.2728, -0.2664, -0.2760,  ..., -7.6240, -7.8220, -7.5274],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [30/32], Loss: 0.0639791488647461\n",
      "Logits: tensor([-0.2715, -0.2721, -0.2780,  ..., -7.7985, -7.8852, -7.8587],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [31/32], Loss: 0.06451644003391266\n",
      "Logits: tensor([-0.2706, -0.2708, -0.2736,  ..., -7.8684, -7.5666, -7.9658],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Batch [32/32], Loss: 0.06425729393959045\n",
      "Logits: tensor([-0.2702, -0.2705, -0.2767,  ..., -7.7781, -8.1945, -7.7135],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [42/50], Loss: 0.06425729393959045\n",
      "Epoch [43/50], Batch [1/32], Loss: 0.06459684669971466\n",
      "Logits: tensor([-0.2626, -0.2770, -0.2711,  ..., -7.6422, -7.9168, -7.6987],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [2/32], Loss: 0.06428337097167969\n",
      "Logits: tensor([-0.2750, -0.2746, -0.2700,  ..., -6.8982, -7.7427, -8.1252],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [3/32], Loss: 0.06462462246417999\n",
      "Logits: tensor([-0.2690, -0.2765, -0.2741,  ..., -8.1081, -8.0797, -7.8747],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [4/32], Loss: 0.06467904150485992\n",
      "Logits: tensor([-0.2695, -0.2695, -0.2759,  ..., -7.7970, -7.7597, -7.9023],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [5/32], Loss: 0.06433726102113724\n",
      "Logits: tensor([-0.2722, -0.2703, -0.2660,  ..., -8.0779, -7.8402, -7.7719],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [6/32], Loss: 0.06384894996881485\n",
      "Logits: tensor([-0.2630, -0.2712, -0.2633,  ..., -8.1147, -8.0207, -7.5786],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [7/32], Loss: 0.06429637223482132\n",
      "Logits: tensor([-0.2669, -0.2709, -0.2724,  ..., -7.8753, -7.6878, -7.9729],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [8/32], Loss: 0.0637112557888031\n",
      "Logits: tensor([-0.2746, -0.2765, -0.2705,  ..., -7.7364, -7.7803, -7.8462],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [9/32], Loss: 0.06419704109430313\n",
      "Logits: tensor([-0.2657, -0.2667, -0.2658,  ..., -7.6300, -7.8790, -7.8226],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [10/32], Loss: 0.06427459418773651\n",
      "Logits: tensor([-0.2752, -0.2692, -0.2702,  ..., -7.7051, -7.8996, -7.8608],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [11/32], Loss: 0.06391844153404236\n",
      "Logits: tensor([-0.2659, -0.2717, -0.2717,  ..., -8.0130, -7.7888, -8.0807],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [12/32], Loss: 0.0637388825416565\n",
      "Logits: tensor([-0.2724, -0.2693, -0.2679,  ..., -7.9958, -7.8650, -8.1779],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [13/32], Loss: 0.0637650340795517\n",
      "Logits: tensor([-0.2685, -0.2765, -0.2702,  ..., -7.7614, -7.9245, -7.7996],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [14/32], Loss: 0.06437477469444275\n",
      "Logits: tensor([-0.2754, -0.2731, -0.2730,  ..., -7.6109, -7.6367, -7.8619],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [15/32], Loss: 0.06419574469327927\n",
      "Logits: tensor([-0.2732, -0.2685, -0.2685,  ..., -7.9497, -8.0382, -7.7387],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [16/32], Loss: 0.06440269201993942\n",
      "Logits: tensor([-0.2718, -0.2736, -0.2704,  ..., -8.1158, -7.8204, -7.8103],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [17/32], Loss: 0.06393875181674957\n",
      "Logits: tensor([-0.2668, -0.2706, -0.2722,  ..., -7.8987, -7.8482, -8.0203],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [18/32], Loss: 0.06442733108997345\n",
      "Logits: tensor([-0.2700, -0.2640, -0.2684,  ..., -7.7300, -7.8067, -7.7717],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [19/32], Loss: 0.06452836096286774\n",
      "Logits: tensor([-0.2705, -0.2642, -0.2686,  ..., -8.0650, -8.0112, -7.6595],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [20/32], Loss: 0.063835009932518\n",
      "Logits: tensor([-0.2671, -0.2697, -0.2707,  ..., -7.5791, -7.9814, -7.6747],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [21/32], Loss: 0.06398552656173706\n",
      "Logits: tensor([-0.2672, -0.2626, -0.2689,  ..., -7.8341, -7.7910, -7.8517],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [22/32], Loss: 0.06420272588729858\n",
      "Logits: tensor([-0.2723, -0.2604, -0.2710,  ..., -7.6236, -7.3277, -7.9051],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [23/32], Loss: 0.06412383168935776\n",
      "Logits: tensor([-0.2678, -0.2704, -0.2728,  ..., -8.2713, -7.7355, -7.7941],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [24/32], Loss: 0.06380438804626465\n",
      "Logits: tensor([-0.2646, -0.2677, -0.2663,  ..., -8.0977, -8.0557, -7.3932],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [25/32], Loss: 0.06401657313108444\n",
      "Logits: tensor([-0.2711, -0.2647, -0.2619,  ..., -7.7861, -7.7120, -8.3513],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [26/32], Loss: 0.0642063245177269\n",
      "Logits: tensor([-0.2730, -0.2625, -0.2709,  ..., -8.1371, -7.9422, -7.9278],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [27/32], Loss: 0.06368545442819595\n",
      "Logits: tensor([-0.2699, -0.2686, -0.2675,  ..., -7.9537, -7.9492, -7.9281],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [28/32], Loss: 0.06466010212898254\n",
      "Logits: tensor([-0.2716, -0.2744, -0.2676,  ..., -7.9171, -7.9630, -7.8367],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [29/32], Loss: 0.06389822065830231\n",
      "Logits: tensor([-0.2634, -0.2646, -0.2681,  ..., -7.8049, -7.9811, -7.9663],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [30/32], Loss: 0.06383560597896576\n",
      "Logits: tensor([-0.2687, -0.2694, -0.2624,  ..., -7.9617, -7.9142, -7.8651],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [31/32], Loss: 0.06447675079107285\n",
      "Logits: tensor([-0.2664, -0.2634, -0.2646,  ..., -7.9454, -8.0035, -7.7692],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Batch [32/32], Loss: 0.06355566531419754\n",
      "Logits: tensor([-0.2562, -0.2648, -0.2642,  ..., -7.7635, -7.8278, -7.8870],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [43/50], Loss: 0.06355566531419754\n",
      "Epoch [44/50], Batch [1/32], Loss: 0.063780277967453\n",
      "Logits: tensor([-0.2621, -0.2630, -0.2622,  ..., -7.9027, -7.9812, -7.9439],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [2/32], Loss: 0.06430705636739731\n",
      "Logits: tensor([-0.2651, -0.2650, -0.2652,  ..., -8.1996, -7.8052, -7.8990],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [3/32], Loss: 0.06407027691602707\n",
      "Logits: tensor([-0.2621, -0.2691, -0.2696,  ..., -7.9908, -8.2384, -7.8578],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [4/32], Loss: 0.06410284340381622\n",
      "Logits: tensor([-0.2668, -0.2636, -0.2693,  ..., -7.6020, -8.1613, -8.0113],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [5/32], Loss: 0.0641390010714531\n",
      "Logits: tensor([-0.2668, -0.2672, -0.2587,  ..., -7.7680, -8.1111, -7.8672],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [6/32], Loss: 0.06440185755491257\n",
      "Logits: tensor([-0.2762, -0.2684, -0.2632,  ..., -7.6988, -7.8764, -7.9339],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [7/32], Loss: 0.06415876001119614\n",
      "Logits: tensor([-0.2626, -0.2678, -0.2672,  ..., -8.0721, -7.3999, -7.8398],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [8/32], Loss: 0.06447555869817734\n",
      "Logits: tensor([-0.2649, -0.2660, -0.2738,  ..., -7.9774, -8.0282, -7.6325],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [9/32], Loss: 0.06457599997520447\n",
      "Logits: tensor([-0.2681, -0.2652, -0.2632,  ..., -8.1011, -8.1889, -7.4237],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [10/32], Loss: 0.0642094612121582\n",
      "Logits: tensor([-0.2638, -0.2631, -0.2672,  ..., -8.0825, -8.0286, -7.6760],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [11/32], Loss: 0.06458000838756561\n",
      "Logits: tensor([-0.2630, -0.2639, -0.2633,  ..., -7.8244, -7.9132, -7.9742],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [12/32], Loss: 0.06420722603797913\n",
      "Logits: tensor([-0.2676, -0.2693, -0.2655,  ..., -6.9191, -7.7676, -8.1517],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [13/32], Loss: 0.06482189893722534\n",
      "Logits: tensor([-0.2649, -0.2649, -0.2705,  ..., -8.1188, -8.0005, -7.9742],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [14/32], Loss: 0.06404973566532135\n",
      "Logits: tensor([-0.2632, -0.2612, -0.2654,  ..., -7.7871, -7.6302, -8.1552],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [15/32], Loss: 0.0639985129237175\n",
      "Logits: tensor([-0.2642, -0.2596, -0.2684,  ..., -7.9360, -8.2434, -8.1004],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [16/32], Loss: 0.06398852169513702\n",
      "Logits: tensor([-0.2666, -0.2595, -0.2613,  ..., -8.0675, -8.0065, -7.9300],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [17/32], Loss: 0.06417330354452133\n",
      "Logits: tensor([-0.2629, -0.2613, -0.2670,  ..., -7.7293, -7.7823, -7.8736],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [18/32], Loss: 0.06398467719554901\n",
      "Logits: tensor([-0.2625, -0.2706, -0.2613,  ..., -7.8190, -8.0231, -7.7583],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [19/32], Loss: 0.06409236043691635\n",
      "Logits: tensor([-0.2614, -0.2673, -0.2647,  ..., -7.8705, -8.3297, -7.7332],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [20/32], Loss: 0.06412564218044281\n",
      "Logits: tensor([-0.2618, -0.2656, -0.2635,  ..., -8.1036, -7.7949, -7.7484],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [21/32], Loss: 0.06376757472753525\n",
      "Logits: tensor([-0.2614, -0.2680, -0.2683,  ..., -7.8854, -8.0542, -7.8915],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [22/32], Loss: 0.06382662057876587\n",
      "Logits: tensor([-0.2593, -0.2642, -0.2620,  ..., -7.8607, -8.1584, -7.8585],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [23/32], Loss: 0.06382081657648087\n",
      "Logits: tensor([-0.2644, -0.2600, -0.2622,  ..., -8.0172, -7.5978, -7.7434],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [24/32], Loss: 0.06430666148662567\n",
      "Logits: tensor([-0.2669, -0.2676, -0.2627,  ..., -7.7492, -8.0458, -8.2327],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [25/32], Loss: 0.06355644017457962\n",
      "Logits: tensor([-0.2662, -0.2643, -0.2637,  ..., -8.0547, -8.2846, -7.8706],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [26/32], Loss: 0.06443920731544495\n",
      "Logits: tensor([-0.2620, -0.2657, -0.2607,  ..., -7.7756, -7.8699, -7.9724],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [27/32], Loss: 0.06399428099393845\n",
      "Logits: tensor([-0.2664, -0.2589, -0.2572,  ..., -7.8711, -7.7890, -7.6370],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [28/32], Loss: 0.06346876919269562\n",
      "Logits: tensor([-0.2602, -0.2621, -0.2602,  ..., -7.9496, -8.0713, -8.0451],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [29/32], Loss: 0.06381083279848099\n",
      "Logits: tensor([-0.2605, -0.2608, -0.2636,  ..., -8.1506, -8.0933, -7.7742],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [30/32], Loss: 0.06384973973035812\n",
      "Logits: tensor([-0.2603, -0.2609, -0.2646,  ..., -7.7927, -7.8143, -7.8237],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [31/32], Loss: 0.06451834738254547\n",
      "Logits: tensor([-0.2633, -0.2668, -0.2617,  ..., -7.8686, -7.5645, -8.0139],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Batch [32/32], Loss: 0.0640527755022049\n",
      "Logits: tensor([-0.2602, -0.2656, -0.2634,  ..., -7.8231, -7.9353, -8.0032],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [44/50], Loss: 0.0640527755022049\n",
      "Epoch [45/50], Batch [1/32], Loss: 0.06387890130281448\n",
      "Logits: tensor([-0.2670, -0.2603, -0.2656,  ..., -8.1734, -7.8437, -7.7740],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [2/32], Loss: 0.06428537517786026\n",
      "Logits: tensor([-0.2641, -0.2609, -0.2596,  ..., -8.1196, -7.8424, -7.8598],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [3/32], Loss: 0.06399737298488617\n",
      "Logits: tensor([-0.2603, -0.2587, -0.2644,  ..., -7.8000, -7.9078, -7.6140],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [4/32], Loss: 0.06454407423734665\n",
      "Logits: tensor([-0.2561, -0.2604, -0.2632,  ..., -7.6642, -8.1033, -7.8224],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [5/32], Loss: 0.06359605491161346\n",
      "Logits: tensor([-0.2606, -0.2624, -0.2640,  ..., -7.7737, -7.9610, -8.2869],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [6/32], Loss: 0.06427406519651413\n",
      "Logits: tensor([-0.2639, -0.2641, -0.2635,  ..., -8.3630, -8.0702, -8.3805],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [7/32], Loss: 0.06452490389347076\n",
      "Logits: tensor([-0.2647, -0.2658, -0.2615,  ..., -8.3011, -8.0880, -7.7938],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [8/32], Loss: 0.0642690360546112\n",
      "Logits: tensor([-0.2619, -0.2620, -0.2552,  ..., -7.8114, -8.0470, -7.8362],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [9/32], Loss: 0.06395028531551361\n",
      "Logits: tensor([-0.2556, -0.2639, -0.2558,  ..., -8.0319, -8.1745, -8.2357],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [10/32], Loss: 0.06398577988147736\n",
      "Logits: tensor([-0.2588, -0.2619, -0.2574,  ..., -7.8774, -8.1758, -7.8753],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [11/32], Loss: 0.06394387036561966\n",
      "Logits: tensor([-0.2636, -0.2597, -0.2600,  ..., -8.0845, -7.9292, -8.1047],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [12/32], Loss: 0.06400441378355026\n",
      "Logits: tensor([-0.2609, -0.2611, -0.2621,  ..., -7.8943, -8.2759, -7.7719],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [13/32], Loss: 0.0639154314994812\n",
      "Logits: tensor([-0.2583, -0.2522, -0.2625,  ..., -7.8429, -7.8638, -7.6541],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [14/32], Loss: 0.06348168104887009\n",
      "Logits: tensor([-0.2552, -0.2603, -0.2633,  ..., -8.0899, -8.3640, -7.5790],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [15/32], Loss: 0.06431625783443451\n",
      "Logits: tensor([-0.2587, -0.2593, -0.2644,  ..., -7.8380, -8.1435, -8.1040],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [16/32], Loss: 0.06452499330043793\n",
      "Logits: tensor([-0.2637, -0.2545, -0.2673,  ..., -7.6845, -8.1236, -7.8444],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [17/32], Loss: 0.06394030153751373\n",
      "Logits: tensor([-0.2645, -0.2580, -0.2477,  ..., -7.8232, -7.8715, -7.8550],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [18/32], Loss: 0.06457134336233139\n",
      "Logits: tensor([-0.2567, -0.2633, -0.2541,  ..., -8.2872, -7.7056, -7.9938],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [19/32], Loss: 0.06401050090789795\n",
      "Logits: tensor([-0.2593, -0.2598, -0.2586,  ..., -8.1513, -7.9151, -7.6198],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [20/32], Loss: 0.06381845474243164\n",
      "Logits: tensor([-0.2565, -0.2604, -0.2585,  ..., -7.9011, -8.0391, -7.8480],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [21/32], Loss: 0.06394925713539124\n",
      "Logits: tensor([-0.2586, -0.2615, -0.2530,  ..., -7.8256, -7.7509, -8.3945],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [22/32], Loss: 0.0639476478099823\n",
      "Logits: tensor([-0.2627, -0.2599, -0.2574,  ..., -8.1384, -8.0138, -8.1410],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [23/32], Loss: 0.06407418102025986\n",
      "Logits: tensor([-0.2633, -0.2621, -0.2601,  ..., -7.7824, -7.8730, -8.0208],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [24/32], Loss: 0.0641355887055397\n",
      "Logits: tensor([-0.2565, -0.2575, -0.2623,  ..., -7.8840, -7.8638, -7.9863],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [25/32], Loss: 0.06421616673469543\n",
      "Logits: tensor([-0.2563, -0.2581, -0.2530,  ..., -7.8588, -8.1183, -7.7678],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [26/32], Loss: 0.06348980218172073\n",
      "Logits: tensor([-0.2534, -0.2580, -0.2621,  ..., -8.0530, -8.1432, -7.9045],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [27/32], Loss: 0.06394033879041672\n",
      "Logits: tensor([-0.2604, -0.2655, -0.2562,  ..., -7.8931, -7.8050, -7.6004],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [28/32], Loss: 0.06417938321828842\n",
      "Logits: tensor([-0.2544, -0.2571, -0.2596,  ..., -8.1722, -7.8053, -8.1174],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [29/32], Loss: 0.0643245279788971\n",
      "Logits: tensor([-0.2559, -0.2558, -0.2560,  ..., -7.6970, -8.2132, -7.7434],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [30/32], Loss: 0.0639839842915535\n",
      "Logits: tensor([-0.2614, -0.2636, -0.2599,  ..., -8.2037, -8.0543, -8.0090],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [31/32], Loss: 0.06432328373193741\n",
      "Logits: tensor([-0.2611, -0.2480, -0.2596,  ..., -7.8132, -8.0840, -8.0127],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Batch [32/32], Loss: 0.06432190537452698\n",
      "Logits: tensor([-0.2570, -0.2610, -0.2617,  ..., -7.8051, -7.9625, -7.8863],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [45/50], Loss: 0.06432190537452698\n",
      "Epoch [46/50], Batch [1/32], Loss: 0.06423885375261307\n",
      "Logits: tensor([-0.2598, -0.2548, -0.2527,  ..., -8.1703, -7.7204, -7.8242],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [2/32], Loss: 0.0643114522099495\n",
      "Logits: tensor([-0.2591, -0.2589, -0.2591,  ..., -8.0996, -7.6994, -7.8641],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [3/32], Loss: 0.06404189020395279\n",
      "Logits: tensor([-0.2616, -0.2531, -0.2617,  ..., -7.8573, -7.7970, -8.1235],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [4/32], Loss: 0.06383334845304489\n",
      "Logits: tensor([-0.2522, -0.2575, -0.2560,  ..., -8.1057, -7.9824, -8.1528],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [5/32], Loss: 0.0640917420387268\n",
      "Logits: tensor([-0.2554, -0.2539, -0.2593,  ..., -8.1264, -8.1327, -7.7852],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [6/32], Loss: 0.06381531059741974\n",
      "Logits: tensor([-0.2569, -0.2553, -0.2548,  ..., -7.8825, -7.9909, -8.0282],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [7/32], Loss: 0.0641506165266037\n",
      "Logits: tensor([-0.2597, -0.2558, -0.2544,  ..., -8.0352, -7.7763, -8.0819],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [8/32], Loss: 0.06422391533851624\n",
      "Logits: tensor([-0.2553, -0.2511, -0.2580,  ..., -7.9095, -7.8879, -7.9544],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [9/32], Loss: 0.06417008489370346\n",
      "Logits: tensor([-0.2567, -0.2516, -0.2514,  ..., -8.0086, -7.9901, -7.4745],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [10/32], Loss: 0.06404923647642136\n",
      "Logits: tensor([-0.2560, -0.2553, -0.2624,  ..., -7.9669, -7.7452, -7.8554],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [11/32], Loss: 0.06414135545492172\n",
      "Logits: tensor([-0.2521, -0.2516, -0.2552,  ..., -8.1373, -8.0049, -7.8902],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [12/32], Loss: 0.06407643854618073\n",
      "Logits: tensor([-0.2544, -0.2522, -0.2498,  ..., -8.0566, -7.9814, -7.7751],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [13/32], Loss: 0.06440761685371399\n",
      "Logits: tensor([-0.2533, -0.2527, -0.2565,  ..., -7.7542, -7.9609, -7.7934],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [14/32], Loss: 0.06342541426420212\n",
      "Logits: tensor([-0.2555, -0.2567, -0.2552,  ..., -8.1459, -7.8463, -7.9092],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [15/32], Loss: 0.06441816687583923\n",
      "Logits: tensor([-0.2486, -0.2478, -0.2557,  ..., -8.2052, -8.1037, -7.9954],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [16/32], Loss: 0.06450878083705902\n",
      "Logits: tensor([-0.2582, -0.2507, -0.2489,  ..., -7.9280, -7.8928, -8.0165],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [17/32], Loss: 0.06398965418338776\n",
      "Logits: tensor([-0.2545, -0.2534, -0.2603,  ..., -8.0523, -8.0797, -8.3052],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [18/32], Loss: 0.06427764147520065\n",
      "Logits: tensor([-0.2488, -0.2568, -0.2548,  ..., -8.0506, -8.2852, -8.0329],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [19/32], Loss: 0.06403886526823044\n",
      "Logits: tensor([-0.2552, -0.2484, -0.2532,  ..., -7.8692, -7.7798, -7.9839],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [20/32], Loss: 0.06423421949148178\n",
      "Logits: tensor([-0.2524, -0.2472, -0.2529,  ..., -8.1188, -7.9590, -7.9052],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [21/32], Loss: 0.06390925496816635\n",
      "Logits: tensor([-0.2508, -0.2506, -0.2547,  ..., -8.0302, -7.8527, -7.6864],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [22/32], Loss: 0.06335799396038055\n",
      "Logits: tensor([-0.2554, -0.2539, -0.2545,  ..., -8.1547, -8.0297, -8.1573],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [23/32], Loss: 0.06423760205507278\n",
      "Logits: tensor([-0.2560, -0.2543, -0.2476,  ..., -8.3525, -8.0294, -8.2546],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [24/32], Loss: 0.06398477405309677\n",
      "Logits: tensor([-0.2516, -0.2523, -0.2630,  ..., -8.2956, -7.9573, -7.7880],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [25/32], Loss: 0.06410089135169983\n",
      "Logits: tensor([-0.2550, -0.2520, -0.2539,  ..., -8.0099, -7.6203, -7.8656],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [26/32], Loss: 0.06386793404817581\n",
      "Logits: tensor([-0.2494, -0.2536, -0.2510,  ..., -8.2008, -8.0519, -7.8761],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [27/32], Loss: 0.06428713351488113\n",
      "Logits: tensor([-0.2555, -0.2492, -0.2508,  ..., -8.0860, -7.7434, -8.1415],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [28/32], Loss: 0.06374877691268921\n",
      "Logits: tensor([-0.2521, -0.2556, -0.2509,  ..., -7.8318, -7.9372, -7.9438],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [29/32], Loss: 0.06312043964862823\n",
      "Logits: tensor([-0.2544, -0.2514, -0.2514,  ..., -7.8288, -7.9216, -7.9891],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [30/32], Loss: 0.06392203271389008\n",
      "Logits: tensor([-0.2464, -0.2564, -0.2535,  ..., -8.1856, -7.8871, -7.8768],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [31/32], Loss: 0.0642676055431366\n",
      "Logits: tensor([-0.2505, -0.2462, -0.2491,  ..., -7.9138, -8.1235, -7.7277],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Batch [32/32], Loss: 0.06449490785598755\n",
      "Logits: tensor([-0.2427, -0.2499, -0.2512,  ..., -7.5559, -7.8154, -7.9867],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [46/50], Loss: 0.06449490785598755\n",
      "Epoch [47/50], Batch [1/32], Loss: 0.06461334973573685\n",
      "Logits: tensor([-0.2550, -0.2542, -0.2486,  ..., -7.8209, -8.1031, -7.9698],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [2/32], Loss: 0.06367138028144836\n",
      "Logits: tensor([-0.2481, -0.2548, -0.2460,  ..., -8.3031, -8.2011, -7.8406],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [3/32], Loss: 0.06424286961555481\n",
      "Logits: tensor([-0.2532, -0.2465, -0.2486,  ..., -7.9154, -8.1471, -7.8541],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [4/32], Loss: 0.06391630321741104\n",
      "Logits: tensor([-0.2552, -0.2468, -0.2556,  ..., -8.2930, -7.9910, -8.0185],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [5/32], Loss: 0.06373102217912674\n",
      "Logits: tensor([-0.2552, -0.2508, -0.2499,  ..., -8.1907, -7.8919, -7.8816],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [6/32], Loss: 0.06421275436878204\n",
      "Logits: tensor([-0.2521, -0.2499, -0.2388,  ..., -8.0739, -8.0554, -7.9889],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [7/32], Loss: 0.06435567140579224\n",
      "Logits: tensor([-0.2556, -0.2491, -0.2388,  ..., -8.2106, -7.7324, -7.7823],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [8/32], Loss: 0.06381061673164368\n",
      "Logits: tensor([-0.2438, -0.2479, -0.2485,  ..., -7.5628, -7.8896, -7.9276],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [9/32], Loss: 0.06370888650417328\n",
      "Logits: tensor([-0.2508, -0.2461, -0.2461,  ..., -8.2350, -7.6593, -7.7497],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [10/32], Loss: 0.06462232768535614\n",
      "Logits: tensor([-0.2557, -0.2507, -0.2493,  ..., -7.8527, -7.8466, -7.8763],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [11/32], Loss: 0.0633484423160553\n",
      "Logits: tensor([-0.2491, -0.2517, -0.2455,  ..., -8.1348, -7.8690, -8.2364],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [12/32], Loss: 0.06368062645196915\n",
      "Logits: tensor([-0.2540, -0.2485, -0.2493,  ..., -7.9718, -7.9942, -8.0267],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [13/32], Loss: 0.06364171952009201\n",
      "Logits: tensor([-0.2513, -0.2535, -0.2488,  ..., -8.1075, -7.8812, -8.3157],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [14/32], Loss: 0.06432828307151794\n",
      "Logits: tensor([-0.2451, -0.2477, -0.2500,  ..., -8.2466, -8.1039, -7.9952],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [15/32], Loss: 0.06401709467172623\n",
      "Logits: tensor([-0.2461, -0.2514, -0.2524,  ..., -7.9467, -7.8413, -7.8340],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [16/32], Loss: 0.06404190510511398\n",
      "Logits: tensor([-0.2539, -0.2498, -0.2479,  ..., -8.1059, -7.9334, -7.8925],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [17/32], Loss: 0.06396908313035965\n",
      "Logits: tensor([-0.2472, -0.2482, -0.2482,  ..., -8.0223, -7.7366, -8.1068],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [18/32], Loss: 0.06373857706785202\n",
      "Logits: tensor([-0.2422, -0.2494, -0.2427,  ..., -7.9358, -8.1578, -7.9744],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [19/32], Loss: 0.06376218050718307\n",
      "Logits: tensor([-0.2462, -0.2538, -0.2513,  ..., -7.9548, -7.7445, -8.1866],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [20/32], Loss: 0.06403850764036179\n",
      "Logits: tensor([-0.2480, -0.2458, -0.2434,  ..., -8.0005, -8.0314, -8.2967],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [21/32], Loss: 0.0639018639922142\n",
      "Logits: tensor([-0.2530, -0.2466, -0.2489,  ..., -7.9860, -8.0787, -7.9350],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [22/32], Loss: 0.06420885771512985\n",
      "Logits: tensor([-0.2512, -0.2512, -0.2468,  ..., -7.8847, -8.0545, -8.2313],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [23/32], Loss: 0.06384358555078506\n",
      "Logits: tensor([-0.2516, -0.2447, -0.2443,  ..., -7.6949, -7.7217, -7.9497],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [24/32], Loss: 0.0644153505563736\n",
      "Logits: tensor([-0.2521, -0.2463, -0.2455,  ..., -7.6559, -7.8302, -7.7745],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [25/32], Loss: 0.06401946395635605\n",
      "Logits: tensor([-0.2467, -0.2491, -0.2441,  ..., -8.1732, -8.2618, -7.4878],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [26/32], Loss: 0.06361831724643707\n",
      "Logits: tensor([-0.2507, -0.2464, -0.2470,  ..., -8.0843, -8.1839, -7.9867],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [27/32], Loss: 0.0640646368265152\n",
      "Logits: tensor([-0.2503, -0.2450, -0.2475,  ..., -7.7253, -8.0870, -8.0280],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [28/32], Loss: 0.06408830732107162\n",
      "Logits: tensor([-0.2472, -0.2434, -0.2430,  ..., -7.7942, -7.9818, -7.9309],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [29/32], Loss: 0.06422197073698044\n",
      "Logits: tensor([-0.2425, -0.2519, -0.2499,  ..., -7.9899, -8.0366, -7.8767],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [30/32], Loss: 0.06407923251390457\n",
      "Logits: tensor([-0.2499, -0.2435, -0.2480,  ..., -7.9165, -7.9386, -7.9103],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [31/32], Loss: 0.06446229666471481\n",
      "Logits: tensor([-0.2448, -0.2539, -0.2478,  ..., -7.9702, -7.8911, -7.9326],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Batch [32/32], Loss: 0.06356658786535263\n",
      "Logits: tensor([-0.2470, -0.2492, -0.2472,  ..., -7.9882, -7.7158, -7.7191],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [47/50], Loss: 0.06356658786535263\n",
      "Epoch [48/50], Batch [1/32], Loss: 0.06329942494630814\n",
      "Logits: tensor([-0.2490, -0.2495, -0.2499,  ..., -7.9687, -7.6261, -7.8782],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [2/32], Loss: 0.06378506869077682\n",
      "Logits: tensor([-0.2491, -0.2484, -0.2465,  ..., -7.8436, -7.9393, -8.0429],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [3/32], Loss: 0.06393999606370926\n",
      "Logits: tensor([-0.2442, -0.2461, -0.2441,  ..., -7.8673, -7.9495, -8.0193],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [4/32], Loss: 0.0643124133348465\n",
      "Logits: tensor([-0.2500, -0.2455, -0.2513,  ..., -8.0493, -8.0012, -7.9512],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [5/32], Loss: 0.06424980610609055\n",
      "Logits: tensor([-0.2491, -0.2445, -0.2457,  ..., -8.0190, -7.9896, -8.3306],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [6/32], Loss: 0.06417199224233627\n",
      "Logits: tensor([-0.2393, -0.2461, -0.2476,  ..., -8.2584, -7.8060, -7.9709],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [7/32], Loss: 0.06374654918909073\n",
      "Logits: tensor([-0.2439, -0.2419, -0.2493,  ..., -8.1701, -8.0514, -8.0320],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [8/32], Loss: 0.06377703696489334\n",
      "Logits: tensor([-0.2482, -0.2508, -0.2431,  ..., -7.9749, -8.0166, -7.8323],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [9/32], Loss: 0.0637255385518074\n",
      "Logits: tensor([-0.2421, -0.2481, -0.2421,  ..., -7.9420, -8.1949, -8.1838],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [10/32], Loss: 0.06406760960817337\n",
      "Logits: tensor([-0.2448, -0.2431, -0.2454,  ..., -7.7385, -8.1328, -8.0565],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [11/32], Loss: 0.06360136717557907\n",
      "Logits: tensor([-0.2449, -0.2503, -0.2483,  ..., -8.2064, -8.1663, -7.6514],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [12/32], Loss: 0.06409745663404465\n",
      "Logits: tensor([-0.2483, -0.2398, -0.2484,  ..., -8.0845, -8.0237, -8.0946],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [13/32], Loss: 0.06379970908164978\n",
      "Logits: tensor([-0.2433, -0.2464, -0.2418,  ..., -8.1091, -8.0514, -7.6809],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [14/32], Loss: 0.06406330317258835\n",
      "Logits: tensor([-0.2367, -0.2462, -0.2480,  ..., -8.2764, -8.3266, -8.0764],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [15/32], Loss: 0.06393889337778091\n",
      "Logits: tensor([-0.2451, -0.2409, -0.2427,  ..., -8.1096, -7.7303, -8.1499],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [16/32], Loss: 0.06364915519952774\n",
      "Logits: tensor([-0.2410, -0.2445, -0.2428,  ..., -7.9432, -7.9369, -7.9814],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [17/32], Loss: 0.06451041251420975\n",
      "Logits: tensor([-0.2452, -0.2427, -0.2391,  ..., -7.8951, -7.9904, -7.9829],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [18/32], Loss: 0.06429699808359146\n",
      "Logits: tensor([-0.2399, -0.2469, -0.2472,  ..., -8.0986, -8.2218, -7.9692],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [19/32], Loss: 0.06377167999744415\n",
      "Logits: tensor([-0.2467, -0.2405, -0.2410,  ..., -7.9492, -7.7435, -7.8502],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [20/32], Loss: 0.06422416865825653\n",
      "Logits: tensor([-0.2497, -0.2418, -0.2417,  ..., -8.0527, -7.9287, -7.7795],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [21/32], Loss: 0.06424270570278168\n",
      "Logits: tensor([-0.2442, -0.2398, -0.2463,  ..., -8.2034, -8.1358, -8.0314],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [22/32], Loss: 0.06393326818943024\n",
      "Logits: tensor([-0.2449, -0.2410, -0.2425,  ..., -8.0098, -8.2918, -8.0867],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [23/32], Loss: 0.06444495171308517\n",
      "Logits: tensor([-0.2463, -0.2424, -0.2392,  ..., -8.3416, -7.7773, -7.9592],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [24/32], Loss: 0.06383389234542847\n",
      "Logits: tensor([-0.2401, -0.2432, -0.2525,  ..., -8.1949, -8.0272, -7.8739],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [25/32], Loss: 0.0638999491930008\n",
      "Logits: tensor([-0.2479, -0.2452, -0.2485,  ..., -8.1235, -8.0919, -7.8099],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [26/32], Loss: 0.06415220350027084\n",
      "Logits: tensor([-0.2455, -0.2427, -0.2452,  ..., -7.7659, -8.1537, -7.8862],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [27/32], Loss: 0.06387212127447128\n",
      "Logits: tensor([-0.2422, -0.2371, -0.2437,  ..., -7.8882, -8.0375, -8.2189],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [28/32], Loss: 0.06431175768375397\n",
      "Logits: tensor([-0.2432, -0.2382, -0.2431,  ..., -7.7548, -8.1429, -8.1144],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [29/32], Loss: 0.0637843981385231\n",
      "Logits: tensor([-0.2412, -0.2389, -0.2385,  ..., -8.0128, -8.1580, -8.2147],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [30/32], Loss: 0.06411576271057129\n",
      "Logits: tensor([-0.2410, -0.2388, -0.2419,  ..., -7.9258, -7.8035, -7.8451],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [31/32], Loss: 0.06359289586544037\n",
      "Logits: tensor([-0.2416, -0.2424, -0.2427,  ..., -8.4236, -8.0468, -8.3728],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Batch [32/32], Loss: 0.06398190557956696\n",
      "Logits: tensor([-0.2398, -0.2464, -0.2416,  ..., -7.2460, -8.0127, -7.9158],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [48/50], Loss: 0.06398190557956696\n",
      "Epoch [49/50], Batch [1/32], Loss: 0.06392373144626617\n",
      "Logits: tensor([-0.2439, -0.2388, -0.2398,  ..., -8.0312, -8.1707, -7.8599],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [2/32], Loss: 0.06389648467302322\n",
      "Logits: tensor([-0.2436, -0.2429, -0.2439,  ..., -7.9806, -7.8747, -7.8674],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [3/32], Loss: 0.06355059891939163\n",
      "Logits: tensor([-0.2392, -0.2420, -0.2356,  ..., -8.0738, -8.2239, -8.2838],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [4/32], Loss: 0.06434575468301773\n",
      "Logits: tensor([-0.2358, -0.2354, -0.2404,  ..., -7.7612, -7.9308, -8.2480],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [5/32], Loss: 0.06369352340698242\n",
      "Logits: tensor([-0.2425, -0.2457, -0.2482,  ..., -8.0136, -8.1382, -7.7997],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [6/32], Loss: 0.06429881602525711\n",
      "Logits: tensor([-0.2425, -0.2477, -0.2397,  ..., -8.0790, -8.0849, -7.6148],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [7/32], Loss: 0.06454722583293915\n",
      "Logits: tensor([-0.2492, -0.2377, -0.2403,  ..., -8.0971, -8.0909, -7.7228],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [8/32], Loss: 0.06421905755996704\n",
      "Logits: tensor([-0.2398, -0.2387, -0.2379,  ..., -8.2391, -8.0041, -7.7049],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [9/32], Loss: 0.06396821141242981\n",
      "Logits: tensor([-0.2407, -0.2396, -0.2464,  ..., -8.0310, -7.7679, -7.6006],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [10/32], Loss: 0.0641813799738884\n",
      "Logits: tensor([-0.2417, -0.2404, -0.2379,  ..., -7.7867, -8.1705, -8.0228],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [11/32], Loss: 0.06394343823194504\n",
      "Logits: tensor([-0.2411, -0.2445, -0.2394,  ..., -8.2818, -8.0500, -7.9921],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [12/32], Loss: 0.06372339278459549\n",
      "Logits: tensor([-0.2388, -0.2415, -0.2352,  ..., -8.1087, -8.2321, -7.9790],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [13/32], Loss: 0.06398431956768036\n",
      "Logits: tensor([-0.2435, -0.2416, -0.2393,  ..., -8.1504, -8.3842, -7.9636],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [14/32], Loss: 0.0634642168879509\n",
      "Logits: tensor([-0.2329, -0.2289, -0.2392,  ..., -8.2375, -8.2449, -8.1674],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [15/32], Loss: 0.06387549638748169\n",
      "Logits: tensor([-0.2456, -0.2434, -0.2404,  ..., -8.0123, -8.0952, -8.2071],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [16/32], Loss: 0.0637676939368248\n",
      "Logits: tensor([-0.2417, -0.2388, -0.2413,  ..., -8.1770, -8.1925, -8.0200],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [17/32], Loss: 0.06401638686656952\n",
      "Logits: tensor([-0.2395, -0.2323, -0.2389,  ..., -8.1382, -7.5775, -7.7904],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [18/32], Loss: 0.063449427485466\n",
      "Logits: tensor([-0.2424, -0.2345, -0.2371,  ..., -7.9966, -7.6877, -8.0958],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [19/32], Loss: 0.06418215483427048\n",
      "Logits: tensor([-0.2400, -0.2347, -0.2353,  ..., -8.1960, -7.8872, -8.1529],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [20/32], Loss: 0.06338698416948318\n",
      "Logits: tensor([-0.2363, -0.2341, -0.2387,  ..., -8.1490, -7.9753, -7.9343],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [21/32], Loss: 0.06382369995117188\n",
      "Logits: tensor([-0.2376, -0.2358, -0.2375,  ..., -7.8161, -7.9548, -7.9731],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [22/32], Loss: 0.06438502669334412\n",
      "Logits: tensor([-0.2359, -0.2386, -0.2411,  ..., -7.9238, -8.2305, -7.7836],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [23/32], Loss: 0.06386588513851166\n",
      "Logits: tensor([-0.2433, -0.2394, -0.2344,  ..., -7.7463, -8.0191, -8.1637],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [24/32], Loss: 0.0642092153429985\n",
      "Logits: tensor([-0.2404, -0.2359, -0.2443,  ..., -8.1803, -7.9341, -8.0501],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [25/32], Loss: 0.06391697376966476\n",
      "Logits: tensor([-0.2342, -0.2372, -0.2364,  ..., -8.3372, -7.9726, -7.9102],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [26/32], Loss: 0.06377974152565002\n",
      "Logits: tensor([-0.2436, -0.2385, -0.2358,  ..., -8.0038, -8.0458, -7.8605],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [27/32], Loss: 0.06351793557405472\n",
      "Logits: tensor([-0.2327, -0.2394, -0.2336,  ..., -7.9908, -8.3556, -8.0939],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [28/32], Loss: 0.06392887979745865\n",
      "Logits: tensor([-0.2335, -0.2386, -0.2399,  ..., -8.2998, -8.1561, -8.0463],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [29/32], Loss: 0.06396207958459854\n",
      "Logits: tensor([-0.2374, -0.2369, -0.2334,  ..., -7.9653, -7.9758, -7.9200],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [30/32], Loss: 0.06401629000902176\n",
      "Logits: tensor([-0.2378, -0.2344, -0.2346,  ..., -7.7580, -8.2450, -7.8704],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [31/32], Loss: 0.06422977894544601\n",
      "Logits: tensor([-0.2382, -0.2338, -0.2359,  ..., -7.9814, -7.8768, -8.2854],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Batch [32/32], Loss: 0.06443165242671967\n",
      "Logits: tensor([-0.2367, -0.2374, -0.2349,  ..., -8.1246, -8.0837, -7.8522],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [49/50], Loss: 0.06443165242671967\n",
      "Epoch [50/50], Batch [1/32], Loss: 0.06351513415575027\n",
      "Logits: tensor([-0.2377, -0.2312, -0.2404,  ..., -7.8992, -8.0202, -8.0101],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [2/32], Loss: 0.0638289675116539\n",
      "Logits: tensor([-0.2411, -0.2381, -0.2349,  ..., -8.1564, -8.0821, -7.6231],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [3/32], Loss: 0.06416644901037216\n",
      "Logits: tensor([-0.2345, -0.2329, -0.2423,  ..., -7.8500, -8.4110, -8.1494],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [4/32], Loss: 0.06430573016405106\n",
      "Logits: tensor([-0.2354, -0.2373, -0.2383,  ..., -7.9722, -7.9660, -8.0106],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [5/32], Loss: 0.06395632773637772\n",
      "Logits: tensor([-0.2292, -0.2375, -0.2430,  ..., -8.2826, -8.1590, -8.1322],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [6/32], Loss: 0.06404206901788712\n",
      "Logits: tensor([-0.2326, -0.2363, -0.2390,  ..., -7.8879, -8.1571, -7.9338],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [7/32], Loss: 0.06413201987743378\n",
      "Logits: tensor([-0.2331, -0.2335, -0.2354,  ..., -8.1321, -8.2325, -8.0337],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [8/32], Loss: 0.06385746598243713\n",
      "Logits: tensor([-0.2390, -0.2413, -0.2354,  ..., -7.9050, -8.2522, -8.2136],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [9/32], Loss: 0.06399599462747574\n",
      "Logits: tensor([-0.2389, -0.2285, -0.2430,  ..., -7.8175, -8.0872, -7.9346],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [10/32], Loss: 0.0641259178519249\n",
      "Logits: tensor([-0.2396, -0.2357, -0.2337,  ..., -8.1584, -8.2476, -7.6359],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [11/32], Loss: 0.06397242844104767\n",
      "Logits: tensor([-0.2326, -0.2306, -0.2342,  ..., -7.9911, -7.9522, -8.1419],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [12/32], Loss: 0.06363710016012192\n",
      "Logits: tensor([-0.2352, -0.2302, -0.2351,  ..., -7.9666, -7.8714, -7.6527],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [13/32], Loss: 0.06385262310504913\n",
      "Logits: tensor([-0.2376, -0.2359, -0.2385,  ..., -7.8773, -8.0635, -8.4469],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [14/32], Loss: 0.06379613280296326\n",
      "Logits: tensor([-0.2360, -0.2282, -0.2298,  ..., -8.2790, -8.1344, -7.8263],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [15/32], Loss: 0.06364951282739639\n",
      "Logits: tensor([-0.2372, -0.2353, -0.2335,  ..., -8.0011, -7.9676, -7.8731],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [16/32], Loss: 0.06386644393205643\n",
      "Logits: tensor([-0.2313, -0.2311, -0.2350,  ..., -8.4246, -7.9578, -8.2257],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [17/32], Loss: 0.06373815983533859\n",
      "Logits: tensor([-0.2386, -0.2336, -0.2285,  ..., -8.0103, -8.1812, -8.2431],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [18/32], Loss: 0.0638396218419075\n",
      "Logits: tensor([-0.2351, -0.2383, -0.2409,  ..., -8.2836, -8.0051, -7.8765],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [19/32], Loss: 0.06400404870510101\n",
      "Logits: tensor([-0.2369, -0.2316, -0.2330,  ..., -8.0858, -7.8373, -8.1754],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [20/32], Loss: 0.06420806050300598\n",
      "Logits: tensor([-0.2356, -0.2284, -0.2302,  ..., -7.8730, -7.9302, -8.1913],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [21/32], Loss: 0.0637616366147995\n",
      "Logits: tensor([-0.2282, -0.2365, -0.2284,  ..., -8.3223, -8.1771, -8.0880],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [22/32], Loss: 0.0639471560716629\n",
      "Logits: tensor([-0.2373, -0.2257, -0.2298,  ..., -7.9977, -8.0472, -7.6932],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [23/32], Loss: 0.06409063190221786\n",
      "Logits: tensor([-0.2345, -0.2323, -0.2315,  ..., -7.7207, -8.0088, -8.1305],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [24/32], Loss: 0.06393301486968994\n",
      "Logits: tensor([-0.2239, -0.2307, -0.2326,  ..., -7.8147, -8.1248, -7.9645],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [25/32], Loss: 0.06397237628698349\n",
      "Logits: tensor([-0.2353, -0.2334, -0.2313,  ..., -8.3263, -7.9185, -8.3502],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [26/32], Loss: 0.06370803713798523\n",
      "Logits: tensor([-0.2336, -0.2269, -0.2318,  ..., -7.9423, -8.0123, -8.0980],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [27/32], Loss: 0.06362653523683548\n",
      "Logits: tensor([-0.2367, -0.2316, -0.2378,  ..., -8.0301, -8.1848, -8.1934],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [28/32], Loss: 0.06378526240587234\n",
      "Logits: tensor([-0.2369, -0.2394, -0.2405,  ..., -7.8212, -8.2522, -7.8162],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [29/32], Loss: 0.06376488506793976\n",
      "Logits: tensor([-0.2321, -0.2273, -0.2278,  ..., -7.7681, -8.0419, -8.1870],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [30/32], Loss: 0.06399162113666534\n",
      "Logits: tensor([-0.2350, -0.2328, -0.2315,  ..., -8.1433, -8.3668, -8.1141],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [31/32], Loss: 0.06402800232172012\n",
      "Logits: tensor([-0.2345, -0.2324, -0.2298,  ..., -7.9949, -8.1076, -7.7582],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Batch [32/32], Loss: 0.06413399428129196\n",
      "Logits: tensor([-0.2301, -0.2314, -0.2285,  ..., -8.2745, -8.0381, -7.7372],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "Epoch [50/50], Loss: 0.06413399428129196\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import dgl\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to convert a bipartite graph to DGLGraph format\n",
    "def convert_to_dgl_format(spectrum_matrix, bug_labels):\n",
    "    num_test_cases, num_statements = spectrum_matrix.shape\n",
    "    \n",
    "    # Create a new DGLGraph\n",
    "    g = dgl.graph(([], []), num_nodes=num_test_cases + num_statements)\n",
    "    \n",
    "    # Add edges between test case nodes and statement nodes based on spectrum\n",
    "    src = []\n",
    "    dst = []\n",
    "    for i in range(num_test_cases):\n",
    "        for j in range(num_statements):\n",
    "            if spectrum_matrix[i][j] == 1:\n",
    "                src.append(i)\n",
    "                dst.append(num_test_cases + j)\n",
    "    \n",
    "    g.add_edges(src, dst)\n",
    "    \n",
    "    # Add self-loops to all nodes\n",
    "    g = dgl.add_self_loop(g)\n",
    "    \n",
    "    # Add node features (spectrum matrix)\n",
    "    features = torch.zeros((num_test_cases + num_statements, num_statements), dtype=torch.float)\n",
    "    features[:num_test_cases, :] = torch.tensor(spectrum_matrix, dtype=torch.float)\n",
    "    g.ndata['x'] = features\n",
    "    \n",
    "    # Add labels (bug labels)\n",
    "    labels = torch.zeros((num_test_cases + num_statements, 1), dtype=torch.float)\n",
    "    labels[:num_test_cases, 0] = torch.tensor(bug_labels, dtype=torch.float)\n",
    "    g.ndata['y'] = labels\n",
    "    \n",
    "    return g\n",
    "\n",
    "# Load and preprocess the dataset of graphs\n",
    "def load_and_preprocess_dataset(graphs_dir):\n",
    "    graph_files = [f for f in os.listdir(graphs_dir) if f.endswith('.pickle')]\n",
    "    dataset = []\n",
    "    \n",
    "    for graph_file in graph_files:\n",
    "        with open(os.path.join(graphs_dir, graph_file), 'rb') as f:\n",
    "            G = pickle.load(f)\n",
    "        \n",
    "        # Extract spectrum and bug labels from G (assuming you have stored these in G)\n",
    "        spectrum_matrix = np.random.randint(0, 2, size=(50, 500))  # Replace with actual data\n",
    "        bug_labels = np.random.randint(0, 2, size=50)  # Replace with actual data\n",
    "        \n",
    "        # Convert to DGLGraph format\n",
    "        dgl_graph = convert_to_dgl_format(spectrum_matrix, bug_labels)\n",
    "        \n",
    "        dataset.append(dgl_graph)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Define your GNN model\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=64):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = dgl.nn.GraphConv(num_features, hidden_dim, allow_zero_in_degree=True)\n",
    "        self.conv2 = dgl.nn.GraphConv(hidden_dim, 1, allow_zero_in_degree=True)\n",
    "    \n",
    "    def forward(self, g, features):\n",
    "        h = features\n",
    "        h = torch.relu(self.conv1(g, h))\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "\n",
    "# Custom collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    batched_graph = dgl.batch(batch)\n",
    "    return batched_graph\n",
    "\n",
    "# Function to train the GNN model\n",
    "def train_gnn_model(dataset, num_epochs=50, batch_size=32):\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    model = GNNModel(num_features=dataset[0].ndata['x'].shape[1])  # Initialize the model\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Binary cross-entropy loss with logits\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch, batch.ndata['x']).squeeze()\n",
    "            labels = batch.ndata['y'].squeeze()\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print the output of the neural network\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {loss.item()}\")\n",
    "            print(\"Logits:\", logits)\n",
    "        \n",
    "        # Print loss or validation metrics after each epoch if needed\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "# Example usage:\n",
    "graphs_dir = 'graphs'  # Directory where the graphs are stored\n",
    "dataset = load_and_preprocess_dataset(graphs_dir)\n",
    "train_gnn_model(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c172e76d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m graphs_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgraphs\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Directory where the graphs are stored\u001b[39;00m\n\u001b[1;32m     61\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_and_preprocess_dataset(graphs_dir)\n\u001b[0;32m---> 62\u001b[0m train_gnn_model(dataset)\n",
      "Cell \u001b[0;32mIn[75], line 46\u001b[0m, in \u001b[0;36mtrain_gnn_model\u001b[0;34m(dataset, num_epochs, batch_size)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     45\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 46\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(batch, batch\u001b[38;5;241m.\u001b[39mndata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     47\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mndata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     48\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(logits, labels)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[72], line 9\u001b[0m, in \u001b[0;36mGNNModel.forward\u001b[0;34m(self, g, features)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, g, features):\n\u001b[1;32m      8\u001b[0m     h \u001b[38;5;241m=\u001b[39m features\n\u001b[0;32m----> 9\u001b[0m     h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(g, h))\n\u001b[1;32m     10\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(g, h)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m h\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/dgl/nn/pytorch/conv/graphconv.py:476\u001b[0m, in \u001b[0;36mGraphConv.forward\u001b[0;34m(self, graph, feat, weight, edge_weight)\u001b[0m\n\u001b[1;32m    473\u001b[0m     rst \u001b[38;5;241m=\u001b[39m rst \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 476\u001b[0m     rst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activation(rst)\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rst\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import dgl\n",
    "\n",
    "# Custom loss function to minimize wastage of work\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, weight_false_positive=1.0, weight_false_negative=1.0):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.weight_false_positive = weight_false_positive  # Weight for false positive penalty\n",
    "        self.weight_false_negative = weight_false_negative  # Weight for false negative penalty\n",
    "    \n",
    "    def forward(self, logits, labels):\n",
    "        # Calculate Binary Cross-Entropy Loss\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
    "        \n",
    "        # Penalize false positives and false negatives\n",
    "        predicted_labels = torch.sigmoid(logits)  # Convert logits to probabilities\n",
    "        predicted_labels_binary = torch.round(predicted_labels)  # Binary predictions\n",
    "        \n",
    "        # False positives penalty\n",
    "        false_positives = torch.logical_and(predicted_labels_binary == 1, labels == 0).float()\n",
    "        fp_penalty = torch.sum(false_positives) * self.weight_false_positive\n",
    "        \n",
    "        # False negatives penalty (for statements that are buggy but predicted as non-buggy)\n",
    "        false_negatives = torch.logical_and(predicted_labels_binary == 0, labels == 1).float()\n",
    "        fn_penalty = torch.sum(false_negatives) * self.weight_false_negative\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = bce_loss + fp_penalty + fn_penalty\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "# Function to train the GNN model\n",
    "def train_gnn_model(dataset, num_epochs=50, batch_size=32):\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    model = GNNModel(num_features=dataset[0].ndata['x'].shape[1])  # Initialize the model\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = CustomLoss(weight_false_positive=2.0, weight_false_negative=5.0)  # Custom loss function with penalty weights\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch, batch.ndata['x']).squeeze()\n",
    "            labels = batch.ndata['y'].squeeze()\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print the output of the neural network\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {loss.item()}\")\n",
    "            print(\"Logits:\", logits)\n",
    "        \n",
    "        # Print loss or validation metrics after each epoch if needed\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "# Example usage:\n",
    "graphs_dir = 'graphs'  # Directory where the graphs are stored\n",
    "dataset = load_and_preprocess_dataset(graphs_dir)\n",
    "train_gnn_model(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09ab3af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
